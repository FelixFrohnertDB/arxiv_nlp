{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4b06eeb-b7bc-4787-a65f-de3ef301bd4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import gc\n",
    "import re\n",
    "# import random\n",
    "# import time\n",
    "# import pickle\n",
    "import logging\n",
    "import multiprocessing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# import networkx as nx\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset#, random_split, TensorDataset\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler#, MinMaxScaler, RobustScaler\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, auc, precision_score, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "# from sklearn.decomposition import PCA\n",
    "# from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "# from statsmodels.tsa.stattools import adfuller, acf, pacf, ccf\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import RULE_DEFAULT, RULE_KEEP\n",
    "from gensim.models.callbacks import CallbackAny2Vec\n",
    "\n",
    "# from scipy import sparse\n",
    "\n",
    "# from typing import List, Tuple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31d6f869",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_co_occurrences(word_year_list,word_co_occurrences):\n",
    "    # Iterate through the words in the list\n",
    "    word_list, year = word_year_list\n",
    "    \n",
    "    for word in word_list:\n",
    "        # If the word is not already in the dictionary, add it with an empty list\n",
    "        if word not in word_co_occurrences:\n",
    "            word_co_occurrences[word] = {}\n",
    "        \n",
    "        # Add words from the list to the co-occurrence list for the current word\n",
    "        for other_word in word_list:\n",
    "            # if other_word != word and other_word not in word_co_occurrences[word]:\n",
    "            #     word_co_occurrences[word].append(other_word)\n",
    "            if other_word != word and other_word not in word_co_occurrences[word]:\n",
    "                word_co_occurrences[word][other_word] = [year] \n",
    "            \n",
    "            elif other_word != word and other_word in word_co_occurrences[word]:\n",
    "                # word_co_occurrences[word][other_word][0] +=1\n",
    "                word_co_occurrences[word][other_word].append(year)\n",
    "\n",
    "def keep_words_with_underscore(input_string):\n",
    "    # Define a regular expression pattern to match words with underscores\n",
    "    pattern = r'\\b\\w*_[\\w_]*\\b'\n",
    "\n",
    "    # Use re.findall to extract words that match the pattern\n",
    "    matching_words = re.findall(pattern, input_string)\n",
    "\n",
    "    # Join the matching words to form the final string\n",
    "    result = ' '.join(matching_words)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3be6ba50",
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_abstracts = np.load(\"saved_files/ngram_abstracts.npy\", mmap_mode=\"r\")\n",
    "filtered_concept_arr = np.unique(np.load(\"saved_files/overlapping_filtered_5_concepts.npy\"))\n",
    "year_arr = np.load(\"saved_files/year_arr.npy\", mmap_mode=\"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff5fde71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63633/63633 [00:02<00:00, 29345.82it/s]\n"
     ]
    }
   ],
   "source": [
    "df_ab = pd.DataFrame(data=ngram_abstracts,    # values\n",
    "                columns=[\"ab\"])  # 1st row as the column names\n",
    "df_ab[\"year\"] = year_arr\n",
    "\n",
    "# Define important words\n",
    "phys_concept_dict = {k:1 for k in filtered_concept_arr}\n",
    "\n",
    "ocurr_arr = []\n",
    "for abstract, year in zip(ngram_abstracts, year_arr):\n",
    "    temp = keep_words_with_underscore(abstract)\n",
    "    if temp.count(\" \") > 0:\n",
    "        temp = temp.split(\" \") \n",
    "        temp = [s for s in temp if s in phys_concept_dict]\n",
    "        ocurr_arr.append([list(filter((\"_\").__ne__, temp)),year])\n",
    "                        \n",
    "word_co_occurrences = {}\n",
    "for word_list in tqdm(ocurr_arr):\n",
    "    update_co_occurrences(word_list, word_co_occurrences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d0d1fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "load = True \n",
    "\n",
    "year_train = 2017 #train_label_years => [2018, 2019, 2020]\n",
    "year_test = 2020 # test_label_years => [2021, 2022, 2023]\n",
    "if not load:\n",
    "\n",
    "    # Custom rule function\n",
    "    def custom_rule(word, count, min_count):\n",
    "        if word in phys_concept_dict:\n",
    "            return RULE_KEEP\n",
    "        else:\n",
    "            return RULE_DEFAULT\n",
    "\n",
    "    cores = multiprocessing.cpu_count()\n",
    "\n",
    "    class LossLogger(CallbackAny2Vec):\n",
    "        '''Callback to log loss after each epoch.'''\n",
    "        def __init__(self):\n",
    "            self.epoch = 0\n",
    "            self.losses = []\n",
    "            self.previous_loss = 0\n",
    "            self.initial_loss = 0\n",
    "\n",
    "        def on_epoch_begin(self, model):\n",
    "            # Record the initial loss at the start of each epoch\n",
    "            self.initial_loss = model.get_latest_training_loss()\n",
    "\n",
    "        def on_epoch_end(self, model):\n",
    "            current_loss = model.get_latest_training_loss()\n",
    "            # Calculate the loss for the current epoch\n",
    "            loss = current_loss - self.initial_loss\n",
    "            self.losses.append(loss)\n",
    "            self.initial_loss = current_loss  # Reset for the next epoch\n",
    "            self.epoch += 1\n",
    "\n",
    "    \n",
    "\n",
    "        \n",
    "    ngram_abstracts_year = [ab.split() for ab in df_ab[df_ab['year'] <= year_train][\"ab\"].to_numpy()]\n",
    "    loss_logger = LossLogger()\n",
    "    w2v_model = Word2Vec(min_count=5,\n",
    "                        window=15,\n",
    "                        vector_size=128,\n",
    "                        sample=1e-4, \n",
    "                        alpha=0.01, \n",
    "                        min_alpha= 0.0001, \n",
    "                        negative=15,\n",
    "                        workers=cores-1)\n",
    "    # Build the vocabulary on the first year's data\n",
    "    w2v_model.build_vocab(ngram_abstracts_year, trim_rule=custom_rule)\n",
    "    w2v_model.train(ngram_abstracts_year, total_examples=w2v_model.corpus_count, epochs=50, compute_loss=True, callbacks=[loss_logger])\n",
    "    w2v_model.save(f\"saved_models/baseline_1_train_model_{year_train}.model\")\n",
    "\n",
    "    ngram_abstracts_year = [ab.split() for ab in df_ab[df_ab['year'] <= year_test][\"ab\"].to_numpy()]\n",
    "    loss_logger = LossLogger()\n",
    "    w2v_model = Word2Vec(min_count=5,\n",
    "                        window=15,\n",
    "                        vector_size=128,\n",
    "                        sample=1e-4, \n",
    "                        alpha=0.01, \n",
    "                        min_alpha= 0.0001, \n",
    "                        negative=15,\n",
    "                        workers=cores-1)\n",
    "    # Build the vocabulary on the first year's data\n",
    "    w2v_model.build_vocab(ngram_abstracts_year, trim_rule=custom_rule)\n",
    "    w2v_model.train(ngram_abstracts_year, total_examples=w2v_model.corpus_count, epochs=50, compute_loss=True, callbacks=[loss_logger])\n",
    "    w2v_model.save(f\"saved_models/baseline_1_test_model_{year_test}.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "40065dd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-04 12:57:32,315 - INFO - loading Word2Vec object from saved_models/baseline_1_train_model_2017.model\n",
      "2024-08-04 12:57:32,329 - INFO - loading wv recursively from saved_models/baseline_1_train_model_2017.model.wv.* with mmap=None\n",
      "2024-08-04 12:57:32,330 - INFO - setting ignored attribute cum_table to None\n",
      "2024-08-04 12:57:32,423 - INFO - Word2Vec lifecycle event {'fname': 'saved_models/baseline_1_train_model_2017.model', 'datetime': '2024-08-04T12:57:32.423548', 'gensim': '4.3.2', 'python': '3.9.13 (main, Aug 25 2022, 23:26:10) \\n[GCC 11.2.0]', 'platform': 'Linux-6.5.0-45-generic-x86_64-with-glibc2.35', 'event': 'loaded'}\n",
      "2024-08-04 12:57:32,448 - INFO - loading Word2Vec object from saved_models/baseline_1_test_model_2020.model\n",
      "2024-08-04 12:57:32,461 - INFO - loading wv recursively from saved_models/baseline_1_test_model_2020.model.wv.* with mmap=None\n",
      "2024-08-04 12:57:32,462 - INFO - setting ignored attribute cum_table to None\n",
      "2024-08-04 12:57:32,560 - INFO - Word2Vec lifecycle event {'fname': 'saved_models/baseline_1_test_model_2020.model', 'datetime': '2024-08-04T12:57:32.560075', 'gensim': '4.3.2', 'python': '3.9.13 (main, Aug 25 2022, 23:26:10) \\n[GCC 11.2.0]', 'platform': 'Linux-6.5.0-45-generic-x86_64-with-glibc2.35', 'event': 'loaded'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9934 vectors, missed 301 vectors.\n",
      "Found 10172 vectors, missed 63 vectors.\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "def get_baseline_1_embeddings(loaded_w2v: KeyedVectors, phys_concept_dict: dict, embedding_dim: int = 128):\n",
    "    \"\"\"\n",
    "    Extract embeddings for physical concepts from a Word2Vec model.\n",
    "\n",
    "    Args:\n",
    "        loaded_w2v (KeyedVectors): The preloaded Word2Vec model.\n",
    "        phys_concept_dict (dict): Dictionary of physical concepts.\n",
    "        embedding_dim (int): Dimensionality of the embeddings. Defaults to 128.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - np.ndarray: Array of concept embeddings.\n",
    "            - np.ndarray: Array of concept identifiers.\n",
    "    \"\"\"\n",
    "    c_dict = {}\n",
    "    \n",
    "    for c in phys_concept_dict:\n",
    "        try:\n",
    "            vec_enc = loaded_w2v.wv[c]  # Use direct indexing for simplicity\n",
    "            c_dict[c] = vec_enc\n",
    "        except KeyError:\n",
    "            continue\n",
    "\n",
    "    num_found = len(c_dict)\n",
    "    num_missed = len(phys_concept_dict) - num_found\n",
    "    print(f\"Found {num_found} vectors, missed {num_missed} vectors.\")\n",
    "    \n",
    "    if num_found == 0:\n",
    "        return np.empty((0, embedding_dim)), np.empty((0,), dtype=\"<U55\")\n",
    "\n",
    "    c_encoding_arr = np.zeros((num_found, embedding_dim))\n",
    "    c_inx_arr = np.zeros((num_found,), dtype=\"<U55\")\n",
    "\n",
    "    for cnt, (concept, encoding) in enumerate(c_dict.items()):\n",
    "        c_encoding_arr[cnt] = encoding \n",
    "        c_inx_arr[cnt] = concept\n",
    "\n",
    "    return c_encoding_arr, c_inx_arr\n",
    "\n",
    "baseline_1_train_encoding_arr, baseline_1_train_c_arr = get_baseline_1_embeddings(Word2Vec.load(f\"saved_models/baseline_1_train_model_{year_train}.model\"),phys_concept_dict)\n",
    "\n",
    "baseline_1_test_encoding_arr, baseline_1_test_c_arr = get_baseline_1_embeddings(Word2Vec.load(f\"saved_models/baseline_1_test_model_{year_test}.model\"),phys_concept_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "4bf65177",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.1613820528800398,\n",
       " 0.32338415699922424,\n",
       " -0.06582609016035569,\n",
       " 0.15827089773175929,\n",
       " 0.0784891981166732,\n",
       " 0.09550957606975895,\n",
       " 0.07506304065007421,\n",
       " 0.31014132433572555,\n",
       " 0.08969592491936489,\n",
       " 0.5195501686770382,\n",
       " -0.07280688982005569,\n",
       " 0.10723640671616619,\n",
       " 0.39932312020836086,\n",
       " 0.12261352050477052,\n",
       " 0.15954382078986637,\n",
       " 0.01494135143815298,\n",
       " 0.5313000917090198,\n",
       " -0.12593486335995965,\n",
       " -0.0083101382341321,\n",
       " -0.1812047638690492,\n",
       " 0.1561292687386481,\n",
       " 0.1376889586567927,\n",
       " 0.009938005601319558,\n",
       " 0.03750354772136718,\n",
       " -0.20793603222098775,\n",
       " 0.19772464999300857,\n",
       " 0.1308876968143443,\n",
       " 0.07599231415406418,\n",
       " -0.12854209358942348,\n",
       " 0.2112423730483663,\n",
       " 0.20410942608805688,\n",
       " 0.12022451672077762,\n",
       " 0.03824176384256605,\n",
       " -0.12306424855432507,\n",
       " 0.3320062632748581,\n",
       " 0.1905096013028731,\n",
       " 0.04782337742412653,\n",
       " 0.2382104823289742,\n",
       " 0.544840337350808,\n",
       " -0.028343927428805107,\n",
       " 0.06327384530524406,\n",
       " 0.22036106733825933,\n",
       " -0.021399474078879606,\n",
       " 0.2446899508626764,\n",
       " 0.22129088839782102,\n",
       " 0.10446890786613042,\n",
       " 0.23778976964049092,\n",
       " 0.25331282435851366,\n",
       " 0.13672049370194853,\n",
       " 0.017486150189968058,\n",
       " 0.029461935518042704,\n",
       " 0.08141359981141821,\n",
       " 0.29548259406561755,\n",
       " 0.2715736821378854,\n",
       " 0.16974700610798366,\n",
       " 0.2107707128181107,\n",
       " 0.17301826695884973,\n",
       " 0.06332088273104279,\n",
       " 0.12840067795346094,\n",
       " -0.08449390510263187,\n",
       " 0.12205590850977258,\n",
       " 0.44952285305045325,\n",
       " 0.37421241327217136,\n",
       " 0.2051061889558278,\n",
       " 0.06566620402767888,\n",
       " 0.09326251704873609,\n",
       " -0.14013762876903085,\n",
       " 0.08668088730257723,\n",
       " 0.373436088886787,\n",
       " 0.15932147979565411,\n",
       " 0.07621839271068903,\n",
       " 0.06638810167660028,\n",
       " 0.09095896108481116,\n",
       " 0.33678133874453836,\n",
       " -0.01768661351314561,\n",
       " -0.10036965294854464,\n",
       " 0.1525152456041265,\n",
       " 0.08582793716013087,\n",
       " 0.11666667409887047,\n",
       " 0.0128890785409945,\n",
       " 0.18753160791292225,\n",
       " 0.27687805514800656,\n",
       " 0.3413028786036263,\n",
       " 0.07118518397084231,\n",
       " -0.143378769728772,\n",
       " 0.02287072059654017,\n",
       " -0.12461021779127483,\n",
       " 0.12953845628489788,\n",
       " 0.14089817882110414,\n",
       " 0.1776724889477589,\n",
       " 0.15755522798823005,\n",
       " 0.03127290791890805,\n",
       " 0.22134809976319444,\n",
       " -0.05315506285543391,\n",
       " -0.02181461809294772,\n",
       " 0.02835886079712106,\n",
       " -0.05937494335725747,\n",
       " 0.2515438463746661,\n",
       " 0.12442863005888723,\n",
       " 0.1577638613689461,\n",
       " -0.15930148227855367,\n",
       " -0.02082899605648682,\n",
       " -0.1512504693296423,\n",
       " 0.29606981454801384,\n",
       " 0.10755625219481765,\n",
       " 0.03330385375172495,\n",
       " -0.023479226812900395,\n",
       " 0.16789782529023725,\n",
       " 0.10512562562226126,\n",
       " 0.04837790732633843,\n",
       " 0.10695609732614925,\n",
       " 0.17802117709925264,\n",
       " 0.33247889155880506,\n",
       " -0.21752708468471924,\n",
       " 0.21970133128613867,\n",
       " 0.011515412420560074,\n",
       " 0.3838353894915109,\n",
       " 0.1537798690626593,\n",
       " 0.1444542659089221,\n",
       " -6.639472286830206e-05,\n",
       " 0.08406436997732107,\n",
       " 0.11688197421385874,\n",
       " -0.15595637493577041,\n",
       " -0.10385901083887852,\n",
       " 0.23584636700344563,\n",
       " 0.17652533465713693,\n",
       " 0.1679745114324188,\n",
       " 0.12036427889292209,\n",
       " 0.2415572683353623,\n",
       " 0.35852345093858773,\n",
       " 0.12439235364139249,\n",
       " 0.07050461609200953,\n",
       " 0.5475601168498899,\n",
       " 0.2602868007673385,\n",
       " -0.033708257908290316,\n",
       " 0.22475839074469325,\n",
       " 0.0736982344570432,\n",
       " 0.2449116903959098,\n",
       " 0.10201500737299365,\n",
       " 0.2595952909540144,\n",
       " 0.09620440241417606,\n",
       " 0.077856820931337,\n",
       " -0.10107221691528291,\n",
       " -0.03264459074643633,\n",
       " 0.11560062563745517,\n",
       " 0.2030598365257335,\n",
       " 0.11137100123425561,\n",
       " 0.11162066222069746,\n",
       " 0.0518569727214796,\n",
       " 0.16683447623268793,\n",
       " 0.1065729859112796,\n",
       " 0.15683751984295632,\n",
       " 0.24718073070318608,\n",
       " 0.2091763437556031,\n",
       " 0.02288106097262867,\n",
       " -0.13511792061766406,\n",
       " 0.2402812670123842,\n",
       " 0.20257751593501228,\n",
       " 0.1724859673565842,\n",
       " 0.13630837733929174,\n",
       " -0.04848323159133962,\n",
       " 0.06698260454071883,\n",
       " -0.12646684456847332,\n",
       " 0.07075845266136378,\n",
       " 0.17248882103062144,\n",
       " 0.2550298260804483,\n",
       " -0.21809267495975948,\n",
       " 0.01942530117357805,\n",
       " 0.3068531123088535,\n",
       " 0.5124944231575134,\n",
       " 0.0780403553239201,\n",
       " -0.02656911623022151,\n",
       " 0.1573257860412644,\n",
       " 0.05786293410165473,\n",
       " 0.3969629930982773,\n",
       " 0.01576462615078614,\n",
       " 0.14506659947859954,\n",
       " 0.14797079106398064,\n",
       " 0.19252722897260227,\n",
       " 0.3873489669680861,\n",
       " 0.12984322674631343,\n",
       " 0.2427000789837687,\n",
       " -0.18398051358186732,\n",
       " 0.03941582423076545,\n",
       " 0.08005824124654394,\n",
       " 0.22421589039484588,\n",
       " 0.060811534183111686,\n",
       " 0.19284329267463007,\n",
       " -0.037391484738072,\n",
       " 0.18548593283745232,\n",
       " -0.17419712827813325,\n",
       " 0.13282323204657454,\n",
       " 0.07924683734427518,\n",
       " 0.032855099103259684,\n",
       " 0.012475943911722796,\n",
       " 0.3093200606231919,\n",
       " -0.015539957879853973,\n",
       " 0.12045480473055051,\n",
       " -0.11407022834434288,\n",
       " -0.043148707879536834,\n",
       " -0.18248889605539095,\n",
       " 0.24242788888969208,\n",
       " -0.22294427378505963,\n",
       " 0.2650714517239829,\n",
       " 0.02655208458958364,\n",
       " 0.22524093189750874,\n",
       " 0.12121609719908895,\n",
       " 0.13053981059400294,\n",
       " 0.08167774314174414,\n",
       " 0.11554533305676819,\n",
       " -0.007533527813227326,\n",
       " 0.27109282436273663,\n",
       " 0.009106130764000346,\n",
       " 0.14918174983567786,\n",
       " 0.26198018043234506,\n",
       " -0.1255586159620457,\n",
       " 0.33122968144462217,\n",
       " 0.1728119867709589,\n",
       " 0.15542317249059345,\n",
       " -0.055971801928144424,\n",
       " -0.2662737039906192,\n",
       " 0.2788741006520297,\n",
       " -0.07594907447228337,\n",
       " -0.2137033838203438,\n",
       " -0.02030676532958588,\n",
       " 0.23453797998663253,\n",
       " 0.005618816850736424,\n",
       " 0.3071053546245436,\n",
       " 0.08128026003878566,\n",
       " 0.3571862162798377,\n",
       " -0.00746773182748257,\n",
       " 0.13269914015130105,\n",
       " 0.18302554744344093,\n",
       " 0.030462325659112832,\n",
       " 0.1105166650332849,\n",
       " -0.053031645205971645,\n",
       " 0.11010694495278259,\n",
       " 0.1757742697339781,\n",
       " 0.335386320025464,\n",
       " -0.09732083472133948,\n",
       " -0.018486701822768726,\n",
       " 0.3011711783427589,\n",
       " 0.16043036544445613,\n",
       " 0.16883004435897292,\n",
       " -0.063108539875637,\n",
       " 0.30112338082167,\n",
       " 0.034611822453380243,\n",
       " 0.09152942068800377,\n",
       " 0.22408661735190233,\n",
       " -0.10603582492198174,\n",
       " 0.2994974416759276,\n",
       " 0.22729721059151758,\n",
       " 0.032809507230396634,\n",
       " 0.023212586445658257,\n",
       " 0.304693682075389,\n",
       " 0.028512476401237743,\n",
       " 0.4101188670156991,\n",
       " -0.11626283794519665,\n",
       " -0.337852232349038,\n",
       " 0.309707109391545,\n",
       " 0.010024709692244704,\n",
       " 0.06649105541245613,\n",
       " 0.3438196762938143,\n",
       " -0.007321714965879809,\n",
       " 0.19677549308144318,\n",
       " 0.13058697862839763,\n",
       " -0.0675164841267957,\n",
       " 0.5077700443456206,\n",
       " -0.05854692826979935,\n",
       " 0.10063229962921517,\n",
       " 0.04951725744601544,\n",
       " -0.04976339932754888,\n",
       " 0.2922257190335988,\n",
       " 0.08068626638845157,\n",
       " -0.03083853964538968,\n",
       " 0.6608443302040862,\n",
       " 0.15771321636010502,\n",
       " 0.35777462951077166,\n",
       " 0.2168322931274055,\n",
       " -0.06996429385681624,\n",
       " -0.015066218784645122,\n",
       " -0.11414129073553035,\n",
       " 0.09295540028966263,\n",
       " -0.024242860052562584,\n",
       " 0.20431106361799323,\n",
       " 0.07762489151489949,\n",
       " 0.3596192693441032,\n",
       " 0.18410161810230305,\n",
       " 0.1351850123851162,\n",
       " 0.4190240125709298,\n",
       " 0.15526664900797948,\n",
       " 0.44791250247519565,\n",
       " 0.24420647001615264,\n",
       " -0.07706927888637075,\n",
       " 0.5201540954939357,\n",
       " 0.3365391876272027,\n",
       " 0.1986615418238484,\n",
       " 0.13426466656949546,\n",
       " -0.18801582841718417,\n",
       " 0.15189552980675022,\n",
       " 0.33139841783281615,\n",
       " -0.15791425892788458,\n",
       " 0.1887799699297267,\n",
       " 0.18893447151494852,\n",
       " 0.14025016990412784,\n",
       " 0.10035205542514644,\n",
       " -0.1466754833664922,\n",
       " 0.03739357032276078,\n",
       " 0.1138665752305943,\n",
       " -0.17740845665072755,\n",
       " 0.13940679420962823,\n",
       " 0.17461658332352278,\n",
       " 0.1709411526900633,\n",
       " 0.1730338592134777,\n",
       " -0.08762717780889638,\n",
       " 0.08887657667213043,\n",
       " 0.5076324574838761,\n",
       " 0.07420890069282017,\n",
       " 0.03397659901941123,\n",
       " 0.010321956583735074,\n",
       " -0.05028248079580376,\n",
       " -0.055366649037143195,\n",
       " 0.23616796545669425,\n",
       " -0.024937025021964383,\n",
       " 0.207965179155033,\n",
       " 0.035296435969826734,\n",
       " 0.13359315853246,\n",
       " 0.16536889346061526,\n",
       " -0.19282081157276318,\n",
       " 0.36603234027828563,\n",
       " 0.2898630794302561,\n",
       " 0.07214011146848069,\n",
       " 0.15076754341552273,\n",
       " 0.056783708943794364,\n",
       " 0.4853804924110166,\n",
       " -0.027730356595486432,\n",
       " 0.004926125410417199,\n",
       " 0.05232218337572343,\n",
       " -0.05975341321217474,\n",
       " 0.30231305983355156,\n",
       " 0.22046692425060022,\n",
       " 0.08022331041837975,\n",
       " 0.08455334670306613,\n",
       " -0.12979190856304038,\n",
       " -0.05168548519845964,\n",
       " 0.49174035622622514,\n",
       " 0.29963178944756624,\n",
       " 0.36861563868499714,\n",
       " 0.3058189490497693,\n",
       " -0.045019340142037866,\n",
       " 0.04873500222567414,\n",
       " 0.22007839707811339,\n",
       " 0.15105486511821925,\n",
       " 0.31263870575329383,\n",
       " 0.14820747517216326,\n",
       " 0.3528935464479236,\n",
       " 0.017071855494893064,\n",
       " -0.11144167780116018,\n",
       " 0.04930685051277289,\n",
       " 0.07886910417879726,\n",
       " -0.05850542950746593,\n",
       " 0.18497392878810978,\n",
       " 0.1583179394779342,\n",
       " 0.3311058759419597,\n",
       " -0.022210472203961665,\n",
       " 0.14153285570665405,\n",
       " 0.156623664764371,\n",
       " 0.010066912921151062,\n",
       " -0.1376374066324313,\n",
       " 0.22282403304658116,\n",
       " 0.14222187556419308,\n",
       " -0.032694885715607705,\n",
       " 0.19794060339643088,\n",
       " 0.4611558634888257,\n",
       " 0.011824296353100583,\n",
       " -0.06876703362760611,\n",
       " 0.2516812385803359,\n",
       " 0.20626585731052172,\n",
       " 0.01622909296958443,\n",
       " 0.14080350439629388,\n",
       " -0.007365992975315639,\n",
       " -0.24229098305475874,\n",
       " 0.01674337117707355,\n",
       " 0.1524243462836295,\n",
       " 0.3988911950225655,\n",
       " 0.19026939846055446,\n",
       " -0.07162486420071035,\n",
       " -0.050655774223400264,\n",
       " 0.21541216488309176,\n",
       " 0.3178890422994466,\n",
       " -0.09018523400531801,\n",
       " 0.30878051812037793,\n",
       " 0.2447852228670045,\n",
       " 0.2026795474165283,\n",
       " 0.21912085354773897,\n",
       " 0.15516963022500993,\n",
       " -0.07689215115480316,\n",
       " 0.04153590891798564,\n",
       " 0.16182911599717603,\n",
       " 0.010158545171331615,\n",
       " 0.10710603228150897,\n",
       " -0.1528914797158921,\n",
       " 0.1692695316078218,\n",
       " -0.01358224673327338,\n",
       " -0.18601157825964498,\n",
       " -0.190245369484734,\n",
       " 0.030999890842846777,\n",
       " 0.10676277050020613,\n",
       " 0.13447384498385687,\n",
       " 0.20809511954350912,\n",
       " -0.08020687967088162,\n",
       " 0.4702401051104769,\n",
       " -0.011907778275778652,\n",
       " 0.04605514778937423,\n",
       " 0.20419086186051824,\n",
       " 0.10410223067353208,\n",
       " 0.36156607195627316,\n",
       " 0.6771118878127973,\n",
       " 0.018269523727911727,\n",
       " 0.21158820873012646,\n",
       " -0.07825409641021884,\n",
       " -0.11673423417754698,\n",
       " 0.06915714728875508,\n",
       " 0.37635582962343783,\n",
       " 0.2507421443502459,\n",
       " 0.15087240518402417,\n",
       " -0.03788123208277861,\n",
       " 0.09576475251668297,\n",
       " 0.03907409353239317,\n",
       " 0.37438587653382666,\n",
       " 0.13464276768324546,\n",
       " 0.2042817970007998,\n",
       " 0.20000377166801375,\n",
       " 0.32092030693592133,\n",
       " -0.061997325952440736,\n",
       " -0.1501060301422386,\n",
       " 0.17074730375105399,\n",
       " 0.07042134936925268,\n",
       " -0.16301761409214885,\n",
       " -0.08448913174575201,\n",
       " -0.16574845941549587,\n",
       " 0.09741956164907926,\n",
       " 0.02294303057553711,\n",
       " 0.17987248928388028,\n",
       " 0.42130307808293027,\n",
       " -0.21295379828642272,\n",
       " 0.18613729106564714,\n",
       " 0.2103717473577757,\n",
       " 0.5863423383782199,\n",
       " 0.14235744860465938,\n",
       " 0.03294765853601443,\n",
       " 0.15902268948289683,\n",
       " 0.19473855733495649,\n",
       " -0.019520253759460655,\n",
       " -0.05716763957987702,\n",
       " 0.08091980828224742,\n",
       " 0.16207794040041648,\n",
       " 0.14475204959116528,\n",
       " 0.2862498854907231,\n",
       " 0.11569033459386474,\n",
       " -0.11358303485005229,\n",
       " 0.10635689226917895,\n",
       " -0.09793765571194275,\n",
       " -0.05058921870604594,\n",
       " -0.19540362869021582,\n",
       " 0.35107521741877884,\n",
       " -0.065368323822944,\n",
       " -0.12927706827985985,\n",
       " -0.17300779481345357,\n",
       " 0.1798637359111972,\n",
       " 0.11887922386099004,\n",
       " 0.36227459883720703,\n",
       " 0.16848724103256651,\n",
       " -0.06299473049762933,\n",
       " 0.12196707063357168,\n",
       " -0.09264093898857696,\n",
       " -0.1280848519507317,\n",
       " 0.27398050828186776,\n",
       " -0.08278024327590335,\n",
       " 0.40800331770380993,\n",
       " 0.13082125686423063,\n",
       " 0.1099785567815408,\n",
       " 0.16015285371844773,\n",
       " 0.2002503861731454,\n",
       " -0.027069347000118916,\n",
       " -0.044117262477851193,\n",
       " 0.2054656687707574,\n",
       " 0.17336060276816073,\n",
       " 0.3061008968400302,\n",
       " -0.1146345001381899,\n",
       " 0.45217224606775963,\n",
       " -0.23085806963765879,\n",
       " 0.24741941263764097,\n",
       " 0.0008386003817796449,\n",
       " 0.3416129664009122,\n",
       " 0.03402795685102342,\n",
       " 0.3898999199719596,\n",
       " -0.024257139705149172,\n",
       " 0.13614006238478454,\n",
       " -0.06709108717268093,\n",
       " 0.14667420993581856,\n",
       " 0.34977379736272557,\n",
       " 0.10469732761593346,\n",
       " 0.2568665091939558,\n",
       " 0.19424107547801978,\n",
       " 0.24177997547419655,\n",
       " 0.30235890865804455,\n",
       " 0.07192470709485937,\n",
       " 0.07480476126156202,\n",
       " -0.0683106298957859,\n",
       " 0.19784071840774214,\n",
       " 0.25828842550095577,\n",
       " 0.406340885150953,\n",
       " 0.015100692145591121,\n",
       " 0.07613864839459167,\n",
       " 0.014130018094795168,\n",
       " 0.03797011937508997,\n",
       " 0.34836255280512207,\n",
       " 0.2716801620559904,\n",
       " 0.11186347462548077,\n",
       " -0.029563046594331573,\n",
       " 0.1809531353910802,\n",
       " 0.2075974355068713,\n",
       " 0.2427940524354508,\n",
       " 0.14510244259110638,\n",
       " 0.06889076035397822,\n",
       " 0.044062147481979556,\n",
       " 0.11083605967322227,\n",
       " 0.348924016597724,\n",
       " 0.021758993507765095,\n",
       " 0.05201755694177046,\n",
       " -0.039634286368793924,\n",
       " 0.27922136840556083,\n",
       " 0.005875196618760438,\n",
       " 0.07273394507694893,\n",
       " 0.03813210809495761,\n",
       " 0.14191884701933621,\n",
       " 0.0035367860929308385,\n",
       " 0.047686580730498564,\n",
       " 0.28758601972296294,\n",
       " -0.08347186927395049,\n",
       " 0.02935691280990982,\n",
       " 0.16054188914823725,\n",
       " 0.09615271109349151,\n",
       " 0.15789902754756438,\n",
       " -0.004967066095973875,\n",
       " 0.06733389270521287,\n",
       " 0.037016881075405315,\n",
       " 0.04094031633475141,\n",
       " 0.09942825909946063,\n",
       " 0.016335699900508052,\n",
       " -0.0015544647629628647,\n",
       " 0.008597670758569705,\n",
       " -0.002176836685921271,\n",
       " 0.22234201043723947,\n",
       " -0.03085905808959332,\n",
       " -0.030991301630964266,\n",
       " 0.18997552149048433,\n",
       " 0.21671445175208454,\n",
       " 0.0818780667900943,\n",
       " -0.09683870819329828,\n",
       " 0.22407665066659666,\n",
       " 0.5495968827314313,\n",
       " 0.055067694502863807,\n",
       " 0.15302164571129123,\n",
       " -0.11837294777136735,\n",
       " -0.10051431955820331,\n",
       " -0.012924845187828195,\n",
       " 0.15426176593195054,\n",
       " 0.13384041009165532,\n",
       " 0.06955390179359008,\n",
       " 0.3685991267802809,\n",
       " 0.1006435524170567,\n",
       " -0.10570097642185461,\n",
       " 0.15494762800576975,\n",
       " 0.05069223563857845,\n",
       " 0.2546028720535086,\n",
       " -0.14106462428075592,\n",
       " 0.10893500703571998,\n",
       " 0.09166146765066,\n",
       " 0.1087519704036268,\n",
       " -0.03887810926445346,\n",
       " 0.15220173943662052,\n",
       " 0.30002866090664915,\n",
       " 0.16901705953632162,\n",
       " 0.18203897052471973,\n",
       " 0.07112900469677148,\n",
       " -0.02960998672362243,\n",
       " 0.2605931105415796,\n",
       " 0.2372281253561804,\n",
       " 0.13741539933361918,\n",
       " 0.11899788188029081,\n",
       " 0.3758226750547722,\n",
       " 0.3440184011490471,\n",
       " -0.04094144074339313,\n",
       " -0.06425306693311178,\n",
       " 0.2379973761410746,\n",
       " 0.01389782008273074,\n",
       " 0.14121408801836527,\n",
       " 0.11886943279404207,\n",
       " 0.2607525645784839,\n",
       " -0.05823402591173763,\n",
       " 0.019189925869506862,\n",
       " 0.051449687713066544,\n",
       " 0.020177127136637826,\n",
       " -0.19070470923184973,\n",
       " -0.13205258035693682,\n",
       " 0.18654980013892994,\n",
       " 0.33759097514869757,\n",
       " 0.12893169520949407,\n",
       " 0.12309103052793413,\n",
       " -0.05024660687600807,\n",
       " 0.0175279229954431,\n",
       " -0.2286058950561579,\n",
       " 0.14997984491325417,\n",
       " 0.08147471577559556,\n",
       " -0.10020076689573325,\n",
       " -0.03714028008879904,\n",
       " 0.015151207276509562,\n",
       " 0.39482308662195625,\n",
       " -0.04789299641448143,\n",
       " -0.25013360092930614,\n",
       " 0.1787753092482592,\n",
       " 0.04790370000969048,\n",
       " 0.04196780341675759,\n",
       " 0.18991849503802985,\n",
       " 0.5317341171360318,\n",
       " 0.1955602150082606,\n",
       " 0.220207904964997,\n",
       " 0.1634550236367286,\n",
       " 0.17417435236269885,\n",
       " 0.0956843004996073,\n",
       " -0.024895604754612097,\n",
       " 0.5393770481960763,\n",
       " 0.24398637501545017,\n",
       " 0.018073345187709117,\n",
       " 0.11531400366865453,\n",
       " -0.09224489952607866,\n",
       " 0.11909156594609585,\n",
       " 0.38875669100179466,\n",
       " -0.09840610069904485,\n",
       " -0.16180659365342434,\n",
       " 0.06900980033027446,\n",
       " 0.38817244671924184,\n",
       " -0.028973452016178326,\n",
       " 0.16024027328764362,\n",
       " -0.04645962958230354,\n",
       " 0.027549400017176505,\n",
       " -0.1805275041444589,\n",
       " 0.008892327830072332,\n",
       " 0.27717500214662283,\n",
       " 0.11573329539166242,\n",
       " 0.21790746759559265,\n",
       " -0.1939433434641493,\n",
       " 0.15118960969256015,\n",
       " 0.17307135324437917,\n",
       " 0.08727487865844616,\n",
       " 0.4284938882171613,\n",
       " 0.04137525017322321,\n",
       " 0.21935204675916234,\n",
       " 0.12602336084902238,\n",
       " -0.01730275754467481,\n",
       " 0.2896076466595566,\n",
       " 0.5073957527129651,\n",
       " 0.2427904819453028,\n",
       " 0.13210773153958752,\n",
       " -0.02266850782372592,\n",
       " 0.23953081262766437,\n",
       " 0.028346117380460535,\n",
       " -0.001112723760615592,\n",
       " 0.08202209350194789,\n",
       " 0.1178213644717271,\n",
       " -0.21862779210124883,\n",
       " 0.2928608946906116,\n",
       " 0.20287353870698427,\n",
       " 0.30089880075770514,\n",
       " -0.14374286986759788,\n",
       " 0.3477467862899246,\n",
       " 0.19337542470203517,\n",
       " 0.18752457080795357,\n",
       " 0.0972560353883481,\n",
       " 0.10520205137507278,\n",
       " 0.3348070418901937,\n",
       " 0.5127168058847701,\n",
       " 0.4656141106221975,\n",
       " 0.1997495300801118,\n",
       " 0.06449212413698792,\n",
       " 0.21651372285447346,\n",
       " -0.030788255026036396,\n",
       " 0.012898473571654042,\n",
       " 0.0522643320506511,\n",
       " 0.22854903326746615,\n",
       " 0.023155639123283486,\n",
       " 0.17832153668198195,\n",
       " -0.22993961879425556,\n",
       " 0.03025181875015135,\n",
       " -0.0593051745038912,\n",
       " 0.23058559395584408,\n",
       " 0.100450805356028,\n",
       " 0.15528868995239656,\n",
       " 0.2433667538616393,\n",
       " -0.033009989765962235,\n",
       " 0.14435298756463555,\n",
       " -0.13888176545105876,\n",
       " 0.4269415989382412,\n",
       " 0.16641602869321037,\n",
       " 0.13683220753251416,\n",
       " 0.10311341405784516,\n",
       " 0.1391563649474359,\n",
       " 0.17504935485937978,\n",
       " 0.04879360736687759,\n",
       " 0.19960659158899646,\n",
       " 0.21507944177147706,\n",
       " 0.026386205008980253,\n",
       " 0.19573049919096938,\n",
       " 0.038965531055250864,\n",
       " 0.15096213935388483,\n",
       " 0.15171579238796196,\n",
       " -0.17850784687306936,\n",
       " 0.26468021159756855,\n",
       " 0.3347991493852781,\n",
       " 0.009434594191475834,\n",
       " -0.014015222327330986,\n",
       " 0.14770665535540556,\n",
       " 0.13553401090928002,\n",
       " 0.19734320603917319,\n",
       " 0.27732512158044653,\n",
       " 0.09258759801863287,\n",
       " 0.3335972030560993,\n",
       " 0.08638381544280789,\n",
       " 0.33832388821410064,\n",
       " 0.15330336422383375,\n",
       " 0.21490842360619103,\n",
       " -0.010480642840496551,\n",
       " 0.14332510934494871,\n",
       " 0.1480731797875077,\n",
       " 0.28523408352693713,\n",
       " -0.06756657722257622,\n",
       " 0.06792361546423084,\n",
       " 0.35780297122424093,\n",
       " -0.12008478671718956,\n",
       " 0.1112393749076911,\n",
       " -0.07373189834430849,\n",
       " -0.03370536581028739,\n",
       " 0.2256711093021563,\n",
       " -0.11388543285373476,\n",
       " -0.12901940327766395,\n",
       " -0.11877433089046441,\n",
       " -0.04052481379627437,\n",
       " -0.21479687067545009,\n",
       " 0.08201596199691596,\n",
       " 0.04651475381468666,\n",
       " 0.19897315881451375,\n",
       " 0.221177424926378,\n",
       " 0.22368638614228237,\n",
       " -0.08114856773283974,\n",
       " 0.27551415841267257,\n",
       " 0.2793267794815526,\n",
       " 0.09307605105797896,\n",
       " 0.15658881299301733,\n",
       " 0.06649073660747236,\n",
       " -0.07167668132685051,\n",
       " 0.1327398760495914,\n",
       " 0.06121263952326505,\n",
       " 0.024339341261084273,\n",
       " 0.4058480151178999,\n",
       " 0.18813728828537865,\n",
       " -0.043758721570582935,\n",
       " 0.5117170711418606,\n",
       " 0.12181434224640242,\n",
       " 0.1392330465674444,\n",
       " 0.10073953795617943,\n",
       " 0.3663051880829682,\n",
       " 0.08281026350068064,\n",
       " 0.2621049988951589,\n",
       " 0.17006343776031052,\n",
       " -0.03167547423572159,\n",
       " 0.13704867226534953,\n",
       " 0.027464110852197833,\n",
       " -0.02620532831821383,\n",
       " 0.26841456588884116,\n",
       " 0.07530350804450149,\n",
       " 0.08364841242551875,\n",
       " 0.08657824278770547,\n",
       " 0.058604071184141106,\n",
       " 0.07075643936137609,\n",
       " 0.34171496185667444,\n",
       " 0.4521051476825894,\n",
       " 0.23735696368669246,\n",
       " 0.1305528869600219,\n",
       " 0.2620873982202511,\n",
       " 0.1617672427681931,\n",
       " 0.14541962039979464,\n",
       " 0.32530491686225854,\n",
       " 0.1760643378224195,\n",
       " 0.03927443159774441,\n",
       " 0.06808954185093041,\n",
       " 0.13739961191974412,\n",
       " 0.09713037477064977,\n",
       " -0.07787544141026519,\n",
       " 0.34796355871693824,\n",
       " 0.29177114980108615,\n",
       " 0.2925980225231348,\n",
       " 0.19810204317361052,\n",
       " 0.17757764757321115,\n",
       " 0.16324271247783853,\n",
       " -0.03430408093658423,\n",
       " 0.18702728020780723,\n",
       " 0.08456697628454869,\n",
       " 0.056649198528668926,\n",
       " -0.03819981699917103,\n",
       " 0.005981259850879753,\n",
       " -0.019373421607073057,\n",
       " -0.02569756805544131,\n",
       " -0.013435968201206834,\n",
       " 0.019389279147283386,\n",
       " 0.2928288926725794,\n",
       " -0.0004308001492035008,\n",
       " 0.24782780970277793,\n",
       " 0.09571769692157306,\n",
       " 0.170438225726115,\n",
       " 0.17232169467172354,\n",
       " 0.1843988517811413,\n",
       " 0.17760841795462487,\n",
       " -3.482018706666622e-05,\n",
       " 0.03512504605386017,\n",
       " 0.09034370905400611,\n",
       " 0.05607232163677992,\n",
       " 0.1333255252215619,\n",
       " 0.19369412496872127,\n",
       " 0.02751820971668861,\n",
       " 0.15054331771432222,\n",
       " 0.3437732405172257,\n",
       " 0.2636297096399129,\n",
       " 0.06874533841298604,\n",
       " 0.09140983482867826,\n",
       " 0.11505025329147592,\n",
       " -0.08615201630500545,\n",
       " 0.23943800908366608,\n",
       " -0.009215126875557724,\n",
       " 0.05793652104966359,\n",
       " -0.03776846095502267,\n",
       " 0.1523121597527311,\n",
       " -0.08580917812971584,\n",
       " 0.39501571549190617,\n",
       " -0.05311187562933212,\n",
       " 0.08614664099945556,\n",
       " -0.15676583623320048,\n",
       " 0.15481550172316003,\n",
       " 0.1522068028173824,\n",
       " 0.04251566729499986,\n",
       " 0.09093840477845075,\n",
       " 0.3607828934285283,\n",
       " 0.3019148260188345,\n",
       " 0.17268245337423752,\n",
       " 0.24894741002147958,\n",
       " -0.07261364672941456,\n",
       " 0.028775935316540248,\n",
       " -0.006309459053868046,\n",
       " -0.0420542674872294,\n",
       " 0.16271301225183563,\n",
       " -0.0013889079419671798,\n",
       " -0.2534076259836739,\n",
       " 0.15867790198352952,\n",
       " 0.07779643416689769,\n",
       " 0.17677795282307654,\n",
       " 0.09751069952296262,\n",
       " -0.1347544612469256,\n",
       " 0.20707393292235168,\n",
       " 0.051346364728391124,\n",
       " 0.27842012950405215,\n",
       " 0.05833767942625315,\n",
       " -0.014306336364895785,\n",
       " 0.0780627201977894,\n",
       " -0.15047224145381155,\n",
       " 0.1958595427470023,\n",
       " 0.36546862518353007,\n",
       " 0.14409375111886322,\n",
       " 0.05330253241712006,\n",
       " 0.05696251940445952,\n",
       " 0.14253951178706215,\n",
       " 0.20061808551366328,\n",
       " 0.0983652199234356,\n",
       " 0.32109889886119336,\n",
       " -0.17541562838217528,\n",
       " -0.005972524275274785,\n",
       " 0.1851005179844022,\n",
       " 0.3261841881207447,\n",
       " 0.12373584482395482,\n",
       " 0.15630256498879772,\n",
       " 0.1442785070117491,\n",
       " 0.23357754628451163,\n",
       " 0.3424072037537845,\n",
       " 0.08042186955051973,\n",
       " 0.1900046781963458,\n",
       " 0.3873497263507636,\n",
       " 0.1830300708009012,\n",
       " -0.07556467662886433,\n",
       " 0.12855460991837186,\n",
       " 0.35353189464283735,\n",
       " -0.014380984429740994,\n",
       " -0.0971019778700891,\n",
       " -0.1331913069209393,\n",
       " 0.16868661390473028,\n",
       " 0.18358353616045908,\n",
       " 0.28311518262980795,\n",
       " 0.07573521367500717,\n",
       " 0.1505163959572426,\n",
       " 0.13104674303638322,\n",
       " -0.09203439492743241,\n",
       " 0.19465698812185162,\n",
       " 0.26426312132060253,\n",
       " 0.25578871862084257,\n",
       " 0.3360431966869705,\n",
       " 0.25512767468455455,\n",
       " -0.22738788965119228,\n",
       " 0.11997276178523969,\n",
       " 0.19956107634475728,\n",
       " 0.04830936387933874,\n",
       " 0.034514454380391926,\n",
       " -0.003073997654505119,\n",
       " -0.01114097736692308,\n",
       " 0.185014768321436,\n",
       " 0.015235780605748098,\n",
       " 0.09938506360991108,\n",
       " -0.03909094811615013,\n",
       " 0.23234623110031363,\n",
       " 0.09754302834067952,\n",
       " 0.4086007705664589,\n",
       " 0.3083841154737368,\n",
       " -0.03895484311821021,\n",
       " 0.09490007075062502,\n",
       " -0.0583300781243626,\n",
       " 0.12103683763438797,\n",
       " -0.11711152357696887,\n",
       " -0.09173707919631097,\n",
       " 0.24207276608343117,\n",
       " 0.05844200338484452,\n",
       " 0.2603194241123009,\n",
       " 0.0917152378117314,\n",
       " 0.34574856882186994,\n",
       " 0.13250697741992537,\n",
       " 0.20672031972369645,\n",
       " 0.2255199834544799,\n",
       " 0.13646283757766514,\n",
       " 0.3841494225833926,\n",
       " 0.22560772130741938,\n",
       " -0.031032051651852518,\n",
       " 0.49037598883653394,\n",
       " 0.23651585580351567,\n",
       " -0.06341426437141046,\n",
       " 0.2635582321188985,\n",
       " 0.45809215328966335,\n",
       " 0.043053840912848314,\n",
       " 0.34046735323932553,\n",
       " -0.039122116165607124,\n",
       " 0.2118654625641486,\n",
       " -0.10722436426032894,\n",
       " 0.16732038314188147,\n",
       " 0.2775699778851169,\n",
       " -0.08577371711145476,\n",
       " 0.3456676254339213,\n",
       " -0.08029565214412017,\n",
       " 0.07092326768825465,\n",
       " -0.031161873292173722,\n",
       " 0.19441450943506944,\n",
       " -0.11340276275407929,\n",
       " 0.137794115451794,\n",
       " 0.10261700535754915,\n",
       " -0.01411013513691048,\n",
       " 0.1124638681006216,\n",
       " 0.09407300316380732,\n",
       " 0.13522810627497037,\n",
       " -0.1080378842103112,\n",
       " 0.094265431861633,\n",
       " 0.21650522847868625,\n",
       " -0.04955065077561323,\n",
       " 0.161270508995918,\n",
       " 0.0347562657418403,\n",
       " -0.13826161948644752,\n",
       " 0.07430734931601356,\n",
       " -0.0515375797968092,\n",
       " 0.1788464748622726,\n",
       " 0.08317438614731198,\n",
       " 0.10122296078542804,\n",
       " -0.07704825350409895,\n",
       " 0.189277591695967,\n",
       " 0.1865527136596898,\n",
       " -0.10025003858151135,\n",
       " 0.32995759510280576,\n",
       " -0.07888306583268095,\n",
       " -0.09203834595754026,\n",
       " -0.05379822362889101,\n",
       " -0.000988007654164865,\n",
       " 0.14020193105332576,\n",
       " 0.2130671513789217,\n",
       " -0.08488327656721234,\n",
       " 0.13790999943192284,\n",
       " 0.164814568615321,\n",
       " 0.21396109077771186]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# similarity_cosine(np.random.choice(baseline_1_train_encoding_arr),np.random.choice(baseline_1_train_encoding_arr))[0]\n",
    "aaa\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "cc59d195",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94c59a21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Window: [2017]\n",
      "Label Window: [2018 2019 2020]\n",
      "Finished Positives\n",
      "Finished Negatives\n",
      "Positive: Expect to be years:  [2018 2019 2020]\n",
      "Negative: Expect to be 0:  0\n",
      "Training Window: [2020]\n",
      "Label Window: [2021 2022 2023]\n",
      "Finished Positives\n",
      "Finished Negatives\n",
      "Positive: Expect to be years:  [2021 2022 2023]\n",
      "Negative: Expect to be 0:  0\n"
     ]
    }
   ],
   "source": [
    "def get_co_occur_concept_pair_after_year_arr(word_co_occurrences: dict, first_occ_year: int, final_occ_year: int) -> np.ndarray:\n",
    "    co_occur_concept_pair_arr = []\n",
    "    for concept, v in word_co_occurrences.items():\n",
    "        for co_concept, years in v.items():\n",
    "            if np.min(years) >= first_occ_year and np.max(years) <= final_occ_year:\n",
    "                co_occur_concept_pair_arr.append([concept,co_concept])\n",
    "    return np.array(co_occur_concept_pair_arr)\n",
    "\n",
    "def _get_years_range(start: int, end: int) -> np.ndarray:\n",
    "        return (np.unique(year_arr)[start:] if end == -0 \n",
    "                                 else np.unique(year_arr)[start:end])\n",
    "\n",
    "def create_dataset_indexing(data: np.ndarray, word_co_occurrences: dict, year_arr: np.ndarray, c_inx_arr: np.ndarray, \n",
    "                 input_window_size: int = 5, output_window_size: int = 3, offset_to_current_year: int = 1):\n",
    "    \"\"\"\n",
    "    Dataset indexing for time series data.\n",
    "\n",
    "    Args:\n",
    "        data (np.ndarray): The input data.\n",
    "        word_co_occurrences (dict): Dictionary of word co-occurrences.\n",
    "        year_arr (np.ndarray): Array of years.\n",
    "        c_inx_arr (np.ndarray): Array of concept indices.\n",
    "        input_window_size (int, optional): Size of the input window. Defaults to 5.\n",
    "        output_window_size (int, optional): Size of the output window. Defaults to 3.\n",
    "        offset_to_current_year (int, optional): Offset to the current year. Defaults to 1.\n",
    "        print_test (bool, optional): Whether to print test information. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Positive index pair array.\n",
    "        np.ndarray: Negative index pair array.\n",
    "    \"\"\"\n",
    "    train_window_data = data[:, -input_window_size-output_window_size-offset_to_current_year:-output_window_size-offset_to_current_year]\n",
    "    label_year_range = (year_arr[-output_window_size:] if offset_to_current_year == 0 \n",
    "                                else year_arr[-output_window_size-offset_to_current_year:-offset_to_current_year])\n",
    "\n",
    "    co_occur_concept_pair_arr = get_co_occur_concept_pair_after_year_arr(\n",
    "        word_co_occurrences, label_year_range[0], label_year_range[-1])\n",
    "    \n",
    "    print(f\"Training Window: {_get_years_range(-input_window_size-output_window_size-offset_to_current_year, -output_window_size-offset_to_current_year)}\")\n",
    "    print(f\"Label Window: {_get_years_range(-output_window_size-offset_to_current_year, -offset_to_current_year)}\")\n",
    "\n",
    "    # Precompute indices for each unique concept in c_inx_arr\n",
    "    concept_to_indices = {concept: np.where(c_inx_arr == concept)[0] for concept in np.unique(c_inx_arr)}\n",
    "\n",
    "    # Convert word_co_occurrences to a dictionary of sets for fast membership checking\n",
    "    word_co_occurrences_set = {k: set(v) for k, v in word_co_occurrences.items()}\n",
    "\n",
    "    pos_inx_pair_arr = np.zeros((len(co_occur_concept_pair_arr),2), dtype=int)\n",
    "    \n",
    "    for inx, pair in enumerate(co_occur_concept_pair_arr):\n",
    "        try:\n",
    "            pos_inx_pair_arr[inx, 0] = concept_to_indices[pair[0]][0]\n",
    "            pos_inx_pair_arr[inx, 1] = concept_to_indices[pair[1]][0]\n",
    "        except:\n",
    "            pos_inx_pair_arr[inx, 0] = 0\n",
    "            pos_inx_pair_arr[inx, 1] = 0\n",
    "            \n",
    "    \n",
    "    pos_inx_pair_arr = pos_inx_pair_arr[~((pos_inx_pair_arr[:, 0] == 0) & (pos_inx_pair_arr[:, 1] == 0))]\n",
    "\n",
    "\n",
    "    print(\"Finished Positives\")\n",
    "    \n",
    "    neg_inx_pair_arr = np.zeros((len(pos_inx_pair_arr), 2), dtype=int)\n",
    "    checked_pairs = set()\n",
    "    neg_inx = 0\n",
    "\n",
    "    while neg_inx < len(pos_inx_pair_arr):\n",
    "        sampled_pair = tuple(np.random.choice(train_window_data.shape[0], size=2, replace=False))\n",
    "        \n",
    "        # Ensure the sampled pair is not the same and hasn't been checked before\n",
    "        if sampled_pair not in checked_pairs:\n",
    "            checked_pairs.add(sampled_pair)\n",
    "            concept_0, concept_1 = c_inx_arr[sampled_pair[0]], c_inx_arr[sampled_pair[1]]\n",
    "\n",
    "            if concept_1 not in word_co_occurrences_set.get(concept_0, set()):\n",
    "                neg_inx_pair_arr[neg_inx, 0] = concept_to_indices[concept_0][0]\n",
    "                neg_inx_pair_arr[neg_inx, 1] = concept_to_indices[concept_1][0]\n",
    "                neg_inx += 1\n",
    "    print(\"Finished Negatives\")\n",
    "    \n",
    "    save_pos_arr = [word_co_occurrences[c_inx_arr[pos_inx_pair_arr[_][0]]][c_inx_arr[pos_inx_pair_arr[_][1]]] for _ in range(len(pos_inx_pair_arr))]\n",
    "    save_neg_arr = [c_inx_arr[neg_inx_pair_arr[_][1]] in word_co_occurrences[c_inx_arr[neg_inx_pair_arr[_][0]]] for _ in range(len(neg_inx_pair_arr))]\n",
    "\n",
    "    save_pos_arr = [x for xs in save_pos_arr for x in xs]\n",
    "    \n",
    "    print(\"Positive: Expect to be years: \", np.unique(save_pos_arr))\n",
    "    print(\"Negative: Expect to be 0: \", np.sum(save_neg_arr))\n",
    "\n",
    "\n",
    "    return pos_inx_pair_arr, neg_inx_pair_arr\n",
    "\n",
    "\n",
    "seq_length = 1\n",
    "out_length = 3\n",
    "batch_size = 128\n",
    "\n",
    "train_pos_inx_pair_arr_3_3, train_neg_inx_pair_arr_3_3 = create_dataset_indexing(data=baseline_1_train_encoding_arr, word_co_occurrences=word_co_occurrences, year_arr = np.unique(year_arr), \n",
    "                            c_inx_arr=baseline_1_train_c_arr, input_window_size = seq_length, output_window_size = out_length, offset_to_current_year = 3)\n",
    "\n",
    "test_pos_inx_pair_arr_3_0, test_neg_inx_pair_arr_3_0 = create_dataset_indexing(data=baseline_1_test_encoding_arr, word_co_occurrences=word_co_occurrences, year_arr = np.unique(year_arr), \n",
    "                            c_inx_arr=baseline_1_test_c_arr, input_window_size = seq_length, output_window_size = out_length, offset_to_current_year = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "8bef17fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def similarity_cosine(v1, v2):    \n",
    "#     return np.array([np.dot(v1, v2)/(np.linalg.norm(v1)* np.linalg.norm(v2))])\n",
    "\n",
    "def similarity_cosine(arr1: np.ndarray, arr2: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Compute the cosine similarity between two numpy arrays.\n",
    "\n",
    "    Parameters:\n",
    "    - arr1: np.ndarray\n",
    "    - arr2: np.ndarray\n",
    "\n",
    "    Returns:\n",
    "    - float: Cosine similarity between arr1 and arr2.\n",
    "    \"\"\"\n",
    "    # Check if both arrays are 1-Dimensional or 2-Dimensional.\n",
    "    if arr1.ndim != 1 or arr2.ndim != 1:\n",
    "        raise ValueError(\"Both input arrays must be 1-Dimensional.\")\n",
    "\n",
    "    # Compute the dot product of the arrays\n",
    "    dot_product = np.dot(arr1, arr2)\n",
    "\n",
    "    # Compute the magnitudes (norms) of the arrays\n",
    "    norm_arr1 = np.linalg.norm(arr1)\n",
    "    norm_arr2 = np.linalg.norm(arr2)\n",
    "\n",
    "    # Check for zero vectors to avoid division by zero\n",
    "    if norm_arr1 == 0 or norm_arr2 == 0:\n",
    "        raise ValueError(\"One or both of the vectors have zero magnitude.\")\n",
    "\n",
    "    # Compute the cosine similarity\n",
    "    cosine_sim = dot_product / (norm_arr1 * norm_arr2)\n",
    "    \n",
    "    return np.array([cosine_sim])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b6d4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_1_train_encoding_arr.shape\n",
    "aaa = []\n",
    "for i1,i2 in test_pos_inx_pair_arr_3_0:\n",
    "    aaa.append(similarity_cosine(baseline_1_test_encoding_arr[i1],baseline_1_test_encoding_arr[i2])[0])\n",
    "plt.hist(aaa)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "79084d30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9934, 128)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_1_train_encoding_arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "459db792",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomBaselineDataset(Dataset):\n",
    "    def __init__(self, train_window_data, pair_arr, labels, input_window_size, output_window_size, offset_to_current_year):\n",
    "        self.train_window_data = train_window_data#[:, -input_window_size-output_window_size-offset_to_current_year:-output_window_size-offset_to_current_year]\n",
    "        self.pair_arr = pair_arr\n",
    "        self.labels = labels\n",
    "        self.shape = self.train_window_data.shape\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pair_arr)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        inx_0, inx_1 = self.pair_arr[idx]\n",
    "        label = self.labels[idx]\n",
    "        enc_0 = self.train_window_data[inx_0]\n",
    "        enc_1 = self.train_window_data[inx_1]\n",
    "        enc_01 = similarity_cosine(enc_0, enc_1)\n",
    "        # print(similarity_cosine(self.train_window_data[inx_0], self.train_window_data[inx_1]))\n",
    "        return torch.from_numpy(enc_01), torch.tensor([label], dtype=torch.float32), inx_0, inx_1 \n",
    "\n",
    "def create_train_val_datasets(train_window_data, pos_inx_pair_arr, neg_inx_pair_arr, input_window_size, output_window_size, offset_to_current_year, test_size=0.2, random_state=42):\n",
    "    # Create labels\n",
    "    pos_labels = np.ones(len(pos_inx_pair_arr))\n",
    "    neg_labels = np.zeros(len(neg_inx_pair_arr))\n",
    "    \n",
    "    # Concatenate positive and negative pairs and labels\n",
    "    all_pairs = np.vstack((pos_inx_pair_arr, neg_inx_pair_arr))\n",
    "    all_labels = np.concatenate((pos_labels, neg_labels))\n",
    "    \n",
    "    # Split indices into training and test sets\n",
    "    train_idx, test_idx = train_test_split(\n",
    "        np.arange(len(all_pairs)), test_size=test_size, random_state=random_state, stratify=all_labels\n",
    "    )\n",
    "    \n",
    "    # Create training and test datasets\n",
    "    train_dataset = CustomBaselineDataset(train_window_data, all_pairs[train_idx], all_labels[train_idx], input_window_size, output_window_size, offset_to_current_year)\n",
    "    test_dataset = CustomBaselineDataset(train_window_data, all_pairs[test_idx], all_labels[test_idx], input_window_size, output_window_size, offset_to_current_year)\n",
    "    \n",
    "    return train_dataset, test_dataset\n",
    "\n",
    "def create_test_datasets(train_window_data, pos_inx_pair_arr, neg_inx_pair_arr, input_window_size, output_window_size, offset_to_current_year):\n",
    "    # Create labels\n",
    "    pos_labels = np.ones(len(pos_inx_pair_arr))\n",
    "    neg_labels = np.zeros(len(neg_inx_pair_arr))\n",
    "    \n",
    "    # Concatenate positive and negative pairs and labels\n",
    "    all_pairs = np.vstack((pos_inx_pair_arr, neg_inx_pair_arr))\n",
    "    all_labels = np.concatenate((pos_labels, neg_labels))\n",
    "     \n",
    "    return CustomBaselineDataset(train_window_data, all_pairs, all_labels, input_window_size, output_window_size, offset_to_current_year)\n",
    "\n",
    "train_dataset, val_dataset = create_train_val_datasets(\n",
    "    train_window_data=baseline_1_train_encoding_arr, pos_inx_pair_arr=train_pos_inx_pair_arr_3_3, neg_inx_pair_arr=train_neg_inx_pair_arr_3_3, \n",
    "    input_window_size=seq_length, output_window_size=out_length, offset_to_current_year = 3, test_size=0.2)\n",
    "\n",
    "test_dataset = create_test_datasets(\n",
    "    train_window_data=baseline_1_test_encoding_arr, pos_inx_pair_arr=test_pos_inx_pair_arr_3_0, neg_inx_pair_arr=test_neg_inx_pair_arr_3_0, \n",
    "    input_window_size=seq_length, output_window_size=out_length, offset_to_current_year = 0)\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ec843065",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten(nn.Module):\n",
    "    \"\"\"Converts N-dimensional tensor into 'flat' one.\"\"\"\n",
    "\n",
    "    def __init__(self, keep_batch_dim=True):\n",
    "        super().__init__()\n",
    "        self.keep_batch_dim = keep_batch_dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.keep_batch_dim:\n",
    "            return x.view(x.size(0), -1)\n",
    "        return x.view(-1)\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, raw_size, drop=.25):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.raw = nn.Sequential(\n",
    "            Flatten(),\n",
    "            nn.Dropout(drop), nn.Linear(raw_size, 128), nn.PReLU(), nn.BatchNorm1d(128),\n",
    "            nn.Dropout(drop), nn.Linear(128, 64), nn.PReLU(), nn.BatchNorm1d(64),\n",
    "            nn.Dropout(drop), nn.Linear( 64, 64), nn.PReLU(), nn.BatchNorm1d(64))\n",
    "        \n",
    "        self.output = nn.Sequential(\n",
    "            nn.Linear(64, 32), nn.ReLU(inplace=True), nn.Linear(32, 1), nn.Sigmoid())\n",
    "              \n",
    "        self.init_weights(nn.init.kaiming_normal_)\n",
    "        \n",
    "    def init_weights(self, init_fn):\n",
    "        def init(m): \n",
    "            for child in m.children():\n",
    "                if isinstance(child, nn.Conv1d):\n",
    "                    init_fn(child.weights)\n",
    "        init(self)\n",
    "        \n",
    "    def forward(self, t_raw):\n",
    "        raw_out = self.raw(t_raw)\n",
    "        out = self.output(raw_out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "6f203fdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-04 16:39:01,808 - INFO - Epoch [1/50], Train Loss: 0.6898, Train Accuracy: 52.60%, Val Loss: 0.6909, Val Accuracy: 52.59%\n",
      "2024-08-04 16:39:12,519 - INFO - Epoch [2/50], Train Loss: 0.6894, Train Accuracy: 52.54%, Val Loss: 0.6872, Val Accuracy: 53.40%\n",
      "2024-08-04 16:39:23,296 - INFO - Epoch [3/50], Train Loss: 0.6892, Train Accuracy: 52.84%, Val Loss: 0.6875, Val Accuracy: 53.64%\n",
      "2024-08-04 16:39:33,930 - INFO - Epoch [4/50], Train Loss: 0.6891, Train Accuracy: 52.80%, Val Loss: 0.6879, Val Accuracy: 53.60%\n",
      "2024-08-04 16:39:44,066 - INFO - Epoch [5/50], Train Loss: 0.6888, Train Accuracy: 53.06%, Val Loss: 0.6893, Val Accuracy: 53.04%\n",
      "2024-08-04 16:39:56,431 - INFO - Epoch [6/50], Train Loss: 0.6889, Train Accuracy: 52.84%, Val Loss: 0.6899, Val Accuracy: 52.85%\n",
      "2024-08-04 16:40:08,048 - INFO - Epoch [7/50], Train Loss: 0.6887, Train Accuracy: 52.96%, Val Loss: 0.6885, Val Accuracy: 53.59%\n",
      "2024-08-04 16:40:18,779 - INFO - Epoch [8/50], Train Loss: 0.6888, Train Accuracy: 52.97%, Val Loss: 0.6883, Val Accuracy: 53.51%\n",
      "2024-08-04 16:40:29,874 - INFO - Epoch [9/50], Train Loss: 0.6887, Train Accuracy: 53.07%, Val Loss: 0.6889, Val Accuracy: 53.33%\n",
      "2024-08-04 16:40:29,875 - INFO - Early stopping triggered\n",
      "2024-08-04 16:40:29,875 - INFO - Best model saved with {best_val_loss:.2f} accuracy\n"
     ]
    }
   ],
   "source": [
    "# Define the training and validation functions\n",
    "def train_one_epoch(model, train_loader, criterion, optimizer):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "\n",
    "    for data, labels, _, _ in train_loader:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data.float())\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        predicted = (outputs > 0.5).float()\n",
    "        total_train += labels.size(0)\n",
    "        correct_train += (predicted == labels).sum().item()\n",
    "    \n",
    "    train_accuracy = 100 * correct_train / total_train\n",
    "    train_loss = running_loss / len(train_loader)\n",
    "    return train_loss, train_accuracy\n",
    "\n",
    "def validate_one_epoch(model, val_loader, criterion):\n",
    "    model.eval()\n",
    "    running_val_loss = 0.0\n",
    "    correct_val = 0\n",
    "    total_val = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, labels, _, _ in val_loader:\n",
    "            \n",
    "            outputs = model(data.float())\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_val_loss += loss.item()\n",
    "            predicted = (outputs > 0.5).float()\n",
    "            total_val += labels.size(0)\n",
    "            correct_val += (predicted == labels).sum().item()\n",
    "    \n",
    "    val_loss = running_val_loss / len(val_loader)\n",
    "    val_accuracy = 100 * correct_val / total_val\n",
    "    return val_loss, val_accuracy\n",
    "\n",
    "# Training loop with early stopping\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs=50, patience=7, file_name='saved_files/best_x_model.pth'):\n",
    "    best_val_loss = float('inf')\n",
    "    early_stopping_counter = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss, train_accuracy = train_one_epoch(model, train_loader, criterion, optimizer)\n",
    "        val_loss, val_accuracy = validate_one_epoch(model, val_loader, criterion)\n",
    "\n",
    "        scheduler.step()\n",
    "        \n",
    "        logging.info(f'Epoch [{epoch+1}/{num_epochs}], '\n",
    "                     f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%, '\n",
    "                     f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%')\n",
    "\n",
    "        # Early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            early_stopping_counter = 0\n",
    "            torch.save(model.state_dict(), file_name)\n",
    "        else:\n",
    "            early_stopping_counter += 1\n",
    "            if early_stopping_counter >= patience:\n",
    "                logging.info(\"Early stopping triggered\")\n",
    "                logging.info(\"Best model saved with {best_val_loss:.2f} accuracy\")\n",
    "                break\n",
    "\n",
    "# Initialize logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Example usage\n",
    "input_dim = 1 \n",
    "model_mlp = MLP(input_dim)\n",
    "\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model_mlp.parameters(), lr=0.005)\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=10)\n",
    "\n",
    "train_model(model_mlp, train_dataloader, val_dataloader, criterion, optimizer, scheduler,file_name='saved_files/best_baseline_1_model.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "d3bb5c1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 350x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_roc(model, dataloader):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, labels,_ ,_ in dataloader:\n",
    "            outputs = model(data.float(), )\n",
    "            probs = outputs.cpu().numpy()\n",
    "            \n",
    "            all_probs.extend(probs)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    # Convert lists to numpy arrays\n",
    "    all_labels = np.array(all_labels).flatten()\n",
    "    all_probs = np.array(all_probs).flatten()\n",
    "\n",
    "    # Calculate ROC curve and AUC\n",
    "    fpr, tpr, thresholds = roc_curve(all_labels, all_probs)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    # np.save(\"fpr_method.npy\", fpr)\n",
    "    # np.save(\"tpr_method.npy\", tpr)\n",
    "\n",
    "    # Plot ROC curve\n",
    "    plt.figure(figsize=(3.5, 3))\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "\n",
    "model_mlp.load_state_dict(torch.load('saved_files/best_baseline_1_model.pth'))\n",
    "plot_roc(model_mlp, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "94ed9251",
   "metadata": {},
   "outputs": [],
   "source": [
    "for a,b,c,d in train_dataloader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2bb58102",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.],\n",
       "        [ 1.],\n",
       "        [ 1.],\n",
       "        [ 1.],\n",
       "        [ 1.],\n",
       "        [-1.],\n",
       "        [-1.],\n",
       "        [-1.],\n",
       "        [-1.],\n",
       "        [ 1.],\n",
       "        [ 1.],\n",
       "        [-1.],\n",
       "        [-1.],\n",
       "        [ 1.],\n",
       "        [ 1.],\n",
       "        [-1.],\n",
       "        [ 1.],\n",
       "        [ 1.],\n",
       "        [-1.],\n",
       "        [ 1.],\n",
       "        [-1.],\n",
       "        [ 1.],\n",
       "        [ 1.],\n",
       "        [-1.],\n",
       "        [-1.],\n",
       "        [-1.],\n",
       "        [ 1.],\n",
       "        [ 1.],\n",
       "        [ 1.],\n",
       "        [ 1.],\n",
       "        [ 1.],\n",
       "        [-1.],\n",
       "        [-1.],\n",
       "        [ 1.],\n",
       "        [-1.],\n",
       "        [ 1.],\n",
       "        [ 1.],\n",
       "        [ 1.],\n",
       "        [ 1.],\n",
       "        [-1.],\n",
       "        [-1.],\n",
       "        [-1.],\n",
       "        [-1.],\n",
       "        [ 1.],\n",
       "        [-1.],\n",
       "        [-1.],\n",
       "        [ 1.],\n",
       "        [ 1.],\n",
       "        [ 1.],\n",
       "        [ 1.],\n",
       "        [-1.],\n",
       "        [ 1.],\n",
       "        [ 1.],\n",
       "        [ 1.],\n",
       "        [-1.],\n",
       "        [ 1.],\n",
       "        [ 1.],\n",
       "        [ 1.],\n",
       "        [-1.],\n",
       "        [ 1.],\n",
       "        [ 1.],\n",
       "        [-1.],\n",
       "        [-1.],\n",
       "        [-1.],\n",
       "        [ 1.],\n",
       "        [ 1.],\n",
       "        [-1.],\n",
       "        [ 1.],\n",
       "        [-1.],\n",
       "        [ 1.],\n",
       "        [ 1.],\n",
       "        [-1.],\n",
       "        [-1.],\n",
       "        [ 1.],\n",
       "        [-1.],\n",
       "        [ 1.],\n",
       "        [ 1.],\n",
       "        [-1.],\n",
       "        [ 1.],\n",
       "        [-1.],\n",
       "        [ 1.],\n",
       "        [ 1.],\n",
       "        [-1.],\n",
       "        [-1.],\n",
       "        [ 1.],\n",
       "        [ 1.],\n",
       "        [-1.],\n",
       "        [-1.],\n",
       "        [ 1.],\n",
       "        [ 1.],\n",
       "        [ 1.],\n",
       "        [-1.],\n",
       "        [ 1.],\n",
       "        [ 1.],\n",
       "        [-1.],\n",
       "        [ 1.],\n",
       "        [ 1.],\n",
       "        [ 1.],\n",
       "        [ 1.],\n",
       "        [ 1.],\n",
       "        [ 1.],\n",
       "        [ 1.],\n",
       "        [ 1.],\n",
       "        [ 1.],\n",
       "        [ 1.],\n",
       "        [ 1.],\n",
       "        [-1.],\n",
       "        [ 1.],\n",
       "        [ 1.],\n",
       "        [ 1.],\n",
       "        [ 1.],\n",
       "        [-1.],\n",
       "        [ 1.],\n",
       "        [-1.],\n",
       "        [ 1.],\n",
       "        [-1.],\n",
       "        [ 1.],\n",
       "        [-1.],\n",
       "        [ 1.],\n",
       "        [ 1.],\n",
       "        [ 1.],\n",
       "        [-1.],\n",
       "        [ 1.],\n",
       "        [ 1.],\n",
       "        [-1.],\n",
       "        [ 1.],\n",
       "        [ 1.],\n",
       "        [ 1.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
