{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4b06eeb-b7bc-4787-a65f-de3ef301bd4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset, random_split\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.stattools import adfuller, acf, pacf, ccf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from typing import List, Tuple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "311720ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity_cosine(vec1, vec2):\n",
    "    cosine_similarity_arr = []\n",
    "    for v1,v2 in zip(vec1, vec2):\n",
    "        cosine_similarity = np.dot(v1, v2)/(np.linalg.norm(v1)* np.linalg.norm(v2))\n",
    "        cosine_similarity_arr.append(cosine_similarity)\n",
    "    return np.array(cosine_similarity_arr)\n",
    "\n",
    "def keep_words_with_underscore(input_string):\n",
    "    # Define a regular expression pattern to match words with underscores\n",
    "    pattern = r'\\b\\w*_[\\w_]*\\b'\n",
    "\n",
    "    # Use re.findall to extract words that match the pattern\n",
    "    matching_words = re.findall(pattern, input_string)\n",
    "\n",
    "    # Join the matching words to form the final string\n",
    "    result = ' '.join(matching_words)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def update_co_occurrences(word_year_list,word_co_occurrences):\n",
    "    # Iterate through the words in the list\n",
    "    word_list, year = word_year_list\n",
    "    \n",
    "    for word in word_list:\n",
    "        # If the word is not already in the dictionary, add it with an empty list\n",
    "        if word not in word_co_occurrences:\n",
    "            word_co_occurrences[word] = {}\n",
    "        \n",
    "        # Add words from the list to the co-occurrence list for the current word\n",
    "        for other_word in word_list:\n",
    "            # if other_word != word and other_word not in word_co_occurrences[word]:\n",
    "            #     word_co_occurrences[word].append(other_word)\n",
    "            if other_word != word and other_word not in word_co_occurrences[word]:\n",
    "                word_co_occurrences[word][other_word] = [year] \n",
    "            \n",
    "            elif other_word != word and other_word in word_co_occurrences[word]:\n",
    "                # word_co_occurrences[word][other_word][0] +=1\n",
    "                word_co_occurrences[word][other_word].append(year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6094b45a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concepts which were tracked (12770,)\n",
      "Abstracts (157821,)\n",
      "Year associated to abstract (157821,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 152310/152310 [00:06<00:00, 21960.71it/s]\n"
     ]
    }
   ],
   "source": [
    "concept_filtered_arr = np.load(\"files/overlapping_filtered_concepts.npy\")\n",
    "ngram_abstracts = np.load(\"files/ngram_abstracts.npy\", mmap_mode=\"r\")\n",
    "saved_year_arr = np.load(\"files/year_arr.npy\", mmap_mode=\"r\")\n",
    "\n",
    "print(\"Concepts which were tracked\",concept_filtered_arr.shape)\n",
    "print(\"Abstracts\",ngram_abstracts.shape)\n",
    "print(\"Year associated to abstract\",saved_year_arr.shape)\n",
    "\n",
    "phys_filtered_concept_dict = {k:1 for k in concept_filtered_arr}\n",
    "ocurr_arr = []\n",
    "for abstract, year in zip(ngram_abstracts, saved_year_arr):\n",
    "    temp = keep_words_with_underscore(abstract)\n",
    "    if temp.count(\" \") > 0:\n",
    "        temp = temp.split(\" \") \n",
    "        temp = [s for s in temp if s in phys_filtered_concept_dict]\n",
    "        ocurr_arr.append([list(filter((\"_\").__ne__, temp)),year])\n",
    "                        \n",
    "word_co_occurrences = {}\n",
    "\n",
    "for word_list in tqdm(ocurr_arr):\n",
    "    update_co_occurrences(word_list,word_co_occurrences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "549adfca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Representation Vectors for tracked concepts (12368, 31, 128)\n",
      "Concept associted with representation (12368,)\n",
      "Training Window: [2009 2010 2011 2012 2013 2014 2015 2016 2017 2018]\n",
      "Label Window: [2019 2020 2021]\n",
      "\n",
      "Training Window: [2012 2013 2014 2015 2016 2017 2018 2019 2020 2021]\n",
      "Label Window: [2022 2023 2024]\n",
      "\n",
      "Training Window: [2015 2016 2017 2018 2019 2020 2021 2022 2023 2024]\n"
     ]
    }
   ],
   "source": [
    "# class TimeSeriesDataset(Dataset):\n",
    "#     def __init__(self, data, word_co_occurrences, year_arr, c_inx_arr, input_window_size = 5, output_window_size = 3, offset_to_current_year = 1):\n",
    "#         self.train_window_data = data[:,-input_window_size-output_window_size-offset_to_current_year:-output_window_size-offset_to_current_year]\n",
    "#         if offset_to_current_year == 0:\n",
    "#             # self.label_window_data = data[:,-output_window_size:]\n",
    "#             self.label_year_range = year_arr[-output_window_size:]\n",
    "#         else: \n",
    "#             # self.label_window_data = data[:,-output_window_size-offset_to_current_year:-offset_to_current_year]\n",
    "#             self.label_year_range = year_arr[-output_window_size-offset_to_current_year:-offset_to_current_year]\n",
    "\n",
    "        \n",
    "#         self.co_occur_concept_pair_arr = get_co_occur_concept_pair_after_year_arr(word_co_occurrences, first_occ_year=self.label_year_range[0], final_occ_year=self.label_year_range[-1])\n",
    "#         self.c_inx_arr = c_inx_arr\n",
    "#         self.input_window_size = input_window_size \n",
    "#         self.output_window_size = output_window_size \n",
    "#         self.offset_to_current_year = offset_to_current_year \n",
    "\n",
    "#     def __len__(self):\n",
    "#         return 64*500\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "        \n",
    "#         if np.random.rand() < 0.5:\n",
    "#             return self._get_positive_sample()\n",
    "#         else:\n",
    "#             return self._get_negative_sample()\n",
    "    \n",
    "\n",
    "#     def _get_positive_sample(self):\n",
    "#         while True:\n",
    "#             sampled_pairs = np.random.choice(len(self.co_occur_concept_pair_arr), size=1)\n",
    "#             c_pair = self.co_occur_concept_pair_arr[sampled_pairs][0]\n",
    "#             inx_0 = np.where(self.c_inx_arr == c_pair[0])[0]\n",
    "#             inx_1 = np.where(self.c_inx_arr == c_pair[1])[0]\n",
    "#             if inx_0.size > 0 and inx_1.size > 0:\n",
    "#                 break\n",
    "#         enc_0 = self.train_window_data[inx_0][0]\n",
    "#         enc_1 = self.train_window_data[inx_1][0]\n",
    "#         enc_01 = np.concatenate((enc_0, enc_1), axis=-1)\n",
    "#         return torch.from_numpy(enc_01), torch.ones(1), torch.from_numpy(np.array([inx_0,inx_1]))\n",
    "\n",
    "#     def _get_negative_sample(self):\n",
    "#         while True:\n",
    "#             sampled_pair = np.random.choice(self.train_window_data.shape[0], size=2)\n",
    "#             if self.c_inx_arr[sampled_pair[1]] not in word_co_occurrences[self.c_inx_arr[sampled_pair[0]]]:\n",
    "#                 break\n",
    "#         inx_0 = np.where(self.c_inx_arr == self.c_inx_arr[sampled_pair[0]])[0]\n",
    "#         inx_1 = np.where(self.c_inx_arr == self.c_inx_arr[sampled_pair[1]])[0]\n",
    "#         enc_0 = self.train_window_data[inx_0][0]\n",
    "#         enc_1 = self.train_window_data[inx_1][0]\n",
    "#         enc_01 = np.concatenate((enc_0, enc_1), axis=-1)\n",
    "#         return torch.from_numpy(enc_01), torch.zeros(1), torch.from_numpy(np.array([inx_0,inx_1]))\n",
    "    \n",
    "#     def _check_indexing(self):\n",
    "#         if self.offset_to_current_year != 0 :\n",
    "#             print(f\"... {np.unique(saved_year_arr)[-self.input_window_size-self.output_window_size-self.offset_to_current_year-3:-self.input_window_size-self.output_window_size-self.offset_to_current_year]}\",f\" -> Training Window {np.unique(saved_year_arr)[-self.input_window_size-self.output_window_size-self.offset_to_current_year:-self.output_window_size-self.offset_to_current_year]}\",f\" <- {np.unique(saved_year_arr)[-self.output_window_size-self.offset_to_current_year:]}\")\n",
    "#             print(f\"... {np.unique(saved_year_arr)[-self.output_window_size-self.offset_to_current_year-3:-self.output_window_size-self.offset_to_current_year]}\",f\" -> Label Window {np.unique(saved_year_arr)[-self.output_window_size-self.offset_to_current_year:-self.offset_to_current_year]}\",f\" <- {np.unique(saved_year_arr)[-self.offset_to_current_year:]}\")\n",
    "#         else:\n",
    "#             print(f\"... {np.unique(saved_year_arr)[-self.input_window_size-self.output_window_size-3:-self.input_window_size-self.output_window_size]}\",f\" -> Training Window {np.unique(saved_year_arr)[-self.input_window_size-self.output_window_size:-self.output_window_size]}\",f\" <- {np.unique(saved_year_arr)[-self.output_window_size:]}\")\n",
    "#             print(f\"... {np.unique(saved_year_arr)[-self.output_window_size-3:-self.output_window_size]}\",f\" -> Label Window {np.unique(saved_year_arr)[-self.output_window_size:]}\",f\" <- {[]}\")\n",
    "\n",
    "class NovelSeriesDataset(Dataset):\n",
    "    def __init__(self, data: np.ndarray, c_inx_arr: np.ndarray, input_window_size: int = 5):\n",
    "        \"\"\"\n",
    "        Dataset for novel series data.\n",
    "\n",
    "        Args:\n",
    "            data (np.ndarray): The input data.\n",
    "            c_inx_arr (np.ndarray): Array of concept indices.\n",
    "            input_window_size (int, optional): Size of the input window. Defaults to 5.\n",
    "        \"\"\"\n",
    "        self.train_window_data = data[:, -input_window_size:]\n",
    "        self.c_inx_arr = c_inx_arr\n",
    "        self.input_window_size = input_window_size\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return 64 * 500\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        while True:\n",
    "            sampled_pair = np.random.choice(self.train_window_data.shape[0], size=2)\n",
    "            if self.c_inx_arr[sampled_pair[1]] not in word_co_occurrences[self.c_inx_arr[sampled_pair[0]]]:\n",
    "                break\n",
    "        inx_0 = np.where(self.c_inx_arr == self.c_inx_arr[sampled_pair[0]])[0]\n",
    "        inx_1 = np.where(self.c_inx_arr == self.c_inx_arr[sampled_pair[1]])[0]\n",
    "        enc_0 = self.train_window_data[inx_0][0]\n",
    "        enc_1 = self.train_window_data[inx_1][0]\n",
    "        enc_01 = np.concatenate((enc_0, enc_1), axis=-1)\n",
    "        return torch.from_numpy(enc_01), torch.from_numpy(np.array([inx_0, inx_1]))\n",
    "\n",
    "    def _check_indexing(self):\n",
    "        print(f\"Training Window: {np.unique(saved_year_arr)[-self.input_window_size:]}\")\n",
    "\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, data: np.ndarray, word_co_occurrences: dict, year_arr: np.ndarray, c_inx_arr: np.ndarray, \n",
    "                 input_window_size: int = 5, output_window_size: int = 3, offset_to_current_year: int = 1):\n",
    "        \"\"\"\n",
    "        Dataset for time series data.\n",
    "\n",
    "        Args:\n",
    "            data (np.ndarray): The input data.\n",
    "            word_co_occurrences (dict): Dictionary of word co-occurrences.\n",
    "            year_arr (np.ndarray): Array of years.\n",
    "            c_inx_arr (np.ndarray): Array of concept indices.\n",
    "            input_window_size (int, optional): Size of the input window. Defaults to 5.\n",
    "            output_window_size (int, optional): Size of the output window. Defaults to 3.\n",
    "            offset_to_current_year (int, optional): Offset to the current year. Defaults to 1.\n",
    "        \"\"\"\n",
    "        self.train_window_data = data[:, -input_window_size-output_window_size-offset_to_current_year:-output_window_size-offset_to_current_year]\n",
    "        self.label_year_range = (year_arr[-output_window_size:] if offset_to_current_year == 0 \n",
    "                                 else year_arr[-output_window_size-offset_to_current_year:-offset_to_current_year])\n",
    "\n",
    "        self.co_occur_concept_pair_arr = self.get_co_occur_concept_pair_after_year_arr(\n",
    "            word_co_occurrences, self.label_year_range[0], self.label_year_range[-1])\n",
    "        self.c_inx_arr = c_inx_arr\n",
    "        self.input_window_size = input_window_size\n",
    "        self.output_window_size = output_window_size\n",
    "        self.offset_to_current_year = offset_to_current_year\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return 64 * 5000\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        return (self._get_positive_sample() if np.random.rand() < 0.5 else self._get_negative_sample())\n",
    "\n",
    "    def _get_positive_sample(self) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        while True:\n",
    "            sampled_pairs = np.random.choice(len(self.co_occur_concept_pair_arr), size=1)\n",
    "            c_pair = self.co_occur_concept_pair_arr[sampled_pairs][0]\n",
    "            inx_0 = np.where(self.c_inx_arr == c_pair[0])[0]\n",
    "            inx_1 = np.where(self.c_inx_arr == c_pair[1])[0]\n",
    "            if inx_0.size > 0 and inx_1.size > 0:\n",
    "                break\n",
    "        enc_0 = self.train_window_data[inx_0][0]\n",
    "        enc_1 = self.train_window_data[inx_1][0]\n",
    "        enc_01 = np.concatenate((enc_0, enc_1), axis=-1)\n",
    "        return torch.from_numpy(enc_01), torch.ones(1), torch.from_numpy(np.array([inx_0, inx_1]))\n",
    "\n",
    "    def _get_negative_sample(self) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        while True:\n",
    "            sampled_pair = np.random.choice(self.train_window_data.shape[0], size=2)\n",
    "            if self.c_inx_arr[sampled_pair[1]] not in word_co_occurrences[self.c_inx_arr[sampled_pair[0]]]:\n",
    "                break\n",
    "        inx_0 = np.where(self.c_inx_arr == self.c_inx_arr[sampled_pair[0]])[0]\n",
    "        inx_1 = np.where(self.c_inx_arr == self.c_inx_arr[sampled_pair[1]])[0]\n",
    "        enc_0 = self.train_window_data[inx_0][0]\n",
    "        enc_1 = self.train_window_data[inx_1][0]\n",
    "        enc_01 = np.concatenate((enc_0, enc_1), axis=-1)\n",
    "        return torch.from_numpy(enc_01), torch.zeros(1), torch.from_numpy(np.array([inx_0, inx_1]))\n",
    "\n",
    "    def _check_indexing(self):\n",
    "        print(f\"Training Window: {self._get_years_range(-self.input_window_size-self.output_window_size-self.offset_to_current_year, -self.output_window_size-self.offset_to_current_year)}\")\n",
    "        print(f\"Label Window: {self._get_years_range(-self.output_window_size-self.offset_to_current_year, -self.offset_to_current_year)}\")\n",
    "\n",
    "    def _get_years_range(self, start: int, end: int) -> np.ndarray:\n",
    "        # return np.unique(saved_year_arr)[start:end]\n",
    "    \n",
    "        return (np.unique(saved_year_arr)[start:] if end == -0 \n",
    "                                 else np.unique(saved_year_arr)[start:end])\n",
    "\n",
    "    @staticmethod\n",
    "    def get_co_occur_concept_pair_after_year_arr(word_co_occurrences: dict, first_occ_year: int, final_occ_year: int) -> np.ndarray:\n",
    "        \n",
    "        co_occur_concept_pair_arr = []\n",
    "        for concept, v in word_co_occurrences.items():\n",
    "            for co_concept, years in v.items():\n",
    "                if np.min(years) >= first_occ_year and np.max(years)<=final_occ_year:\n",
    "                    co_occur_concept_pair_arr.append([concept,co_concept])\n",
    "        return np.array(co_occur_concept_pair_arr)\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "num_samples_per_class = 32\n",
    "num_features = 128\n",
    "seq_length = 5\n",
    "batch_size = 128\n",
    "\n",
    "encoding_dat = np.load(\"c_encoding_arr.npy\")\n",
    "c_inx_arr = np.load(\"c_inx_arr.npy\")\n",
    "print(\"Representation Vectors for tracked concepts\",encoding_dat.shape)\n",
    "print(\"Concept associted with representation\", c_inx_arr.shape)\n",
    "scaler = RobustScaler()\n",
    "reshaped_data = encoding_dat.reshape(-1, encoding_dat.shape[-1])  # Shape: (10000*31, 128)\n",
    "normalized_data = scaler.fit_transform(reshaped_data)\n",
    "encoding_data = normalized_data.reshape(encoding_dat.shape)\n",
    "\n",
    "dataset = TimeSeriesDataset(data=encoding_data, word_co_occurrences=word_co_occurrences, year_arr=np.unique(saved_year_arr), \n",
    "                            c_inx_arr=c_inx_arr, input_window_size = 10, output_window_size = 3, offset_to_current_year = 3)\n",
    "dataset._check_indexing()\n",
    "print()\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "testing_dataset = TimeSeriesDataset(data=encoding_data, word_co_occurrences=word_co_occurrences, year_arr=np.unique(saved_year_arr), \n",
    "                            c_inx_arr=c_inx_arr, input_window_size = 10, output_window_size = 3, offset_to_current_year = 0)\n",
    "testing_dataset._check_indexing()\n",
    "testing_dataloader = DataLoader(testing_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "print()\n",
    "novel_dataset = NovelSeriesDataset(data=encoding_data, c_inx_arr=c_inx_arr, input_window_size = 10)\n",
    "novel_dataset._check_indexing()\n",
    "novel_dataloader = DataLoader(novel_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3166a6db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Train Loss: 0.6096, Train Accuracy: 65.98%, Val Loss: 0.5685, Val Accuracy: 71.39%\n",
      "Epoch [2/50], Train Loss: 0.5494, Train Accuracy: 73.30%, Val Loss: 0.5295, Val Accuracy: 75.03%\n",
      "Epoch [3/50], Train Loss: 0.5318, Train Accuracy: 74.49%, Val Loss: 0.5194, Val Accuracy: 74.83%\n",
      "Epoch [4/50], Train Loss: 0.5242, Train Accuracy: 74.78%, Val Loss: 0.5320, Val Accuracy: 74.64%\n",
      "Epoch [5/50], Train Loss: 0.5143, Train Accuracy: 75.38%, Val Loss: 0.5162, Val Accuracy: 75.20%\n",
      "Epoch [6/50], Train Loss: 0.5014, Train Accuracy: 76.34%, Val Loss: 0.5112, Val Accuracy: 75.38%\n",
      "Epoch [7/50], Train Loss: 0.4929, Train Accuracy: 76.71%, Val Loss: 0.5055, Val Accuracy: 75.23%\n",
      "Epoch [8/50], Train Loss: 0.4963, Train Accuracy: 76.38%, Val Loss: 0.4894, Val Accuracy: 76.72%\n",
      "Epoch [9/50], Train Loss: 0.4968, Train Accuracy: 76.28%, Val Loss: 0.4849, Val Accuracy: 77.61%\n",
      "Epoch [10/50], Train Loss: 0.4969, Train Accuracy: 76.13%, Val Loss: 0.4873, Val Accuracy: 77.05%\n",
      "Epoch [11/50], Train Loss: 0.4917, Train Accuracy: 76.53%, Val Loss: 0.4952, Val Accuracy: 76.25%\n",
      "Epoch [12/50], Train Loss: 0.4869, Train Accuracy: 77.22%, Val Loss: 0.4897, Val Accuracy: 77.34%\n",
      "Epoch [13/50], Train Loss: 0.4815, Train Accuracy: 77.26%, Val Loss: 0.4810, Val Accuracy: 77.48%\n",
      "Epoch [14/50], Train Loss: 0.4857, Train Accuracy: 77.13%, Val Loss: 0.4847, Val Accuracy: 77.50%\n",
      "Epoch [15/50], Train Loss: 0.4891, Train Accuracy: 77.28%, Val Loss: 0.4822, Val Accuracy: 77.69%\n",
      "Epoch [16/50], Train Loss: 0.4856, Train Accuracy: 77.54%, Val Loss: 0.4922, Val Accuracy: 76.77%\n",
      "Epoch [17/50], Train Loss: 0.4976, Train Accuracy: 76.49%, Val Loss: 0.4917, Val Accuracy: 76.80%\n",
      "Epoch [18/50], Train Loss: 0.4981, Train Accuracy: 76.60%, Val Loss: 0.5031, Val Accuracy: 75.80%\n",
      "Early stopping triggered\n"
     ]
    }
   ],
   "source": [
    "# class MLP(nn.Module):\n",
    "#     def __init__(self, input_dim):\n",
    "#         super(MLP, self).__init__()\n",
    "#         self.fc1 = nn.Linear(input_dim, 256)\n",
    "#         self.relu = nn.ReLU()\n",
    "#         self.fc2 = nn.Linear(256, 128)\n",
    "#         self.fc3 = nn.Linear(128, 64)\n",
    "#         self.fc4 = nn.Linear(64, 1)\n",
    "        \n",
    "#         self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "#     def forward(self, x):\n",
    "#         x = self.fc1(x)\n",
    "#         x = self.relu(x)\n",
    "#         x = self.fc2(x)\n",
    "#         x = self.relu(x)\n",
    "#         x = self.fc3(x)\n",
    "#         x = self.relu(x)\n",
    "#         x = self.fc4(x)\n",
    "#         x = self.sigmoid(x)\n",
    "#         return x\n",
    "    \n",
    "# # Define the model, loss function, optimizer, and scheduler\n",
    "# input_dim = dataset.train_window_data.shape[1] * dataset.train_window_data.shape[2] * 2  # Flattened size * 2 for concatenated pairs\n",
    "# model = MLP(input_dim=input_dim)\n",
    "\n",
    "# criterion = nn.BCELoss()\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "# scheduler = CosineAnnealingLR(optimizer, T_max=10)\n",
    "\n",
    "# # Training the model with early stopping\n",
    "# num_epochs = 50\n",
    "# patience = 5\n",
    "# best_val_loss = float('inf')\n",
    "# early_stopping_counter = 0\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     model.train()\n",
    "#     running_loss = 0.0\n",
    "#     correct_train = 0\n",
    "#     total_train = 0\n",
    "    \n",
    "#     for data, labels, _ in train_loader:\n",
    "#         data = data.view(data.size(0), -1).float()  # Flatten the input data\n",
    "#         labels = labels.float()\n",
    "        \n",
    "#         optimizer.zero_grad()\n",
    "#         outputs = model(data)\n",
    "        \n",
    "#         loss = criterion(outputs, labels)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "        \n",
    "#         running_loss += loss.item()\n",
    "#         predicted = (outputs > 0.5).float()\n",
    "#         total_train += labels.size(0)\n",
    "#         correct_train += (predicted == labels).sum().item()\n",
    "    \n",
    "#     scheduler.step()\n",
    "#     train_accuracy = 100 * correct_train / total_train\n",
    "#     train_loss = running_loss / len(train_loader)\n",
    "\n",
    "#     # Validation phase\n",
    "#     model.eval()\n",
    "#     running_val_loss = 0.0\n",
    "#     correct_val = 0\n",
    "#     total_val = 0\n",
    "    \n",
    "#     with torch.no_grad():\n",
    "#         for data, labels, _ in val_loader:\n",
    "#             data = data.view(data.size(0), -1).float()  # Flatten the input data\n",
    "#             labels = labels.float()\n",
    "#             outputs = model(data)\n",
    "#             loss = criterion(outputs, labels)\n",
    "#             running_val_loss += loss.item()\n",
    "#             predicted = (outputs > 0.5).float()\n",
    "#             total_val += labels.size(0)\n",
    "#             correct_val += (predicted == labels).sum().item()\n",
    "    \n",
    "#     val_loss = running_val_loss / len(val_loader)\n",
    "#     val_accuracy = 100 * correct_val / total_val\n",
    "\n",
    "#     print(f'Epoch [{epoch+1}/{num_epochs}], '\n",
    "#           f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%, '\n",
    "#           f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%')\n",
    "\n",
    "#     # Early stopping\n",
    "#     if val_loss < best_val_loss:\n",
    "#         best_val_loss = val_loss\n",
    "#         early_stopping_counter = 0\n",
    "#         torch.save(model.state_dict(), 'best_model.pth')\n",
    "#     else:\n",
    "#         early_stopping_counter += 1\n",
    "#         if early_stopping_counter >= patience:\n",
    "#             print(\"Early stopping triggered\")\n",
    "#             break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6697f6f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-13 21:32:35,466 - INFO - Epoch [1/50], Train Loss: 0.5369, Train Accuracy: 73.99%, Val Loss: 0.5135, Val Accuracy: 75.62%\n",
      "2024-06-13 21:33:22,465 - INFO - Epoch [2/50], Train Loss: 0.5196, Train Accuracy: 75.04%, Val Loss: 0.4962, Val Accuracy: 76.69%\n",
      "2024-06-13 21:34:11,741 - INFO - Epoch [3/50], Train Loss: 0.5103, Train Accuracy: 75.69%, Val Loss: 0.4892, Val Accuracy: 76.90%\n",
      "2024-06-13 21:35:00,300 - INFO - Epoch [4/50], Train Loss: 0.5044, Train Accuracy: 75.98%, Val Loss: 0.4827, Val Accuracy: 77.31%\n",
      "2024-06-13 21:35:54,531 - INFO - Epoch [5/50], Train Loss: 0.4963, Train Accuracy: 76.61%, Val Loss: 0.4773, Val Accuracy: 77.58%\n",
      "2024-06-13 21:36:44,152 - INFO - Epoch [6/50], Train Loss: 0.4913, Train Accuracy: 76.86%, Val Loss: 0.4718, Val Accuracy: 77.92%\n",
      "2024-06-13 21:37:34,850 - INFO - Epoch [7/50], Train Loss: 0.4899, Train Accuracy: 76.84%, Val Loss: 0.4689, Val Accuracy: 78.18%\n",
      "2024-06-13 21:38:24,698 - INFO - Epoch [8/50], Train Loss: 0.4872, Train Accuracy: 77.17%, Val Loss: 0.4643, Val Accuracy: 78.51%\n",
      "2024-06-13 21:39:15,618 - INFO - Epoch [9/50], Train Loss: 0.4819, Train Accuracy: 77.38%, Val Loss: 0.4617, Val Accuracy: 78.37%\n",
      "2024-06-13 21:40:06,637 - INFO - Epoch [10/50], Train Loss: 0.4839, Train Accuracy: 77.34%, Val Loss: 0.4599, Val Accuracy: 78.50%\n",
      "2024-06-13 21:40:58,218 - INFO - Epoch [11/50], Train Loss: 0.4837, Train Accuracy: 77.27%, Val Loss: 0.4639, Val Accuracy: 78.45%\n",
      "2024-06-13 21:41:47,573 - INFO - Epoch [12/50], Train Loss: 0.4824, Train Accuracy: 77.34%, Val Loss: 0.4657, Val Accuracy: 78.18%\n",
      "2024-06-13 21:42:37,402 - INFO - Epoch [13/50], Train Loss: 0.4826, Train Accuracy: 77.31%, Val Loss: 0.4635, Val Accuracy: 78.22%\n",
      "2024-06-13 21:43:26,988 - INFO - Epoch [14/50], Train Loss: 0.4837, Train Accuracy: 77.34%, Val Loss: 0.4604, Val Accuracy: 78.60%\n",
      "2024-06-13 21:44:17,969 - INFO - Epoch [15/50], Train Loss: 0.4829, Train Accuracy: 77.35%, Val Loss: 0.4633, Val Accuracy: 78.32%\n",
      "2024-06-13 21:44:17,969 - INFO - Early stopping triggered\n"
     ]
    }
   ],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "# from torch.utils.data import DataLoader, random_split\n",
    "# import numpy as np\n",
    "# from sklearn.preprocessing import RobustScaler\n",
    "import logging\n",
    "\n",
    "# Define the MLP model with enhancements\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim: int):\n",
    "        \"\"\"\n",
    "        Initialize the MLP model.\n",
    "        \n",
    "        Args:\n",
    "            input_dim (int): Dimension of the input features.\n",
    "        \"\"\"\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 256)\n",
    "        self.bn1 = nn.BatchNorm1d(256)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.bn3 = nn.BatchNorm1d(64)\n",
    "        self.fc4 = nn.Linear(64, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass of the model.\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor.\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor.\n",
    "        \"\"\"\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc4(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "# Define the training and validation functions\n",
    "def train_one_epoch(model, train_loader, criterion, optimizer):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "\n",
    "    for data, labels, _ in train_loader:\n",
    "        data = data.view(data.size(0), -1).float()  # Flatten the input data\n",
    "        labels = labels.float()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data)\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        predicted = (outputs > 0.5).float()\n",
    "        total_train += labels.size(0)\n",
    "        correct_train += (predicted == labels).sum().item()\n",
    "    \n",
    "    train_accuracy = 100 * correct_train / total_train\n",
    "    train_loss = running_loss / len(train_loader)\n",
    "    return train_loss, train_accuracy\n",
    "\n",
    "def validate_one_epoch(model, val_loader, criterion):\n",
    "    model.eval()\n",
    "    running_val_loss = 0.0\n",
    "    correct_val = 0\n",
    "    total_val = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, labels, _ in val_loader:\n",
    "            data = data.view(data.size(0), -1).float()  # Flatten the input data\n",
    "            labels = labels.float()\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_val_loss += loss.item()\n",
    "            predicted = (outputs > 0.5).float()\n",
    "            total_val += labels.size(0)\n",
    "            correct_val += (predicted == labels).sum().item()\n",
    "    \n",
    "    val_loss = running_val_loss / len(val_loader)\n",
    "    val_accuracy = 100 * correct_val / total_val\n",
    "    return val_loss, val_accuracy\n",
    "\n",
    "# Training loop with early stopping\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs=50, patience=5):\n",
    "    best_val_loss = float('inf')\n",
    "    early_stopping_counter = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss, train_accuracy = train_one_epoch(model, train_loader, criterion, optimizer)\n",
    "        val_loss, val_accuracy = validate_one_epoch(model, val_loader, criterion)\n",
    "\n",
    "        scheduler.step()\n",
    "        \n",
    "        logging.info(f'Epoch [{epoch+1}/{num_epochs}], '\n",
    "                     f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%, '\n",
    "                     f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%')\n",
    "\n",
    "        # Early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            early_stopping_counter = 0\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "        else:\n",
    "            early_stopping_counter += 1\n",
    "            if early_stopping_counter >= patience:\n",
    "                logging.info(\"Early stopping triggered\")\n",
    "                break\n",
    "\n",
    "# Initialize logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Example usage\n",
    "input_dim = dataset.train_window_data.shape[1] * dataset.train_window_data.shape[2] * 2  # Flattened size * 2 for concatenated pairs\n",
    "model = MLP(input_dim=input_dim)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=10)\n",
    "\n",
    "train_model(model, train_loader, val_loader, criterion, optimizer, scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4adbca04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np\n",
    "import logging\n",
    "\n",
    "# Define the cosine similarity function\n",
    "# def similarity_cosine(vec1, vec2):\n",
    "#     \"\"\"\n",
    "#     Compute the cosine similarity between two vectors.\n",
    "\n",
    "#     Args:\n",
    "#         vec1 (numpy.ndarray): First vector.\n",
    "#         vec2 (numpy.ndarray): Second vector.\n",
    "\n",
    "#     Returns:\n",
    "#         float: Cosine similarity between vec1 and vec2.\n",
    "#     \"\"\"\n",
    "#     cos_sim = np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
    "#     return cos_sim\n",
    "def similarity_cosine(vec1, vec2):\n",
    "    cosine_similarity_arr = []\n",
    "    for v1,v2 in zip(vec1, vec2):\n",
    "        cosine_similarity = np.dot(v1, v2)/(np.linalg.norm(v1)* np.linalg.norm(v2))\n",
    "        cosine_similarity_arr.append(cosine_similarity)\n",
    "    return np.array(cosine_similarity_arr)\n",
    "\n",
    "def load_best_model(model, path='best_model.pth'):\n",
    "    \"\"\"\n",
    "    Load the best model from the given path.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The model to load the state dictionary into.\n",
    "        path (str): Path to the saved model state dictionary.\n",
    "    \"\"\"\n",
    "    model.load_state_dict(torch.load(path))\n",
    "\n",
    "def evaluate_model(model, dataloader, encoding_dat, c_inx_arr):\n",
    "    \"\"\"\n",
    "    Evaluate the model on the provided dataloader.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The trained model.\n",
    "        dataloader (torch.utils.data.DataLoader): DataLoader for the dataset.\n",
    "        encoding_dat (numpy.ndarray): Encoded data.\n",
    "        c_inx_arr (numpy.ndarray): Concept index array.\n",
    "\n",
    "    Returns:\n",
    "        float: Validation accuracy.\n",
    "        numpy.ndarray: Indices of the samples.\n",
    "        numpy.ndarray: Model outputs.\n",
    "        numpy.ndarray: Ground truth labels.\n",
    "        numpy.ndarray: Correct predictions.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    correct_val = 0\n",
    "    total_val = 0\n",
    "\n",
    "    indices = []\n",
    "    outputs_list = []\n",
    "    correct_indices = []\n",
    "    labels_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, labels, inx in dataloader:\n",
    "            data = data.view(data.size(0), -1).float()  # Flatten the input data\n",
    "            labels = labels.float()\n",
    "            outputs = model(data)\n",
    "            predicted = (outputs > 0.5).float()\n",
    "            total_val += labels.size(0)\n",
    "            correct_val += (predicted == labels).sum().item()\n",
    "            \n",
    "            # Collect indices, outputs, labels, and correct predictions\n",
    "            indices.extend(inx.cpu().numpy())\n",
    "            outputs_list.extend(outputs.cpu().numpy())\n",
    "            labels_list.extend(labels.cpu().numpy())\n",
    "            correct_indices.extend((predicted == labels).cpu().numpy())\n",
    "    \n",
    "    # Convert lists to numpy arrays\n",
    "    indices = np.array(indices)\n",
    "    outputs_list = np.array(outputs_list).flatten()\n",
    "    labels_list = np.array(labels_list).flatten()\n",
    "    correct_indices = np.array(correct_indices).flatten()\n",
    "\n",
    "    val_accuracy = 100 * correct_val / total_val\n",
    "    return val_accuracy, indices, outputs_list, labels_list, correct_indices\n",
    "\n",
    "def test_model(model, dataloader):\n",
    "    \"\"\"\n",
    "    Evaluate the model on the provided dataloader.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The trained model.\n",
    "        dataloader (torch.utils.data.DataLoader): DataLoader for the dataset.\n",
    "        encoding_dat (numpy.ndarray): Encoded data.\n",
    "        c_inx_arr (numpy.ndarray): Concept index array.\n",
    "\n",
    "    Returns:\n",
    "        float: Validation accuracy.\n",
    "        numpy.ndarray: Indices of the samples.\n",
    "        numpy.ndarray: Model outputs.\n",
    "        numpy.ndarray: Ground truth labels.\n",
    "        numpy.ndarray: Correct predictions.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    indices = []\n",
    "    outputs_list = []  \n",
    "    predicted_list = []  \n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, inx in dataloader:\n",
    "            data = data.view(data.size(0), -1).float()  # Flatten the input data\n",
    "            \n",
    "            outputs = model(data)\n",
    "            predicted = (outputs > 0.5).float()\n",
    "            \n",
    "            # Collect indices, outputs, labels, and correct predictions\n",
    "            indices.extend(inx.cpu().numpy())\n",
    "            outputs_list.extend(outputs.cpu().numpy())\n",
    "            predicted_list.extend(predicted.cpu().numpy())\n",
    "            \n",
    "    # Convert lists to numpy arrays for sorting\n",
    "    indices = np.array(indices)\n",
    "    outputs_list = np.array(outputs_list).flatten()\n",
    "    predicted_list = np.array(predicted_list).flatten()\n",
    "    \n",
    "    return indices, outputs_list, predicted_list\n",
    "\n",
    "def analyze_novel_predictions(indices, outputs_list, predicted_list, encoding_dat, c_inx_arr):\n",
    "    \"\"\"\n",
    "    Analyze the predictions and print correct predictions for co-occurrence and non-co-occurrence.\n",
    "\n",
    "    Args:\n",
    "        indices (numpy.ndarray): Indices of the samples.\n",
    "        outputs_list (numpy.ndarray): Model outputs.\n",
    "        labels_list (numpy.ndarray): Ground truth labels.\n",
    "        correct_indices (numpy.ndarray): Correct predictions.\n",
    "        encoding_dat (numpy.ndarray): Encoded data.\n",
    "        c_inx_arr (numpy.ndarray): Concept index array.\n",
    "    \"\"\"\n",
    "    sorted_indices = np.argsort(outputs_list)\n",
    "\n",
    "    correct_0 = []\n",
    "    correct_1 = []\n",
    "\n",
    "    for i in sorted_indices:\n",
    "        if predicted_list[i]:\n",
    "            correct_1.append(indices[i])\n",
    "        else:\n",
    "            correct_0.append(indices[i])\n",
    "\n",
    "    print(\"Correct predictions to have no co-occurrence:\")\n",
    "    print_correct_predictions(correct_0, encoding_dat, c_inx_arr, \"no co-occurrence\")\n",
    "\n",
    "    print(\"\\nCorrect predictions to have co-occurrence:\")\n",
    "    print_correct_predictions(correct_1, encoding_dat, c_inx_arr, \"co-occurrence\")\n",
    "\n",
    "def analyze_predictions(indices, outputs_list, labels_list, correct_indices, encoding_dat, c_inx_arr):\n",
    "    \"\"\"\n",
    "    Analyze the predictions and print correct predictions for co-occurrence and non-co-occurrence.\n",
    "\n",
    "    Args:\n",
    "        indices (numpy.ndarray): Indices of the samples.\n",
    "        outputs_list (numpy.ndarray): Model outputs.\n",
    "        labels_list (numpy.ndarray): Ground truth labels.\n",
    "        correct_indices (numpy.ndarray): Correct predictions.\n",
    "        encoding_dat (numpy.ndarray): Encoded data.\n",
    "        c_inx_arr (numpy.ndarray): Concept index array.\n",
    "    \"\"\"\n",
    "    sorted_indices = np.argsort(outputs_list)\n",
    "\n",
    "    correct_0 = []\n",
    "    correct_1 = []\n",
    "\n",
    "    for i in sorted_indices:\n",
    "        if correct_indices[i]:\n",
    "            if labels_list[i] == 0:\n",
    "                correct_0.append(indices[i])\n",
    "            else:\n",
    "                correct_1.append(indices[i])\n",
    "\n",
    "    print(\"Correct predictions to have no co-occurrence:\")\n",
    "    print_correct_predictions(correct_0, encoding_dat, c_inx_arr, \"no co-occurrence\")\n",
    "\n",
    "    print(\"\\nCorrect predictions to have co-occurrence:\")\n",
    "    print_correct_predictions(correct_1, encoding_dat, c_inx_arr, \"co-occurrence\")\n",
    "\n",
    "def print_correct_predictions(correct_list, encoding_dat, c_inx_arr, label):\n",
    "    \"\"\"\n",
    "    Print the correct predictions with their slopes.\n",
    "\n",
    "    Args:\n",
    "        correct_list (list): List of correct predictions.\n",
    "        encoding_dat (numpy.ndarray): Encoded data.\n",
    "        c_inx_arr (numpy.ndarray): Concept index array.\n",
    "        label (str): Label for the type of correct prediction (co-occurrence or no co-occurrence).\n",
    "    \"\"\"\n",
    "    x = np.arange(31).reshape(-1, 1)\n",
    "    lin_model = LinearRegression()\n",
    "\n",
    "    for cnt, idx in enumerate(correct_list):\n",
    "        sim = similarity_cosine(encoding_dat[idx[0]][0], encoding_dat[idx[1]][0])\n",
    "        lin_model.fit(x, sim.reshape(-1, 1))\n",
    "        slope = lin_model.coef_[0][0]\n",
    "        print(c_inx_arr[idx[0]], c_inx_arr[idx[1]], np.round(slope, 3))\n",
    "        if cnt == 5:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4c546c35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-14 00:18:30,801 - INFO - \n",
      "Validation Accuracy: 76.20%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct predictions to have no co-occurrence:\n",
      "['average_phonon_number'] ['quantum_pcp_conjecture'] -0.004\n",
      "['memory_function'] ['boolean_algebra'] 0.003\n",
      "['mechanical_resonance'] ['boolean_algebra'] -0.0\n",
      "['network_coding'] ['electric_polarization'] -0.003\n",
      "['linear_constraint'] ['ultracold_rubidium_atom'] -0.009\n",
      "['expander_graph'] ['spin_wave_mode'] 0.003\n",
      "\n",
      "Correct predictions to have co-occurrence:\n",
      "['spectral_gap'] ['chaos_theory'] 0.004\n",
      "['robust_quantum_information_processing'] ['schrodinger_equation'] -0.005\n",
      "['classical_field_theory'] ['inner_product_space'] 0.003\n",
      "['gauss_law'] ['governing_equation'] -0.003\n",
      "['fermi_hubbard_model'] ['block_encoding'] -0.002\n",
      "['fermi_hubbard_model'] ['block_encoding'] -0.002\n"
     ]
    }
   ],
   "source": [
    "# Initialize logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Load the best model\n",
    "load_best_model(model)\n",
    "\n",
    "# Final evaluation on the validation set\n",
    "val_accuracy, indices, outputs_list, labels_list, correct_indices = evaluate_model(model, testing_dataloader, encoding_dat, c_inx_arr)\n",
    "\n",
    "# Analyze predictions\n",
    "analyze_predictions(indices, outputs_list, labels_list, correct_indices, encoding_dat, c_inx_arr)\n",
    "\n",
    "# Print final validation accuracy\n",
    "logging.info(f\"\\nValidation Accuracy: {val_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d78bf3fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-14 00:20:23,045 - INFO - \n",
      "Validation Accuracy: 76.20%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct predictions to have no co-occurrence:\n",
      "['deterministic_computation'] ['single_cesium_atom'] -0.003\n",
      "['orthogonal_array'] ['infrared_wavelength'] -0.003\n",
      "['deterministic_computation'] ['signal_light'] -0.005\n",
      "['quantum_homomorphic_encryption'] ['esr_measurement'] -0.008\n",
      "['collective_atomic_excitation'] ['complexity_function'] -0.004\n",
      "['structural_similarity'] ['molecular_bond'] -0.006\n",
      "\n",
      "Correct predictions to have co-occurrence:\n",
      "['superfluid_pairing'] ['metastable_helium_atom'] -0.003\n",
      "['quantum_information_encoded'] ['single_phase'] -0.007\n",
      "['quantum_engineering'] ['discrete_rotational_symmetry'] 0.003\n",
      "['multivariate_polynomial'] ['volume_element'] 0.006\n",
      "['linear_velocity'] ['aspect_ratio'] 0.011\n",
      "['spectral_decomposition'] ['antiferromagnetic_interaction'] -0.006\n"
     ]
    }
   ],
   "source": [
    "# Initialize logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Load the best model\n",
    "load_best_model(model)\n",
    "\n",
    "# Final evaluation on the validation set\n",
    "indices, outputs_list, predicted_list = test_model(model, novel_dataloader)\n",
    "\n",
    "# Analyze predictions\n",
    "analyze_novel_predictions(indices, outputs_list, predicted_list, encoding_dat, c_inx_arr)\n",
    "\n",
    "# Print final validation accuracy\n",
    "logging.info(f\"\\nValidation Accuracy: {val_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6127632d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load the best model\n",
    "# model.load_state_dict(torch.load('best_model.pth'))\n",
    "\n",
    "# # Final evaluation on the validation set\n",
    "# model.eval()\n",
    "# correct_val = 0\n",
    "# total_val = 0\n",
    "\n",
    "# indices = []\n",
    "# outputs_list = []\n",
    "# correct_indices = []\n",
    "# labels_list = []\n",
    "\n",
    "# x = np.arange(31).reshape(-1, 1)\n",
    "# lin_model = LinearRegression()\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     for data, labels, inx in testing_dataloader:\n",
    "#         data = data.view(data.size(0), -1).float()  # Flatten the input data\n",
    "#         labels = labels.float()\n",
    "#         outputs = model(data)\n",
    "#         predicted = (outputs > 0.5).float()\n",
    "#         total_val += labels.size(0)\n",
    "#         correct_val += (predicted == labels).sum().item()\n",
    "        \n",
    "#         # Collect indices, outputs, labels, and correct predictions\n",
    "#         indices.extend(inx.cpu().numpy())\n",
    "#         outputs_list.extend(outputs.cpu().numpy())\n",
    "#         labels_list.extend(labels.cpu().numpy())\n",
    "#         correct_indices.extend((predicted == labels).cpu().numpy())\n",
    "        \n",
    "\n",
    "# # Convert lists to numpy arrays for sorting\n",
    "# indices = np.array(indices)\n",
    "# outputs_list = np.array(outputs_list).flatten()\n",
    "# labels_list = np.array(labels_list).flatten()\n",
    "# correct_indices = np.array(correct_indices).flatten()\n",
    "\n",
    "# # Get sorted indices of the outputs\n",
    "# sorted_indices = np.argsort(outputs_list)\n",
    "\n",
    "# # Separate the indices of correct predictions into two categories\n",
    "# correct_0 = []\n",
    "# correct_1 = []\n",
    "\n",
    "# for i in sorted_indices:\n",
    "#     if correct_indices[i]:\n",
    "#         if labels_list[i] == 0:\n",
    "#             correct_0.append(indices[i])\n",
    "#         else:\n",
    "#             correct_1.append(indices[i])\n",
    "\n",
    "# # Print indices of correct predictions\n",
    "# print(\"Correct predictions to have no co-occurance:\")\n",
    "# for cnt,idx in enumerate(correct_0):\n",
    "#     sim = similarity_cosine(encoding_dat[idx[0]][0],encoding_dat[idx[1]][0])\n",
    "#     lin_model.fit(x, sim.reshape(-1, 1))\n",
    "#     slope = lin_model.coef_[0][0]\n",
    "#     print(c_inx_arr[idx[0]],c_inx_arr[idx[1]], np.round(slope,3))\n",
    "#     if cnt ==5:\n",
    "#         break\n",
    "\n",
    "# print(\"\\n Correct predictions to have co-occurance:\")\n",
    "# for cnt,idx in enumerate(correct_1):\n",
    "#     sim = similarity_cosine(encoding_dat[idx[0]][0],encoding_dat[idx[1]][0])\n",
    "#     lin_model.fit(x, sim.reshape(-1, 1))\n",
    "#     slope = lin_model.coef_[0][0]\n",
    "#     print(c_inx_arr[idx[0]],c_inx_arr[idx[1]], np.round(slope,3))\n",
    "#     if cnt ==5:\n",
    "#         break\n",
    "\n",
    "# print(f\"\\nValidation Accuracy: {100 * correct_val / total_val:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "59e527d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Predictions to have no co-occurance:\n",
      "['promise_problem'] ['photon_radiation'] -0.011\n",
      "['jones_polynomial'] ['superradiant_emission'] -0.003\n",
      "['quantum_adiabatic_algorithm'] ['excited_state_lifetime'] -0.002\n",
      "['quantum_adiabatic_algorithm'] ['rydberg_excitons'] -0.001\n",
      "['spin_orientation'] ['quantum_secret_sharing_scheme'] -0.001\n",
      "['quantum_pcp_conjecture'] ['reflected_signal'] 0.004\n",
      "\n",
      " Predictions to have co-occurance:\n",
      "['long_range_anisotropic_interaction'] ['deep_strong_coupling'] -0.0\n",
      "['time_optimal_control'] ['harmonic_oscillator_mode'] 0.009\n",
      "['transfer_protocol'] ['variational_quantum_state'] -0.001\n",
      "['zitterbewegung_effect'] ['circular_ring'] -0.001\n",
      "['electric_field_fluctuation'] ['relaxation_oscillation'] -0.007\n",
      "['particle_number_fluctuation'] ['spinless_particle'] 0.008\n"
     ]
    }
   ],
   "source": [
    "# Load the best model\n",
    "model.load_state_dict(torch.load('best_model.pth'))\n",
    "\n",
    "# Final evaluation on the validation set\n",
    "model.eval()\n",
    "correct_val = 0\n",
    "total_val = 0\n",
    "\n",
    "indices = []\n",
    "outputs_list = []\n",
    "predicted_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data, inx in novel_dataloader:\n",
    "        data = data.view(data.size(0), -1).float()  # Flatten the input data\n",
    "        \n",
    "        outputs = model(data)\n",
    "        predicted = (outputs > 0.5).float()\n",
    "        \n",
    "        # Collect indices, outputs, labels, and correct predictions\n",
    "        indices.extend(inx.cpu().numpy())\n",
    "        outputs_list.extend(outputs.cpu().numpy())\n",
    "        predicted_list.extend(predicted.cpu().numpy())\n",
    "        \n",
    "# Convert lists to numpy arrays for sorting\n",
    "indices = np.array(indices)\n",
    "outputs_list = np.array(outputs_list).flatten()\n",
    "predicted_list = np.array(predicted_list).flatten()\n",
    "\n",
    "\n",
    "# Get sorted indices of the outputs\n",
    "sorted_indices = np.argsort(outputs_list)\n",
    "\n",
    "# Separate the indices of correct predictions into two categories\n",
    "correct_0 = []\n",
    "correct_1 = []\n",
    "\n",
    "for i in sorted_indices:\n",
    "    if predicted_list[i]:\n",
    "        correct_1.append(indices[i])\n",
    "    else:\n",
    "        correct_0.append(indices[i])\n",
    "\n",
    "# Print indices of correct predictions\n",
    "print(\" Predictions to have no co-occurance:\")\n",
    "for cnt,idx in enumerate(correct_0):\n",
    "    sim = similarity_cosine(encoding_dat[idx[0]][0],encoding_dat[idx[1]][0])\n",
    "    lin_model.fit(x, sim.reshape(-1, 1))\n",
    "    slope = lin_model.coef_[0][0]\n",
    "    print(c_inx_arr[idx[0]],c_inx_arr[idx[1]], np.round(slope,3))\n",
    "    if cnt ==5:\n",
    "        break\n",
    "\n",
    "print(\"\\n Predictions to have co-occurance:\")\n",
    "for cnt,idx in enumerate(correct_1):\n",
    "    sim = similarity_cosine(encoding_dat[idx[0]][0],encoding_dat[idx[1]][0])\n",
    "    lin_model.fit(x, sim.reshape(-1, 1))\n",
    "    slope = lin_model.coef_[0][0]\n",
    "    print(c_inx_arr[idx[0]],c_inx_arr[idx[1]],np.round(slope,3))\n",
    "    if cnt ==5:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
