{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4b06eeb-b7bc-4787-a65f-de3ef301bd4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gensim.models.phrases import Phraser\n",
    "import gensim\n",
    "from tqdm import tqdm\n",
    "from thefuzz import fuzz\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b39eefd",
   "metadata": {},
   "source": [
    "Our goal is now to take the preprocessed abstracts and identify the physics concepts within them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d81e22e-252e-4581-903e-5ea7197bb442",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_arx = pd.read_csv('saved_files/arxiv_preprocessed.csv',names=[\"id\",\"abstract\",\"date\"])\n",
    "ab_arr = df_arx[\"abstract\"].to_numpy()\n",
    "df_arx[\"date\"] = pd.to_datetime(df_arx[\"date\"])\n",
    "year_arr = df_arx['date'].dt.year.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2cfa720",
   "metadata": {},
   "source": [
    "As an example, this is a abstract after the preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc066fd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'light transport dense disordered cold atomic ensemble cooperation atomic dipole essentially modifies coupling radiation mode offer alternative approach light matter interfacing protocol cooperativity quasi static dipole interaction affect process light propagation condition electromagnetically induced transparency eit perform comparative analysis self consistent approach ab initio microscopic calculation emphasize role interatomic interaction dipolesdynamics result dense strongly disordered eit based light storage protocol stay relatively insensitive configuration variation obtained essentially atom normally needed dilute configuration'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.choice(ab_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b27ce53",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_atomic = pd.read_csv('saved_files/arxiv_atomic_concept.txt',names=[\"con\"])\n",
    "atomic_arr = np.array([con for con in df_atomic[\"con\"].to_numpy()])\n",
    "\n",
    "df_optic = pd.read_csv('saved_files/arxiv_optics_concept.txt',names=[\"con\"])\n",
    "optic_arr = np.array([con for con in df_optic[\"con\"].to_numpy()])\n",
    "\n",
    "df_quantum = pd.read_csv('saved_files/arxiv_quantum_concept.txt',names=[\"con\"])\n",
    "quantum_arr = np.array([con for con in df_quantum[\"con\"].to_numpy()])\n",
    "\n",
    "concept_compare_arr = np.unique(np.concatenate((atomic_arr,optic_arr,quantum_arr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a61bb550",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157821/157821 [00:33<00:00, 4657.79it/s]\n"
     ]
    }
   ],
   "source": [
    "# Create keyword lookup dictionary\n",
    "keyword_lookup = defaultdict(list)\n",
    "for keyword in concept_compare_arr:\n",
    "    keyword_lookup[keyword].append(keyword)\n",
    "\n",
    "# List to store modified abstracts\n",
    "modified_ab_arr = []\n",
    "matched_concepts = []\n",
    "\n",
    "# Iterate through abstracts\n",
    "for ab in tqdm(ab_arr):\n",
    "    ab_tokens = ab.split()\n",
    "    modified_ab_tokens = []\n",
    "    i = 0\n",
    "    while i < len(ab_tokens):\n",
    "        found_sequence = False\n",
    "        for j in range(6, 0, -1):  # Try different lengths of sequences in descending order\n",
    "            if i + j <= len(ab_tokens):\n",
    "                seq_tokens = ab_tokens[i:i + j]\n",
    "                seq_ = ' '.join(seq_tokens)\n",
    "                if seq_ in keyword_lookup:\n",
    "                    for keyword in keyword_lookup[seq_]:\n",
    "                        modified_ab_tokens.append('_'.join(seq_tokens))\n",
    "                        matched_concepts.append(keyword.replace(' ', '_'))\n",
    "                    i += j  # Move to the next position after the matched sequence\n",
    "                    found_sequence = True\n",
    "                    break\n",
    "        if not found_sequence:\n",
    "            modified_ab_tokens.append(ab_tokens[i])\n",
    "            i += 1\n",
    "    modified_ab_arr.append(' '.join(modified_ab_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cdeda0da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1036312323612418"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(matched_concepts).shape[0]/ quantum_arr.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2ed95f94",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157821/157821 [00:00<00:00, 165763.68it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "19161"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_word_count_subset(corpus, subset_words):\n",
    "    \n",
    "    for document in tqdm(corpus):\n",
    "        for word in document:\n",
    "            if word in subset_words:\n",
    "                subset_words[word] += 1\n",
    "    return subset_words\n",
    "\n",
    "# Compute word count for the subset of words \n",
    "word_count_subset = compute_word_count_subset([row.split() for row in modified_ab_arr], {k:0 for k in np.unique(matched_concepts)})\n",
    "\n",
    "cnt = 0 \n",
    "filtered_arr = []\n",
    "for k,v in word_count_subset.items():\n",
    "    if v > 4:\n",
    "        cnt += 1 \n",
    "        filtered_arr.append(k)\n",
    "cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a41517d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"saved_files/ngram_abstracts.npy\",modified_ab_arr)\n",
    "np.save(\"saved_files/overlapping_concepts.npy\",np.unique(matched_concepts))\n",
    "np.save(\"saved_files/overlapping_filtered_5_concepts.npy\",np.unique(filtered_arr))\n",
    "np.save(\"saved_files/year_arr.npy\",year_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df6af2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_ngrams(sentences):\n",
    "#     \"\"\" Detects n-grams with n up to 4, and replaces those in the abstracts. \"\"\"\n",
    "#     # Train a 2-word (bigram) phrase-detector\n",
    "#     bigram_phrases = gensim.models.phrases.Phrases(sentences,min_count=5,threshold=10)\n",
    "    \n",
    "#     # And construct a phraser from that (an object that will take a sentence\n",
    "#     # and replace in it the bigrams that it knows by single objects)\n",
    "#     bigram = gensim.models.phrases.Phraser(bigram_phrases)\n",
    "    \n",
    "#     # Repeat that for trigrams; the input now are the bigrammed-titles\n",
    "#     ngram_phrases = gensim.models.phrases.Phrases(bigram[sentences],min_count=5,threshold=10)\n",
    "#     ngram         = gensim.models.phrases.Phraser(ngram_phrases)\n",
    "    \n",
    "#     # !! If you want to have more than 4-grams, just repeat the structure of the\n",
    "#     #    above two lines. That is, train another Phrases on the ngram_phrases[titles],\n",
    "#     #    that will get you up to 8-grams. \n",
    "    \n",
    "#     # Now that we have phrasers for bi- and trigrams, let's analyze them\n",
    "#     # The phrases.export_phrases(x) function returns pairs of phrases and their\n",
    "#     # certainty scores from x.\n",
    "#     bigram_info = {}\n",
    "#     for b, score in bigram_phrases.export_phrases().items():\n",
    "#         bigram_info[b] = [score, bigram_info.get(b,[0,0])[1] + 1]\n",
    "#         len\n",
    "#     ngram_info = {}\n",
    "#     for b, score in ngram_phrases.export_phrases().items():\n",
    "#         ngram_info[b] = [score, ngram_info.get(b,[0,0])[1] + 1]\n",
    "    \n",
    "#     # Return a list of 'n-grammed' abtracts, and the bigram and trigram info\n",
    "#     return [ngram[t] for t in sentences], bigram_info, ngram_info\n",
    "\n",
    "# sentences = [row.split() for row in ab_arr]\n",
    "# ngram_abstracts, bigrams, ngrams = get_ngrams(sentences)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
