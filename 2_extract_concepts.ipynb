{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4b06eeb-b7bc-4787-a65f-de3ef301bd4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gensim.models.phrases import Phraser\n",
    "import gensim\n",
    "from tqdm import tqdm\n",
    "from thefuzz import fuzz\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b39eefd",
   "metadata": {},
   "source": [
    "Our goal is now to take the preprocessed 148564 abstracts and identify the physics concepts within them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d81e22e-252e-4581-903e-5ea7197bb442",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_arx = pd.read_csv('files/arxiv_preprocessed.csv',names=[\"id\",\"abstract\",\"date\"])\n",
    "ab_arr = df_arx[\"abstract\"].to_numpy()\n",
    "df_arx[\"date\"] = pd.to_datetime(df_arx[\"date\"])\n",
    "year_arr = df_arx['date'].dt.year.to_numpy()\n",
    "month_arr = df_arx['date'].dt.month.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "306d511a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "157821"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ab_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2cfa720",
   "metadata": {},
   "source": [
    "As an example, this is a abstract after the preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc066fd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'trapped bose einstein condensate subject action alternating external field coherent topological mode resonantly excited depending amplitude external field detuning parameter principally different regime motion mode locking change dynamic regime corresponds dynamic phase transition transition characterized effective order parameter defined difference fractional mode population averaged temporal period oscillation behavior order parameter function detuning pumping amplitude atomic interaction carefully analyzed special attention payed numerical calculation realistic case quadrupole exciting field parameter accessible current experiment'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.choice(ab_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b27ce53",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_atomic = pd.read_csv('files/arxiv_atomic_concept.txt',names=[\"con\"])\n",
    "atomic_arr = np.array([con for con in df_atomic[\"con\"].to_numpy()])\n",
    "\n",
    "df_optic = pd.read_csv('files/arxiv_optics_concept.txt',names=[\"con\"])\n",
    "optic_arr = np.array([con for con in df_optic[\"con\"].to_numpy()])\n",
    "\n",
    "df_quantum = pd.read_csv('files/arxiv_quantum_concept.txt',names=[\"con\"])\n",
    "quantum_arr = np.array([con for con in df_quantum[\"con\"].to_numpy()])\n",
    "\n",
    "concept_compare_arr = np.unique(np.concatenate((atomic_arr,optic_arr,quantum_arr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a61bb550",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157821/157821 [00:24<00:00, 6548.79it/s]\n"
     ]
    }
   ],
   "source": [
    "# Create keyword lookup dictionary\n",
    "keyword_lookup = defaultdict(list)\n",
    "for keyword in concept_compare_arr:\n",
    "    keyword_lookup[keyword].append(keyword)\n",
    "\n",
    "# List to store modified abstracts\n",
    "modified_ab_arr = []\n",
    "matched_concepts = []\n",
    "\n",
    "# Iterate through abstracts\n",
    "for ab in tqdm(ab_arr):\n",
    "    ab_tokens = ab.split()\n",
    "    modified_ab_tokens = []\n",
    "    i = 0\n",
    "    while i < len(ab_tokens):\n",
    "        found_sequence = False\n",
    "        for j in range(6, 0, -1):  # Try different lengths of sequences in descending order\n",
    "            if i + j <= len(ab_tokens):\n",
    "                seq_tokens = ab_tokens[i:i + j]\n",
    "                seq_ = ' '.join(seq_tokens)\n",
    "                if seq_ in keyword_lookup:\n",
    "                    for keyword in keyword_lookup[seq_]:\n",
    "                        modified_ab_tokens.append('_'.join(seq_tokens))\n",
    "                        matched_concepts.append(keyword.replace(' ', '_'))\n",
    "                    i += j  # Move to the next position after the matched sequence\n",
    "                    found_sequence = True\n",
    "                    break\n",
    "        if not found_sequence:\n",
    "            modified_ab_tokens.append(ab_tokens[i])\n",
    "            i += 1\n",
    "    modified_ab_arr.append(' '.join(modified_ab_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cdeda0da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1036312323612418"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(matched_concepts).shape[0]/ quantum_arr.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8aa3bb18",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"files/ngram_abstracts.npy\",modified_ab_arr)\n",
    "np.save(\"files/overlapping_concepts.npy\",np.unique(matched_concepts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "293c63e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'address question presence kerr_nonlinearity multiple_scattering optical_medium offer advantage respect design physical_unclonable_function result suggest certain condition nonlinear physical_unclonable_function robust potential cloning medium relative linear counterpart exploited context cryptographic application'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.choice(modified_ab_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "068764fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1036312323612418"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(matched_concepts).shape[0]/ quantum_arr.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2ed95f94",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157821/157821 [00:00<00:00, 206845.50it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "12770"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_word_count_subset(corpus, subset_words):\n",
    "    \n",
    "    for document in tqdm(corpus):\n",
    "        for word in document:\n",
    "            if word in subset_words:\n",
    "                subset_words[word] += 1\n",
    "    return subset_words\n",
    "\n",
    "# Compute word count for the subset of words \n",
    "word_count_subset = compute_word_count_subset([row.split() for row in modified_ab_arr], {k:0 for k in np.unique(matched_concepts)})\n",
    "\n",
    "cnt = 0 \n",
    "filtered_arr = []\n",
    "for k,v in word_count_subset.items():\n",
    "    if v>10:\n",
    "        cnt += 1 \n",
    "        filtered_arr.append(k)\n",
    "cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a41517d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "np.save(\"files/overlapping_filtered_concepts.npy\",np.unique(filtered_arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "98c909e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save(\"files/ngram_abstracts.npy\",ab_arr.tolist())\n",
    "# np.save(\"files/overlapping_concepts.npy\",np.unique(found_concept_list))\n",
    "# np.save(\"files/year_arr.npy\",year_arr)\n",
    "# np.save(\"files/month_arr.npy\",month_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8898a48c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stop' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mstop\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'stop' is not defined"
     ]
    }
   ],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df6af2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ngrams(sentences):\n",
    "    \"\"\" Detects n-grams with n up to 4, and replaces those in the abstracts. \"\"\"\n",
    "    # Train a 2-word (bigram) phrase-detector\n",
    "    bigram_phrases = gensim.models.phrases.Phrases(sentences,min_count=5,threshold=10)\n",
    "    \n",
    "    # And construct a phraser from that (an object that will take a sentence\n",
    "    # and replace in it the bigrams that it knows by single objects)\n",
    "    bigram = gensim.models.phrases.Phraser(bigram_phrases)\n",
    "    \n",
    "    # Repeat that for trigrams; the input now are the bigrammed-titles\n",
    "    ngram_phrases = gensim.models.phrases.Phrases(bigram[sentences],min_count=5,threshold=10)\n",
    "    ngram         = gensim.models.phrases.Phraser(ngram_phrases)\n",
    "    \n",
    "    # !! If you want to have more than 4-grams, just repeat the structure of the\n",
    "    #    above two lines. That is, train another Phrases on the ngram_phrases[titles],\n",
    "    #    that will get you up to 8-grams. \n",
    "    \n",
    "    # Now that we have phrasers for bi- and trigrams, let's analyze them\n",
    "    # The phrases.export_phrases(x) function returns pairs of phrases and their\n",
    "    # certainty scores from x.\n",
    "    bigram_info = {}\n",
    "    for b, score in bigram_phrases.export_phrases().items():\n",
    "        bigram_info[b] = [score, bigram_info.get(b,[0,0])[1] + 1]\n",
    "        len\n",
    "    ngram_info = {}\n",
    "    for b, score in ngram_phrases.export_phrases().items():\n",
    "        ngram_info[b] = [score, ngram_info.get(b,[0,0])[1] + 1]\n",
    "    \n",
    "    # Return a list of 'n-grammed' abtracts, and the bigram and trigram info\n",
    "    return [ngram[t] for t in sentences], bigram_info, ngram_info\n",
    "\n",
    "sentences = [row.split() for row in ab_arr]\n",
    "ngram_abstracts, bigrams, ngrams = get_ngrams(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a38cdc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "found_concept_dict = {k:1 for k in found_concept_list}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b1fd8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30078/30078 [00:00<00:00, 1679530.79it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 43990/43990 [00:00<00:00, 1608945.49it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9640"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnt = 0 \n",
    "for n in tqdm(bigrams.keys()):\n",
    "    if n in found_concept_dict:\n",
    "        cnt += 1 \n",
    "\n",
    "for n in tqdm(ngrams.keys()):\n",
    "    if n in found_concept_dict:\n",
    "        cnt += 1 \n",
    "cnt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce88ad4",
   "metadata": {},
   "source": [
    "This gives us a list of 10565 ngrams which contain physics concepts as well as some natural language we are not interested in."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e084e54",
   "metadata": {},
   "source": [
    "We now load a given list of suitable physics concepts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9179800",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_atomic = pd.read_csv('files/arxiv_atomic_concept.txt',names=[\"con\"])\n",
    "atomic_arr = np.array([con.replace(\" \", \"_\") for con in df_atomic[\"con\"].to_numpy()])\n",
    "\n",
    "df_optic = pd.read_csv('files/arxiv_optics_concept.txt',names=[\"con\"])\n",
    "optic_arr = np.array([con.replace(\" \", \"_\") for con in df_optic[\"con\"].to_numpy()])\n",
    "\n",
    "df_quantum = pd.read_csv('files/arxiv_quantum_concept.txt',names=[\"con\"])\n",
    "quantum_arr = np.array([con.replace(\" \", \"_\") for con in df_quantum[\"con\"].to_numpy()])\n",
    "\n",
    "concept_compare_arr = np.unique(np.concatenate((atomic_arr,optic_arr,quantum_arr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287420a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26575,)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(quantum_arr).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef3cd0a",
   "metadata": {},
   "source": [
    "and do a naive preprecess, where we check if there are some direct matches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755ea4bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found  5883 entries directly\n",
      "Check  38108 further\n"
     ]
    }
   ],
   "source": [
    "# pre_selec_inx = np.array([c in ngrams for c in concept_compare_arr]).astype(int)\n",
    "pre_selec_inx = np.array([c in concept_compare_arr for c in list(ngrams.keys())]).astype(int)\n",
    "\n",
    "\n",
    "concept_pre_compare_arr = np.array(list(ngrams.keys()))[pre_selec_inx==1]\n",
    "concept_continue_compare_arr = np.array(list(ngrams.keys()))[pre_selec_inx==0]\n",
    "print(\"Found \",concept_pre_compare_arr.shape[0],\"entries directly\") \n",
    "print(\"Check \",concept_continue_compare_arr.shape[0],\"further\")      \n",
    "\n",
    "inx_arr = np.array([1 if any(symbol in concept for symbol in ['<', '>', '(', ')', '#', '~', '*', '=', '[', ']']) else 0 \n",
    "           for concept in concept_continue_compare_arr])\n",
    "\n",
    "\n",
    "concept_continue_compare_arr = np.unique(concept_continue_compare_arr[inx_arr==0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88249544",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 38108/38108 [16:27<00:00, 38.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7719, 3) (30389,)\n"
     ]
    }
   ],
   "source": [
    "from joblib import Parallel, delayed\n",
    "# import numpy as np\n",
    "# from tqdm import tqdm\n",
    "# from fuzzywuzzy import fuzz\n",
    "\n",
    "def process_concept(concept1, concepts2, threshold):\n",
    "    match = False\n",
    "    if any(char in concept1 for char in ['<', '>', '(', ')', '#', '~', '*', '=', \"[\", \"]\"]):\n",
    "        return concept1, None\n",
    "\n",
    "    similarity_ratio = 0  # Initialize similarity_ratio\n",
    "    for concept2 in concepts2:\n",
    "        current_ratio = fuzz.partial_token_sort_ratio(concept1, concept2)\n",
    "        if current_ratio >= threshold and len(concept1) > 3:\n",
    "            match = True\n",
    "            similarity_ratio = current_ratio  # Update similarity_ratio\n",
    "            return concept1, concept2, similarity_ratio\n",
    "    if not match:\n",
    "        return concept1, None, similarity_ratio  # Return similarity_ratio\n",
    "\n",
    "def find_overlapping_concepts(concepts1, concepts2, threshold):\n",
    "    # Parallelize the processing of concepts1\n",
    "    results = Parallel(n_jobs=-1)(delayed(process_concept)(concept1, concepts2, threshold) for concept1 in tqdm(concepts1))\n",
    "    overlapping_concepts = [(concept1, concept2, similarity_ratio) for concept1, concept2, similarity_ratio in results if concept2 is not None]\n",
    "    non_overlapping_concepts = [concept1 for concept1, concept2, similarity_ratio in results if concept2 is None]\n",
    "    return np.array(overlapping_concepts), np.array(non_overlapping_concepts)\n",
    "\n",
    "# Example usage\n",
    "overlapping_concepts, non_overlapping_concepts = find_overlapping_concepts(concept_continue_compare_arr, concept_compare_arr, threshold=95)\n",
    "print(overlapping_concepts.shape, non_overlapping_concepts.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f7c34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"files/ngram_abstracts.npy\",[' '.join(ab) for ab in ngram_abstracts])\n",
    "np.save(\"files/overlapping_concepts.npy\",np.unique(np.concatenate((concept_pre_compare_arr, overlapping_concepts[:,0]))))\n",
    "np.save(\"files/year_arr.npy\",year_arr)\n",
    "np.save(\"files/month_arr.npy\",month_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eff8888",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def find_overlapping_concepts(concepts1, concepts2, threshold):\n",
    "#     # concepts1 are the various ngrams that were extracted with no regard about physics\n",
    "#     # concepts2 are the physics concepts we are in general interested in\n",
    "#     # Goal of this function is to compare the overlap, at a given threshhold the concept is assumed to match\n",
    "#     overlapping_concepts = []\n",
    "#     non_overlapping_concepts = []\n",
    "\n",
    "#     for concept1 in tqdm(concepts1):\n",
    "#         match = False \n",
    "#         if any(char in concept1 for char in ['<', '>', '(', ')','#', '~', '*', '=',\"[\",\"]\"]):\n",
    "#             non_overlapping_concepts.append(concept1)\n",
    "#             continue \n",
    "\n",
    "#         for concept2 in concepts2:\n",
    "\n",
    "#             # Use fuzz ratio to measure similarity\n",
    "            \n",
    "#             similarity_ratio = fuzz.partial_token_sort_ratio(concept1, concept2)\n",
    "           \n",
    "#             # Adjust the threshold based on your requirements\n",
    "#             if similarity_ratio >= threshold and len(concept1)>3:\n",
    "#                 overlapping_concepts.append((concept1, concept2, similarity_ratio))\n",
    "#                 match = True \n",
    "#                 break \n",
    "#         if not match:\n",
    "#             non_overlapping_concepts.append(concept1) \n",
    "\n",
    "#     return np.array(overlapping_concepts), np.array(non_overlapping_concepts)\n",
    "\n",
    "\n",
    "# overlapping_concepts, non_overlapping_concepts = find_overlapping_concepts(concept_continue_compare_arr,concept_compare_arr,threshold=90)\n",
    "# overlapping_concepts.shape, non_overlapping_concepts.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07378bfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['adjacency_laplacian_matrix' 'laplacian_matrix' '100']\n",
      "['adjacency_matrix_model' 'adjacency_matrix' '100']\n",
      "['approximated_matrix_product' 'matrix_product' '100']\n",
      "['bistochastic_map' 'bistochastic_matrix' '97']\n",
      "['complex_hadamard' 'complex_hadamard_matrix' '100']\n",
      "['corner_transfer_matrix' 'transfer_matrix' '100']\n",
      "['density_matrix_exponentiation' 'matrix_exponentiation' '100']\n",
      "['densitymatrix_element' 'density_matrix' '96']\n",
      "['densitymatrix_renormalization' 'density_matrix' '96']\n",
      "['densitymatrix_renormalization_group' 'density_matrix' '96']\n",
      "['densitymatrix_renormalization_group_dmrg' 'density_matrix' '96']\n",
      "['densitymatrix_renormalizationgroup' 'density_matrix' '96']\n",
      "['densitymatrix_simulation' 'density_matrix' '96']\n",
      "['diagonal_element' 'diagonal_density_matrix_element' '100']\n",
      "['diagonal_matrix_element' 'diagonal_density_matrix_element' '100']\n",
      "['dipole_matrix_element' 'dipole_transition_matrix_element' '100']\n",
      "['inclusive_scattering_matrix' 'scatter_matrix' '100']\n",
      "['injective_matrix_product' 'matrix_product' '100']\n",
      "['invertible_map' 'invertible_matrix' '96']\n",
      "['jmatrix_method' 'matrix_method' '100']\n",
      "['jmatrix_method_scattering' 'matrix_method' '100']\n",
      "['kqubit_reduced' 'qubit_reduced_density_matrix' '96']\n",
      "['lh_complex' 'complex_hadamard_matrix' '95']\n",
      "['lowrank_matrix_recovery' 'matrix_recovery' '100']\n",
      "['matrix_rhoa' 'density_matrix_rho' '95']\n",
      "['matrix_u' 'matrix_unit' '100']\n",
      "['matrixproduct_operator' 'matrix_product' '96']\n",
      "['matrixproduct_state' 'continuous_matrix_product_state' '95']\n",
      "['matrixproduct_state_mp' 'matrix_product' '96']\n",
      "['matrixproductstate_mp' 'matrix_product' '96']\n",
      "['matrixproductstate_simulation' 'matrix_product' '96']\n",
      "['matrixvalued_function' 'matrix_function' '100']\n",
      "['mq_coherence' 'coherence_matrix' '96']\n",
      "['offdiagonal_element_density_matrix' 'density_matrix_element' '100']\n",
      "['offdiagonal_matrix_element' 'matrix_element' '100']\n",
      "['orthogonal_ax' 'orthogonal_matrix' '96']\n",
      "['permanent_matrix' 'matrix_permanent' '100']\n",
      "['positive_semidefinite' 'positive_semidefinite_matrix' '100']\n",
      "['product_vector' 'matrix_vector_product' '100']\n",
      "['random_matrix' 'matrix_rank' '95']\n",
      "['random_matrix_theory_rmt' 'matrix_rank' '95']\n",
      "['reduced_densitymatrix' 'density_matrix' '96']\n",
      "['rotation_ax' 'rotation_matrix' '95']\n",
      "['smatrix_element' 'coupling_matrix_element' '97']\n",
      "['spindensity_matrix' 'spin_matrix' '100']\n",
      "['timedependent_densitymatrix' 'density_matrix' '96']\n",
      "['timedependent_matrixproductstate' 'matrix_product' '96']\n",
      "['timeevolving_matrix_product' 'matrix_product' '100']\n",
      "['tmatrix_element' 'coupling_matrix_element' '97']\n",
      "['translational_invariant_matrix_product' 'matrix_product' '100']\n",
      "['translationally_invariant_matrix_product' 'matrix_product' '100']\n",
      "['tridiagonal_matrix_representation' 'matrix_representation' '100']\n",
      "['uniform_matrix_product' 'matrix_product' '100']\n"
     ]
    }
   ],
   "source": [
    "for cnt,_ in enumerate(overlapping_concepts):\n",
    "    if \"matrix\" in _[1]:\n",
    "        print(_)\n",
    "        # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8bcb645",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['multilevel_atom', 'ohmic_spectral', 'box_trap',\n",
       "       'significantly_fewer', 'significantly_improve',\n",
       "       'purely_dispersive', 'sachdevyekitaev_syk_model',\n",
       "       'exponential_growth', 'tightbinding_lattice',\n",
       "       'depends_sensitively', 'achieving_highfidelity',\n",
       "       'ubiquitous_nature', 'kicking_strength', 'array_fpga',\n",
       "       'uniformly_bounded', 'outofplane_magnetic',\n",
       "       'photonnumber_resolving', 'criticism_raised', 'nuclear_physic',\n",
       "       'computed_analytically', 'diode_spad', 'singlephoton_pulse',\n",
       "       'guarantee_existence', 'relatively_insensitive', 'otimes_otimes',\n",
       "       'provide_pedagogical', 'timeevolving_block', 'markov_process',\n",
       "       'despite_decade', 'quasiperiodic_disorder'], dtype='<U51')"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.choice(non_overlapping_concepts,size=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3efcf9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'p' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[94], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mp\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'p' is not defined"
     ]
    }
   ],
   "source": [
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31a119f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert your array into a dataframe\n",
    "# df = pd.DataFrame(np.concatenate((np.sort(concept_continue_compare_arr).reshape(-1,1),np.sort(concept_continue_compare_arr).reshape(-1,1)),axis=-1))  \n",
    "df = pd.DataFrame(np.concatenate((np.sort(overlapping_concepts[:,0]).reshape(-1,1),np.sort(overlapping_concepts[:,0]).reshape(-1,1)),axis=-1))\n",
    "filepath = 'check_concept.csv'\n",
    "df.to_csv(filepath, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ccb143",
   "metadata": {},
   "source": [
    "# Takes the hand inspected .csv file and extracts concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97209779",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_repl = pd.read_csv(\"my_excel_file.csv\")\n",
    "repl_arr = df_repl.to_numpy()\n",
    "non_phys_arr = []\n",
    "replace_phys_arr = {}\n",
    "match_phys_arr = []\n",
    "cnt = 0 \n",
    "for tupel in repl_arr:\n",
    "    if tupel[1]==\"-\":\n",
    "        non_phys_arr.append(tupel[0])\n",
    "        # remove from phys concept list\n",
    "    elif tupel[0] != tupel[1]:\n",
    "        replace_phys_arr[tupel[0]] = tupel[1] \n",
    "        \n",
    "    else:\n",
    "        match_phys_arr.append(tupel[1])\n",
    "        \n",
    "        # replace in all abstracts and add to phys concept list\n",
    "\n",
    "\n",
    "ngram_abstracts_repl = ngram_abstracts.copy()\n",
    "cnt = 0 \n",
    "for sublist in ngram_abstracts:\n",
    "    for i in range(len(sublist)):\n",
    "        if sublist[i] in replace_phys_arr:\n",
    "            sublist[i] = replace_phys_arr[sublist[i]]\n",
    "            cnt += 1 \n",
    "\n",
    "np.save(\"files/ngram_abstracts_repl.npy\",[' '.join(ab) for ab in ngram_abstracts_repl])\n",
    "np.save(\"files/overlapping_save_concepts.npy\",np.unique(np.concatenate((concept_pre_compare_arr,match_phys_arr,list(replace_phys_arr.values())))))\n",
    "np.save(\"files/non_overlapping_save_concepts.npy\",non_phys_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30014c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ngram_abstracts_repl = ngram_abstracts.copy()\n",
    "# words_to_replace_set = set(non_overlapping_concepts)\n",
    "# cnt = 0 \n",
    "# for sublist in ngram_abstracts:\n",
    "#     for i in range(len(sublist)):\n",
    "#         if sublist[i] in words_to_replace_set:\n",
    "#             sublist[i] = sublist[i].replace(\"_\", \" \")\n",
    "#             cnt += 1 \n",
    "\n",
    "# np.save(\"files/ngram_abstracts_repl.npy\",[' '.join(ab) for ab in ngram_abstracts_repl])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb320e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# phys_conc = set(np.unique(np.concatenate((overlapping_concepts[:,0],concept_pre_compare_arr))))\n",
    "\n",
    "# sortedns  = sorted( [(ngrams[b][0], ngrams[b][1], b) for b in ngrams.keys()] )[::-1]\n",
    "# print(\"Top bigrams by certainty:\")\n",
    "# i = 0 \n",
    "# cnt = 0 \n",
    "# while cnt < 20:\n",
    "#     if sortedns[i][2] in phys_conc:\n",
    "#         print(\"{0:2}: {1:50} \\t({2}) \".format(i+1, str(sortedns[i][2]), \"%.2f\"%sortedns[i][0]))\n",
    "#         cnt += 1 \n",
    "#     i+= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e396ed5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# phys_conc = set(np.unique(np.concatenate((overlapping_concepts[:,0],concept_pre_compare_arr))))\n",
    "\n",
    "# sortedns  = sorted( [(ngrams[b][0], ngrams[b][1], b) for b in ngrams.keys()] )#[::-1]\n",
    "# print(\"Least certain bigrams:\")\n",
    "# i = 0 \n",
    "# cnt = 0 \n",
    "# while cnt < 20:\n",
    "#     if sortedns[i][2] in phys_conc:\n",
    "#         print(\"{0:2}: {1:50} \\t({2}) \".format(i+1, str(sortedns[i][2]), \"%.2f\"%sortedns[i][0]))\n",
    "#         cnt += 1 \n",
    "#     i+= 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
