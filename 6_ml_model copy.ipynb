{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4b06eeb-b7bc-4787-a65f-de3ef301bd4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "from statsmodels.tsa.stattools import adfuller, acf, pacf, ccf\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import re\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81da614f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "311720ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity_cosine(vec1, vec2):\n",
    "    cosine_similarity_arr = []\n",
    "    for v1,v2 in zip(vec1, vec2):\n",
    "        cosine_similarity = np.dot(v1, v2)/(np.linalg.norm(v1)* np.linalg.norm(v2))\n",
    "        cosine_similarity_arr.append(cosine_similarity)\n",
    "    return np.array(cosine_similarity_arr)\n",
    "\n",
    "def keep_words_with_underscore(input_string):\n",
    "    # Define a regular expression pattern to match words with underscores\n",
    "    pattern = r'\\b\\w*_[\\w_]*\\b'\n",
    "\n",
    "    # Use re.findall to extract words that match the pattern\n",
    "    matching_words = re.findall(pattern, input_string)\n",
    "\n",
    "    # Join the matching words to form the final string\n",
    "    result = ' '.join(matching_words)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def update_co_occurrences(word_year_list,word_co_occurrences):\n",
    "    # Iterate through the words in the list\n",
    "    word_list, year = word_year_list\n",
    "    \n",
    "    for word in word_list:\n",
    "        # If the word is not already in the dictionary, add it with an empty list\n",
    "        if word not in word_co_occurrences:\n",
    "            word_co_occurrences[word] = {}\n",
    "        \n",
    "        # Add words from the list to the co-occurrence list for the current word\n",
    "        for other_word in word_list:\n",
    "            # if other_word != word and other_word not in word_co_occurrences[word]:\n",
    "            #     word_co_occurrences[word].append(other_word)\n",
    "            if other_word != word and other_word not in word_co_occurrences[word]:\n",
    "                word_co_occurrences[word][other_word] = [year] \n",
    "            \n",
    "            elif other_word != word and other_word in word_co_occurrences[word]:\n",
    "                # word_co_occurrences[word][other_word][0] +=1\n",
    "                word_co_occurrences[word][other_word].append(year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6094b45a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concepts which were tracked (12770,)\n",
      "Abstracts (157821,)\n",
      "Year associated to abstract (157821,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 152310/152310 [00:07<00:00, 21155.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pairs of concepts which co-occur after 2021 (889874, 2)\n"
     ]
    }
   ],
   "source": [
    "concept_filtered_arr = np.load(\"files/overlapping_filtered_concepts.npy\")\n",
    "ngram_abstracts = np.load(\"files/ngram_abstracts.npy\", mmap_mode=\"r\")\n",
    "saved_year_arr = np.load(\"files/year_arr.npy\", mmap_mode=\"r\")\n",
    "\n",
    "print(\"Concepts which were tracked\",concept_filtered_arr.shape)\n",
    "print(\"Abstracts\",ngram_abstracts.shape)\n",
    "print(\"Year associated to abstract\",saved_year_arr.shape)\n",
    "\n",
    "phys_filtered_concept_dict = {k:1 for k in concept_filtered_arr}\n",
    "ocurr_arr = []\n",
    "for abstract, year in zip(ngram_abstracts, saved_year_arr):\n",
    "    temp = keep_words_with_underscore(abstract)\n",
    "    if temp.count(\" \") > 0:\n",
    "        temp = temp.split(\" \") \n",
    "        temp = [s for s in temp if s in phys_filtered_concept_dict]\n",
    "        ocurr_arr.append([list(filter((\"_\").__ne__, temp)),year])\n",
    "                        \n",
    "word_co_occurrences = {}\n",
    "\n",
    "for word_list in tqdm(ocurr_arr):\n",
    "    update_co_occurrences(word_list,word_co_occurrences)\n",
    "\n",
    "# Concepts which cooccur after 2021 \n",
    "def get_co_occur_concept_pair_after_year_arr(word_co_occurrences, first_occ_year, final_occ_year):\n",
    "    cnt = 0 \n",
    "    co_occur_concept_pair_arr = []\n",
    "    for concept, v in word_co_occurrences.items():\n",
    "        for co_concept, years in v.items():\n",
    "            if np.min(years) >= first_occ_year:\n",
    "                co_occur_concept_pair_arr.append([concept,co_concept])\n",
    "                cnt += 1 \n",
    "\n",
    "\n",
    "    return np.array(co_occur_concept_pair_arr)\n",
    "\n",
    "# example \n",
    "print(\"Pairs of concepts which co-occur after 2021\", get_co_occur_concept_pair_after_year_arr(word_co_occurrences, 2021,2023).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "549adfca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Representation Vectors for tracked concepts (12368, 31, 128)\n",
      "Concept associted with representation (12368,)\n",
      "... [2006 2007 2008]  -> Training Window [2009 2010 2011 2012 2013 2014 2015 2016 2017 2018]  <- [2019 2020 2021 2022 2023 2024]\n",
      "... [2016 2017 2018]  -> Label Window [2019 2020 2021]  <- [2022 2023 2024]\n",
      "\n",
      "... [2009 2010 2011]  -> Training Window [2012 2013 2014 2015 2016 2017 2018 2019 2020 2021]  <- [2022 2023 2024]\n",
      "... [2019 2020 2021]  -> Label Window [2022 2023 2024]  <- []\n",
      "\n",
      "... [2012 2013 2014]  -> Training Window [2015 2016 2017 2018 2019 2020 2021 2022 2023 2024]  <- []\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "import numpy as np\n",
    "\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, data, word_co_occurrences, year_arr, c_inx_arr, input_window_size = 5, output_window_size = 3, offset_to_current_year = 1):\n",
    "        self.train_window_data = data[:,-input_window_size-output_window_size-offset_to_current_year:-output_window_size-offset_to_current_year]\n",
    "        if offset_to_current_year == 0:\n",
    "            # self.label_window_data = data[:,-output_window_size:]\n",
    "            self.label_year_range = year_arr[-output_window_size:]\n",
    "        else: \n",
    "            # self.label_window_data = data[:,-output_window_size-offset_to_current_year:-offset_to_current_year]\n",
    "            self.label_year_range = year_arr[-output_window_size-offset_to_current_year:-offset_to_current_year]\n",
    "\n",
    "        \n",
    "        self.co_occur_concept_pair_arr = get_co_occur_concept_pair_after_year_arr(word_co_occurrences, first_occ_year=self.label_year_range[0], final_occ_year=self.label_year_range[-1])\n",
    "        self.c_inx_arr = c_inx_arr\n",
    "        self.input_window_size = input_window_size \n",
    "        self.output_window_size = output_window_size \n",
    "        self.offset_to_current_year = offset_to_current_year \n",
    "\n",
    "    def __len__(self):\n",
    "        return 64*500\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        if np.random.rand() < 0.5:\n",
    "            return self._get_positive_sample()\n",
    "        else:\n",
    "            return self._get_negative_sample()\n",
    "    \n",
    "\n",
    "    def _get_positive_sample(self):\n",
    "        while True:\n",
    "            sampled_pairs = np.random.choice(len(self.co_occur_concept_pair_arr), size=1)\n",
    "            c_pair = self.co_occur_concept_pair_arr[sampled_pairs][0]\n",
    "            inx_0 = np.where(self.c_inx_arr == c_pair[0])[0]\n",
    "            inx_1 = np.where(self.c_inx_arr == c_pair[1])[0]\n",
    "            if inx_0.size > 0 and inx_1.size > 0:\n",
    "                break\n",
    "        enc_0 = self.train_window_data[inx_0][0]\n",
    "        enc_1 = self.train_window_data[inx_1][0]\n",
    "        enc_01 = np.concatenate((enc_0, enc_1), axis=-1)\n",
    "        return torch.from_numpy(enc_01), torch.ones(1), torch.from_numpy(np.array([inx_0,inx_1]))\n",
    "\n",
    "    def _get_negative_sample(self):\n",
    "        while True:\n",
    "            sampled_pair = np.random.choice(self.train_window_data.shape[0], size=2)\n",
    "            if self.c_inx_arr[sampled_pair[1]] not in word_co_occurrences[self.c_inx_arr[sampled_pair[0]]]:\n",
    "                break\n",
    "        inx_0 = np.where(self.c_inx_arr == self.c_inx_arr[sampled_pair[0]])[0]\n",
    "        inx_1 = np.where(self.c_inx_arr == self.c_inx_arr[sampled_pair[1]])[0]\n",
    "        enc_0 = self.train_window_data[inx_0][0]\n",
    "        enc_1 = self.train_window_data[inx_1][0]\n",
    "        enc_01 = np.concatenate((enc_0, enc_1), axis=-1)\n",
    "        return torch.from_numpy(enc_01), torch.zeros(1), torch.from_numpy(np.array([inx_0,inx_1]))\n",
    "    \n",
    "    def _check_indexing(self):\n",
    "        if self.offset_to_current_year != 0 :\n",
    "            print(f\"... {np.unique(saved_year_arr)[-self.input_window_size-self.output_window_size-self.offset_to_current_year-3:-self.input_window_size-self.output_window_size-self.offset_to_current_year]}\",f\" -> Training Window {np.unique(saved_year_arr)[-self.input_window_size-self.output_window_size-self.offset_to_current_year:-self.output_window_size-self.offset_to_current_year]}\",f\" <- {np.unique(saved_year_arr)[-self.output_window_size-self.offset_to_current_year:]}\")\n",
    "            print(f\"... {np.unique(saved_year_arr)[-self.output_window_size-self.offset_to_current_year-3:-self.output_window_size-self.offset_to_current_year]}\",f\" -> Label Window {np.unique(saved_year_arr)[-self.output_window_size-self.offset_to_current_year:-self.offset_to_current_year]}\",f\" <- {np.unique(saved_year_arr)[-self.offset_to_current_year:]}\")\n",
    "        else:\n",
    "            print(f\"... {np.unique(saved_year_arr)[-self.input_window_size-self.output_window_size-3:-self.input_window_size-self.output_window_size]}\",f\" -> Training Window {np.unique(saved_year_arr)[-self.input_window_size-self.output_window_size:-self.output_window_size]}\",f\" <- {np.unique(saved_year_arr)[-self.output_window_size:]}\")\n",
    "            print(f\"... {np.unique(saved_year_arr)[-self.output_window_size-3:-self.output_window_size]}\",f\" -> Label Window {np.unique(saved_year_arr)[-self.output_window_size:]}\",f\" <- {[]}\")\n",
    "        \n",
    "class NovelSeriesDataset(Dataset):\n",
    "    def __init__(self, data, c_inx_arr, input_window_size = 5):\n",
    "        self.train_window_data = data[:,-input_window_size:]\n",
    "        \n",
    "        self.c_inx_arr = c_inx_arr\n",
    "        self.input_window_size = input_window_size \n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return 64*500\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        while True:\n",
    "            sampled_pair = np.random.choice(self.train_window_data.shape[0], size=2)\n",
    "            if self.c_inx_arr[sampled_pair[1]] not in word_co_occurrences[self.c_inx_arr[sampled_pair[0]]]:\n",
    "                break\n",
    "        inx_0 = np.where(self.c_inx_arr == self.c_inx_arr[sampled_pair[0]])[0]\n",
    "        inx_1 = np.where(self.c_inx_arr == self.c_inx_arr[sampled_pair[1]])[0]\n",
    "        enc_0 = self.train_window_data[inx_0][0]\n",
    "        enc_1 = self.train_window_data[inx_1][0]\n",
    "        enc_01 = np.concatenate((enc_0, enc_1), axis=-1)\n",
    "        return torch.from_numpy(enc_01), torch.from_numpy(np.array([inx_0,inx_1]))\n",
    "    \n",
    "    def _check_indexing(self):\n",
    "        \n",
    "        print(f\"... {np.unique(saved_year_arr)[-self.input_window_size-3:-self.input_window_size]}\",f\" -> Training Window {np.unique(saved_year_arr)[-self.input_window_size:]}\",f\" <- {[]}\")\n",
    "        \n",
    "\n",
    "\n",
    "   \n",
    "\n",
    "# Example usage:\n",
    "num_samples_per_class = 32\n",
    "num_features = 128\n",
    "seq_length = 5\n",
    "batch_size = 128\n",
    "\n",
    "encoding_dat = np.load(\"c_encoding_arr.npy\")\n",
    "c_inx_arr = np.load(\"c_inx_arr.npy\")\n",
    "print(\"Representation Vectors for tracked concepts\",encoding_dat.shape)\n",
    "print(\"Concept associted with representation\", c_inx_arr.shape)\n",
    "scaler = RobustScaler()\n",
    "reshaped_data = encoding_dat.reshape(-1, encoding_dat.shape[-1])  # Shape: (10000*31, 128)\n",
    "normalized_data = scaler.fit_transform(reshaped_data)\n",
    "encoding_data = normalized_data.reshape(encoding_dat.shape)\n",
    "\n",
    "dataset = TimeSeriesDataset(data=encoding_data, word_co_occurrences=word_co_occurrences, year_arr=np.unique(saved_year_arr), \n",
    "                            c_inx_arr=c_inx_arr, input_window_size = 10, output_window_size = 3, offset_to_current_year = 3)\n",
    "dataset._check_indexing()\n",
    "print()\n",
    "# dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "testing_dataset = TimeSeriesDataset(data=encoding_data, word_co_occurrences=word_co_occurrences, year_arr=np.unique(saved_year_arr), \n",
    "                            c_inx_arr=c_inx_arr, input_window_size = 10, output_window_size = 3, offset_to_current_year = 0)\n",
    "testing_dataset._check_indexing()\n",
    "testing_dataloader = DataLoader(testing_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "print()\n",
    "novel_dataset = NovelSeriesDataset(data=encoding_data, c_inx_arr=c_inx_arr, input_window_size = 10)\n",
    "novel_dataset._check_indexing()\n",
    "novel_dataloader = DataLoader(novel_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3166a6db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Train Loss: 0.6096, Train Accuracy: 65.98%, Val Loss: 0.5685, Val Accuracy: 71.39%\n",
      "Epoch [2/50], Train Loss: 0.5494, Train Accuracy: 73.30%, Val Loss: 0.5295, Val Accuracy: 75.03%\n",
      "Epoch [3/50], Train Loss: 0.5318, Train Accuracy: 74.49%, Val Loss: 0.5194, Val Accuracy: 74.83%\n",
      "Epoch [4/50], Train Loss: 0.5242, Train Accuracy: 74.78%, Val Loss: 0.5320, Val Accuracy: 74.64%\n",
      "Epoch [5/50], Train Loss: 0.5143, Train Accuracy: 75.38%, Val Loss: 0.5162, Val Accuracy: 75.20%\n",
      "Epoch [6/50], Train Loss: 0.5014, Train Accuracy: 76.34%, Val Loss: 0.5112, Val Accuracy: 75.38%\n",
      "Epoch [7/50], Train Loss: 0.4929, Train Accuracy: 76.71%, Val Loss: 0.5055, Val Accuracy: 75.23%\n",
      "Epoch [8/50], Train Loss: 0.4963, Train Accuracy: 76.38%, Val Loss: 0.4894, Val Accuracy: 76.72%\n",
      "Epoch [9/50], Train Loss: 0.4968, Train Accuracy: 76.28%, Val Loss: 0.4849, Val Accuracy: 77.61%\n",
      "Epoch [10/50], Train Loss: 0.4969, Train Accuracy: 76.13%, Val Loss: 0.4873, Val Accuracy: 77.05%\n",
      "Epoch [11/50], Train Loss: 0.4917, Train Accuracy: 76.53%, Val Loss: 0.4952, Val Accuracy: 76.25%\n",
      "Epoch [12/50], Train Loss: 0.4869, Train Accuracy: 77.22%, Val Loss: 0.4897, Val Accuracy: 77.34%\n",
      "Epoch [13/50], Train Loss: 0.4815, Train Accuracy: 77.26%, Val Loss: 0.4810, Val Accuracy: 77.48%\n",
      "Epoch [14/50], Train Loss: 0.4857, Train Accuracy: 77.13%, Val Loss: 0.4847, Val Accuracy: 77.50%\n",
      "Epoch [15/50], Train Loss: 0.4891, Train Accuracy: 77.28%, Val Loss: 0.4822, Val Accuracy: 77.69%\n",
      "Epoch [16/50], Train Loss: 0.4856, Train Accuracy: 77.54%, Val Loss: 0.4922, Val Accuracy: 76.77%\n",
      "Epoch [17/50], Train Loss: 0.4976, Train Accuracy: 76.49%, Val Loss: 0.4917, Val Accuracy: 76.80%\n",
      "Epoch [18/50], Train Loss: 0.4981, Train Accuracy: 76.60%, Val Loss: 0.5031, Val Accuracy: 75.80%\n",
      "Early stopping triggered\n"
     ]
    }
   ],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 256)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.fc4 = nn.Linear(64, 1)\n",
    "        \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc4(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "    \n",
    "# Define the model, loss function, optimizer, and scheduler\n",
    "input_dim = dataset.train_window_data.shape[1] * dataset.train_window_data.shape[2] * 2  # Flattened size * 2 for concatenated pairs\n",
    "model = MLP(input_dim=input_dim)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=10)\n",
    "\n",
    "# Training the model with early stopping\n",
    "num_epochs = 50\n",
    "patience = 5\n",
    "best_val_loss = float('inf')\n",
    "early_stopping_counter = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "    \n",
    "    for data, labels, _ in train_loader:\n",
    "        data = data.view(data.size(0), -1).float()  # Flatten the input data\n",
    "        labels = labels.float()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data)\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        predicted = (outputs > 0.5).float()\n",
    "        total_train += labels.size(0)\n",
    "        correct_train += (predicted == labels).sum().item()\n",
    "    \n",
    "    scheduler.step()\n",
    "    train_accuracy = 100 * correct_train / total_train\n",
    "    train_loss = running_loss / len(train_loader)\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    running_val_loss = 0.0\n",
    "    correct_val = 0\n",
    "    total_val = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, labels, _ in val_loader:\n",
    "            data = data.view(data.size(0), -1).float()  # Flatten the input data\n",
    "            labels = labels.float()\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_val_loss += loss.item()\n",
    "            predicted = (outputs > 0.5).float()\n",
    "            total_val += labels.size(0)\n",
    "            correct_val += (predicted == labels).sum().item()\n",
    "    \n",
    "    val_loss = running_val_loss / len(val_loader)\n",
    "    val_accuracy = 100 * correct_val / total_val\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], '\n",
    "          f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%, '\n",
    "          f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%')\n",
    "\n",
    "    # Early stopping\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        early_stopping_counter = 0\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "    else:\n",
    "        early_stopping_counter += 1\n",
    "        if early_stopping_counter >= patience:\n",
    "            print(\"Early stopping triggered\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6127632d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct predictions to have no co-occurance:\n",
      "['rayleigh_scattering'] ['abelian_group'] 0.002\n",
      "['spatial_dependence'] ['entanglement_transformation'] -0.005\n",
      "['nitrogen_vacancy_center'] ['information_causality'] -0.01\n",
      "['probabilistic_finite_automaton'] ['impurity_spin'] 0.0\n",
      "['quantum_interactive_proof'] ['analog_model'] -0.014\n",
      "['message_passing'] ['electron_vortex'] -0.003\n",
      "\n",
      " Correct predictions to have co-occurance:\n",
      "['cold_atom_experiment'] ['ultra_high_vacuum'] 0.006\n",
      "['perfect_tensor'] ['tensor_decomposition'] 0.016\n",
      "['general_solution'] ['variational_problem'] 0.003\n",
      "['scale_invariant'] ['metal_insulator_transition'] 0.009\n",
      "['natural_language'] ['effective_field_theory'] 0.009\n",
      "['sign_problem'] ['convex_cone'] -0.002\n",
      "\n",
      "Validation Accuracy: 75.43%\n"
     ]
    }
   ],
   "source": [
    "# Load the best model\n",
    "model.load_state_dict(torch.load('best_model.pth'))\n",
    "\n",
    "# Final evaluation on the validation set\n",
    "model.eval()\n",
    "correct_val = 0\n",
    "total_val = 0\n",
    "\n",
    "indices = []\n",
    "outputs_list = []\n",
    "correct_indices = []\n",
    "labels_list = []\n",
    "\n",
    "x = np.arange(31).reshape(-1, 1)\n",
    "lin_model = LinearRegression()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data, labels, inx in testing_dataloader:\n",
    "        data = data.view(data.size(0), -1).float()  # Flatten the input data\n",
    "        labels = labels.float()\n",
    "        outputs = model(data)\n",
    "        predicted = (outputs > 0.5).float()\n",
    "        total_val += labels.size(0)\n",
    "        correct_val += (predicted == labels).sum().item()\n",
    "        \n",
    "        # Collect indices, outputs, labels, and correct predictions\n",
    "        indices.extend(inx.cpu().numpy())\n",
    "        outputs_list.extend(outputs.cpu().numpy())\n",
    "        labels_list.extend(labels.cpu().numpy())\n",
    "        correct_indices.extend((predicted == labels).cpu().numpy())\n",
    "        \n",
    "\n",
    "# Convert lists to numpy arrays for sorting\n",
    "indices = np.array(indices)\n",
    "outputs_list = np.array(outputs_list).flatten()\n",
    "labels_list = np.array(labels_list).flatten()\n",
    "correct_indices = np.array(correct_indices).flatten()\n",
    "\n",
    "# Get sorted indices of the outputs\n",
    "sorted_indices = np.argsort(outputs_list)\n",
    "\n",
    "# Separate the indices of correct predictions into two categories\n",
    "correct_0 = []\n",
    "correct_1 = []\n",
    "\n",
    "for i in sorted_indices:\n",
    "    if correct_indices[i]:\n",
    "        if labels_list[i] == 0:\n",
    "            correct_0.append(indices[i])\n",
    "        else:\n",
    "            correct_1.append(indices[i])\n",
    "\n",
    "# Print indices of correct predictions\n",
    "print(\"Correct predictions to have no co-occurance:\")\n",
    "for cnt,idx in enumerate(correct_0):\n",
    "    sim = similarity_cosine(encoding_dat[idx[0]][0],encoding_dat[idx[1]][0])\n",
    "    lin_model.fit(x, sim.reshape(-1, 1))\n",
    "    slope = lin_model.coef_[0][0]\n",
    "    print(c_inx_arr[idx[0]],c_inx_arr[idx[1]], np.round(slope,3))\n",
    "    if cnt ==5:\n",
    "        break\n",
    "\n",
    "print(\"\\n Correct predictions to have co-occurance:\")\n",
    "for cnt,idx in enumerate(correct_1):\n",
    "    sim = similarity_cosine(encoding_dat[idx[0]][0],encoding_dat[idx[1]][0])\n",
    "    lin_model.fit(x, sim.reshape(-1, 1))\n",
    "    slope = lin_model.coef_[0][0]\n",
    "    print(c_inx_arr[idx[0]],c_inx_arr[idx[1]], np.round(slope,3))\n",
    "    if cnt ==5:\n",
    "        break\n",
    "\n",
    "print(f\"\\nValidation Accuracy: {100 * correct_val / total_val:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "59e527d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Predictions to have no co-occurance:\n",
      "['promise_problem'] ['photon_radiation'] -0.011\n",
      "['jones_polynomial'] ['superradiant_emission'] -0.003\n",
      "['quantum_adiabatic_algorithm'] ['excited_state_lifetime'] -0.002\n",
      "['quantum_adiabatic_algorithm'] ['rydberg_excitons'] -0.001\n",
      "['spin_orientation'] ['quantum_secret_sharing_scheme'] -0.001\n",
      "['quantum_pcp_conjecture'] ['reflected_signal'] 0.004\n",
      "\n",
      " Predictions to have co-occurance:\n",
      "['long_range_anisotropic_interaction'] ['deep_strong_coupling'] -0.0\n",
      "['time_optimal_control'] ['harmonic_oscillator_mode'] 0.009\n",
      "['transfer_protocol'] ['variational_quantum_state'] -0.001\n",
      "['zitterbewegung_effect'] ['circular_ring'] -0.001\n",
      "['electric_field_fluctuation'] ['relaxation_oscillation'] -0.007\n",
      "['particle_number_fluctuation'] ['spinless_particle'] 0.008\n"
     ]
    }
   ],
   "source": [
    "# Load the best model\n",
    "model.load_state_dict(torch.load('best_model.pth'))\n",
    "\n",
    "# Final evaluation on the validation set\n",
    "model.eval()\n",
    "correct_val = 0\n",
    "total_val = 0\n",
    "\n",
    "indices = []\n",
    "outputs_list = []\n",
    "predicted_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data, inx in novel_dataloader:\n",
    "        data = data.view(data.size(0), -1).float()  # Flatten the input data\n",
    "        \n",
    "        outputs = model(data)\n",
    "        predicted = (outputs > 0.5).float()\n",
    "        \n",
    "        # Collect indices, outputs, labels, and correct predictions\n",
    "        indices.extend(inx.cpu().numpy())\n",
    "        outputs_list.extend(outputs.cpu().numpy())\n",
    "        predicted_list.extend(predicted.cpu().numpy())\n",
    "        \n",
    "# Convert lists to numpy arrays for sorting\n",
    "indices = np.array(indices)\n",
    "outputs_list = np.array(outputs_list).flatten()\n",
    "predicted_list = np.array(predicted_list).flatten()\n",
    "\n",
    "\n",
    "# Get sorted indices of the outputs\n",
    "sorted_indices = np.argsort(outputs_list)\n",
    "\n",
    "# Separate the indices of correct predictions into two categories\n",
    "correct_0 = []\n",
    "correct_1 = []\n",
    "\n",
    "for i in sorted_indices:\n",
    "    if predicted_list[i]:\n",
    "        correct_1.append(indices[i])\n",
    "    else:\n",
    "        correct_0.append(indices[i])\n",
    "\n",
    "# Print indices of correct predictions\n",
    "print(\" Predictions to have no co-occurance:\")\n",
    "for cnt,idx in enumerate(correct_0):\n",
    "    sim = similarity_cosine(encoding_dat[idx[0]][0],encoding_dat[idx[1]][0])\n",
    "    lin_model.fit(x, sim.reshape(-1, 1))\n",
    "    slope = lin_model.coef_[0][0]\n",
    "    print(c_inx_arr[idx[0]],c_inx_arr[idx[1]], np.round(slope,3))\n",
    "    if cnt ==5:\n",
    "        break\n",
    "\n",
    "print(\"\\n Predictions to have co-occurance:\")\n",
    "for cnt,idx in enumerate(correct_1):\n",
    "    sim = similarity_cosine(encoding_dat[idx[0]][0],encoding_dat[idx[1]][0])\n",
    "    lin_model.fit(x, sim.reshape(-1, 1))\n",
    "    slope = lin_model.coef_[0][0]\n",
    "    print(c_inx_arr[idx[0]],c_inx_arr[idx[1]],np.round(slope,3))\n",
    "    if cnt ==5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a5a5b1d3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stop' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mstop\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'stop' is not defined"
     ]
    }
   ],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6709576",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from sklearn.linear_model import LinearRegression\n",
    "# from tqdm import tqdm\n",
    "# from concurrent.futures import ProcessPoolExecutor\n",
    "\n",
    "# def similarity_cosine(vec1, vec2):\n",
    "#     # Compute cosine similarity in a vectorized manner\n",
    "#     dot_product = np.sum(vec1 * vec2, axis=1)\n",
    "#     norm1 = np.linalg.norm(vec1, axis=1)\n",
    "#     norm2 = np.linalg.norm(vec2, axis=1)\n",
    "#     cosine_similarity = dot_product / (norm1 * norm2)\n",
    "#     return cosine_similarity\n",
    "\n",
    "# def compute_similarity_and_slope(args):\n",
    "#     cnt_0, cnt_1, label_window_data, threshold, x = args\n",
    "#     label_subset_0 = label_window_data[cnt_0]\n",
    "#     label_subset_1 = label_window_data[cnt_1]\n",
    "\n",
    "#     sim = similarity_cosine(label_subset_0, label_subset_1)\n",
    "\n",
    "#     model.fit(x, sim.reshape(-1, 1))\n",
    "#     slope = model.coef_[0][0]\n",
    "\n",
    "#     if slope < -threshold:\n",
    "#         return [cnt_0, cnt_1], 0\n",
    "#     elif slope > threshold:\n",
    "#         return [cnt_0, cnt_1], 1\n",
    "#     return None, None\n",
    "\n",
    "# # label_window_data = np.random.rand(10000, 3, 128)  # Replace this with your actual data\n",
    "# x = np.arange(3).reshape(-1, 1)\n",
    "# model = LinearRegression()\n",
    "# threshold = 0.1\n",
    "\n",
    "# # Use ProcessPoolExecutor for parallel processing\n",
    "# executor = ProcessPoolExecutor(max_workers=4)\n",
    "\n",
    "# tasks = [(cnt_0, cnt_1, label_window_data, threshold, x) for cnt_0 in range(label_window_data.shape[0])\n",
    "#          for cnt_1 in range(cnt_0 + 1, label_window_data.shape[0])]\n",
    "\n",
    "# results = list(tqdm(executor.map(compute_similarity_and_slope, tasks), total=len(tasks)))\n",
    "\n",
    "# idx_pair_0_arr = [result[0] for result in results if result[1] == 0]\n",
    "# idx_pair_1_arr = [result[0] for result in results if result[1] == 1]\n",
    "\n",
    "# print(\"idx_pair_0_arr:\", idx_pair_0_arr)\n",
    "# print(\"idx_pair_1_arr:\", idx_pair_1_arr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cdfc10a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from sklearn.linear_model import LinearRegression\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# def similarity_cosine(vec1, vec2):\n",
    "#     # Compute cosine similarity in a vectorized manner\n",
    "#     dot_product = np.sum(vec1 * vec2, axis=1)\n",
    "#     norm1 = np.linalg.norm(vec1, axis=1)\n",
    "#     norm2 = np.linalg.norm(vec2, axis=1)\n",
    "#     cosine_similarity = dot_product / (norm1 * norm2)\n",
    "#     return cosine_similarity\n",
    "\n",
    "\n",
    "# x = np.arange(3).reshape(-1, 1)\n",
    "# model = LinearRegression()\n",
    "# threshold = 0.1\n",
    "\n",
    "# idx_pair_0_arr = []\n",
    "# idx_pair_1_arr = []\n",
    "\n",
    "# # Iterate over pairs in an optimized manner\n",
    "# for cnt_0 in tqdm(range(label_window_data.shape[0])):\n",
    "#     for cnt_1 in range(cnt_0 + 1, label_window_data.shape[0]):\n",
    "#         label_subset_0 = label_window_data[cnt_0]\n",
    "#         label_subset_1 = label_window_data[cnt_1]\n",
    "\n",
    "#         sim = similarity_cosine(label_subset_0, label_subset_1)\n",
    "\n",
    "#         model.fit(x, sim.reshape(-1, 1))\n",
    "#         slope = model.coef_[0][0]\n",
    "\n",
    "#         if slope < -threshold:\n",
    "#             idx_pair_0_arr.append([cnt_0, cnt_1])\n",
    "#         elif slope > threshold:\n",
    "#             idx_pair_1_arr.append([cnt_0, cnt_1])\n",
    "\n",
    "# idx_pair_0_arr = np.array(idx_pair_0_arr)\n",
    "# idx_pair_1_arr = np.array(idx_pair_1_arr)\n",
    "\n",
    "# print(\"idx_pair_0_arr:\", idx_pair_0_arr)\n",
    "# print(\"idx_pair_1_arr:\", idx_pair_1_arr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5d041e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_window_size = 5  # Number of input time steps (e.g., 0-25)\n",
    "# output_window_size = 3  # Number of time steps to predict (e.g., 25-28)\n",
    "# offset_to_current_year = 1 \n",
    "\n",
    "# # Assuming your data is in a NumPy array of shape (10000, 31, 128)\n",
    "# data = np.load(\"c_encoding_arr.npy\")\n",
    "# c_inx_arr = np.load(\"c_inx_arr.npy\")\n",
    "\n",
    "# # Ensure the total number of time steps is enough to create the windows\n",
    "# total_time_steps = data.shape[1]\n",
    "\n",
    "# train_window_data = data[:,-input_window_size-output_window_size-offset_to_current_year:-output_window_size-offset_to_current_year]\n",
    "# label_window_data = data[:,-output_window_size-offset_to_current_year:-offset_to_current_year]\n",
    "\n",
    "\n",
    "\n",
    "# # Label 0 => Similarity Decreasing\n",
    "# # Label 1 => Similarity Increasing \n",
    "# # Linear Fit as threshhold \n",
    "\n",
    "\n",
    "# from sklearn.linear_model import LinearRegression\n",
    "# def similarity_cosine(vec1, vec2):\n",
    "#     cosine_similarity_arr = []\n",
    "#     for v1,v2 in zip(vec1, vec2):\n",
    "#         cosine_similarity = np.dot(v1, v2)/(np.linalg.norm(v1)* np.linalg.norm(v2))\n",
    "#         cosine_similarity_arr.append(cosine_similarity)\n",
    "#     return np.array(cosine_similarity_arr)\n",
    "\n",
    "# x = np.arange(3).reshape(-1, 1)\n",
    "# model = LinearRegression()\n",
    "\n",
    "# idx_pair_0_arr = []\n",
    "# idx_pair_1_arr = []\n",
    "# threshhold = 0.1\n",
    "# for cnt_0, label_subset_0 in tqdm(enumerate(label_window_data)):\n",
    "#     for cnt_1,label_subset_1 in enumerate(label_window_data[cnt_0+1:]):\n",
    "#         sim = similarity_cosine(label_subset_0, label_subset_1)\n",
    "\n",
    "#         model.fit(x, sim.reshape(-1, 1))\n",
    "#         slope = model.coef_[0][0]\n",
    "\n",
    "#         if slope < -threshhold:\n",
    "#             idx_pair_0_arr.append([cnt_0,cnt_1])\n",
    "#         elif slope > threshhold:\n",
    "#             idx_pair_1_arr.append([cnt_0,cnt_1])\n",
    "\n",
    "        \n",
    "#     if cnt_0==50:\n",
    "#         break\n",
    "# idx_pair_0_arr = np.array(idx_pair_0_arr)\n",
    "# idx_pair_1_arr = np.array(idx_pair_1_arr)\n",
    "\n",
    "# idx_pair_0_arr.shape, idx_pair_1_arr.shape\n",
    "\n",
    "# # # Normalize the data\n",
    "# # scaler = RobustScaler()\n",
    "\n",
    "# # # Reshape data to 2D for normalization\n",
    "# # reshaped_data = data.reshape(-1, data.shape[-1])  # Shape: (10000*31, 128)\n",
    "# # normalized_data = scaler.fit_transform(reshaped_data)\n",
    "\n",
    "# # # Reshape back to original shape\n",
    "# # normalized_data = normalized_data.reshape(data.shape)\n",
    "\n",
    "# # # normalized_data = torch.tensor(normalized_data, dtype=torch.float32)\n",
    "\n",
    "# # train_data = normalized_data[:, :28, :]\n",
    "# # train_labels = normalized_data[:, 28:, :]\n",
    "\n",
    "# # Prepare training inputs and outputs\n",
    "# # X_train = normalized_data[:, :input_window_size, :]  # Shape: (10000, input_window_size, 128)\n",
    "# # Y_train = normalized_data[:, input_window_size:input_window_size + output_window_size, :]  # Shape: (, output_window_size, 128)\n",
    "\n",
    "# # # Prepare validation inputs and outputs\n",
    "# # X_val = normalized_data[:, 1:1 + input_window_size, :]  # Shape: (10000, input_window_size, 128)\n",
    "# # Y_val = normalized_data[:, 1 + input_window_size:1 + input_window_size + output_window_size, :]  # Shape: (, output_window_size, 128)\n",
    "\n",
    "# # # Prepare tnormalized_dataest inputs and outputs\n",
    "# # X_test = normalized_data[:, 2:2 + input_window_size, :]  # Shape: (10000, input_window_size, 128)\n",
    "# # Y_test = normalized_data[:, 2 + input_window_size:2 + input_window_size + output_window_size, :]  # Shape: (, output_window_size, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "433ae2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Sample data for demonstration\n",
    "# np.random.seed(42)\n",
    "# data = normalized_data[:100]  # Replace this with your actual data\n",
    "\n",
    "# # Reshape data for easier manipulation\n",
    "# samples, timesteps, features = data.shape\n",
    "# data_reshaped = data[0]#.reshape(-1, features)\n",
    "\n",
    "# # Convert to DataFrame for easier analysis\n",
    "# df = pd.DataFrame(data_reshaped, columns=[f'feature_{i}' for i in range(features)])\n",
    "\n",
    "# # Function to plot time series\n",
    "# def plot_time_series(df, title=''):\n",
    "#     plt.figure(figsize=(15, 6))\n",
    "#     for column in df.columns:\n",
    "#         plt.plot(df[column], label=column)\n",
    "#     plt.title(title)\n",
    "#     plt.xlabel('Time Steps')\n",
    "#     plt.ylabel('Value')\n",
    "#     # plt.legend(loc='upper right')\n",
    "#     plt.show()\n",
    "\n",
    "# # Function to plot rolling statistics\n",
    "# def plot_rolling_statistics(df, window=5):\n",
    "#     rolling_mean = df.rolling(window=window).mean()\n",
    "#     rolling_std = df.rolling(window=window).std()\n",
    "    \n",
    "#     plt.figure(figsize=(15, 6))\n",
    "#     plt.plot(df, label='Original')\n",
    "#     plt.plot(rolling_mean, label='Rolling Mean')\n",
    "#     plt.plot(rolling_std, label='Rolling Std')\n",
    "#     plt.legend(loc='upper right')\n",
    "#     plt.show()\n",
    "\n",
    "# # Function to perform ADF test\n",
    "# def adf_test(series):\n",
    "#     result = adfuller(series)\n",
    "#     print(f'ADF Statistic: {result[0]}')\n",
    "#     print(f'p-value: {result[1]}')\n",
    "#     for key, value in result[4].items():\n",
    "#         print('Critial Values:')\n",
    "#         print(f'   {key}, {value}')\n",
    "\n",
    "# # Function to plot ACF and PACF\n",
    "# def plot_acf_pacf(series, lags=14):\n",
    "#     plt.figure(figsize=(15, 6))\n",
    "#     plt.subplot(121)\n",
    "#     plt.plot(acf(series, nlags=lags))\n",
    "#     plt.title('Autocorrelation Function')\n",
    "#     plt.subplot(122)\n",
    "#     plt.plot(pacf(series, nlags=lags))\n",
    "#     plt.title('Partial Autocorrelation Function')\n",
    "#     plt.show()\n",
    "\n",
    "# # Function to perform seasonal decomposition\n",
    "# def decompose_series(series, freq=7):\n",
    "#     decomposition = seasonal_decompose(series, period=freq)\n",
    "#     decomposition.plot()\n",
    "#     plt.show()\n",
    "\n",
    "# # Perform correlation analysis\n",
    "# correlation_matrix = df.corr()\n",
    "# sns.heatmap(correlation_matrix, cmap='coolwarm')\n",
    "# plt.title('Correlation Matrix')\n",
    "# plt.show()\n",
    "\n",
    "# # Visualize data\n",
    "# plot_time_series(df.head(1000))  # Plot the first 1000 samples\n",
    "\n",
    "# # ADF test for stationarity (example on the first feature)\n",
    "# adf_test(df['feature_0'])\n",
    "\n",
    "# # Plot rolling statistics (example on the first feature)\n",
    "# plot_rolling_statistics(df['feature_0'])\n",
    "\n",
    "\n",
    "\n",
    "# # ACF and PACF plots (example on the first feature)\n",
    "# plot_acf_pacf(df['feature_0'])\n",
    "\n",
    "# # PCA for dimensionality reduction\n",
    "# scaler = StandardScaler()\n",
    "# data_scaled = scaler.fit_transform(data_reshaped)\n",
    "# pca = PCA(n_components=2)\n",
    "# pca_result = pca.fit_transform(data_scaled)\n",
    "\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.scatter(pca_result[:, 0], pca_result[:, 1], alpha=0.2)\n",
    "# plt.title('PCA Results' + str(pca.explained_variance_))\n",
    "# plt.xlabel('Principal Component 1')\n",
    "# plt.ylabel('Principal Component 2')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "560bab8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_pred(forecast_arr,max_inx=10000):\n",
    "#     fig, axs = plt.subplots(3)\n",
    "#     for i in range(3):\n",
    "#         inx_c = np.random.randint(max_inx)\n",
    "#         inx_d = np.random.randint(128)\n",
    "\n",
    "#         axs[i].plot(train_data[inx_c,:,inx_d])\n",
    "#         axs[i].plot(np.arange(28,31),forecast_arr[inx_c,:,inx_d],\".\",label=\"pred\")\n",
    "#         axs[i].plot(np.arange(28,31),train_labels[inx_c,:,inx_d],label=\"real\")\n",
    "#     plt.legend()\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0ed26c",
   "metadata": {},
   "source": [
    "Baseline 1: Naive random walk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a3232772",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Simple benchmark: Repeat the last value of the 28th time step for the next 3 steps\n",
    "# # Extract the last time step from the input sequence\n",
    "# last_time_step = train_data[:, -1, :] \n",
    "\n",
    "# # Repeat the last time step for the next 3 steps\n",
    "# y_pred = np.tile(last_time_step[:, np.newaxis, :], (1, train_labels.shape[1], 1))  \n",
    "\n",
    "# # Calculate performance metrics, e.g., Mean Squared Error (MSE)\n",
    "# mse = mean_squared_error(train_labels.reshape(-1, train_labels.shape[1] * 128), y_pred.reshape(-1, train_labels.shape[1] * 128))\n",
    "\n",
    "# print(f'Mean Squared Error (MSE) of the benchmark model: {np.round(mse,3)}')  \n",
    "\n",
    "# plot_pred(y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4698f7fc",
   "metadata": {},
   "source": [
    "Baseline 2: Two-Step naive random walk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "800c62be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Extract the last two time steps\n",
    "# second_last_time_step = train_data[:, -5, :]  \n",
    "# last_time_step = train_data[:, -1, :]         \n",
    "\n",
    "# # Calculate the trend (difference between the last two time steps)\n",
    "# trend = last_time_step - second_last_time_step  \n",
    "\n",
    "# # Use the last value and add the trend for each future time step\n",
    "# y_pred_informed_random_walk = np.zeros((train_labels.shape[0], train_labels.shape[1], 128))\n",
    "\n",
    "# for i in range(train_labels.shape[1]):\n",
    "#     y_pred_informed_random_walk[:, i, :] = last_time_step + (i + 1) * trend\n",
    "\n",
    "# mse = mean_squared_error(train_labels.reshape(-1, train_labels.shape[1] * 128), y_pred_informed_random_walk.reshape(-1, train_labels.shape[1] * 128))\n",
    "\n",
    "# print(f'Mean Squared Error (MSE) of the benchmark model: {np.round(mse,3)}')\n",
    "# plot_pred(y_pred_informed_random_walk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223351c1",
   "metadata": {},
   "source": [
    "Baseline 3: Continue with historical mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a87d9754",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean_time_step = np.mean(train_data[:,-5:,:], axis=1)  # Shape: (10000, 128)\n",
    "# y_pred_mean = np.tile(mean_time_step[:, np.newaxis, :], (1, train_labels.shape[1], 1))  # Shape: (10000, 5, 128)\n",
    "\n",
    "# # Calculate performance metrics, e.g., Mean Squared Error (MSE)\n",
    "# mse = mean_squared_error(train_labels.reshape(-1, train_labels.shape[1] * 128), y_pred_mean.reshape(-1, train_labels.shape[1] * 128))\n",
    "\n",
    "# print(f'Mean Squared Error (MSE) of the benchmark model: {np.round(mse,3)}')\n",
    "# plot_pred(y_pred_mean)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae92799a",
   "metadata": {},
   "source": [
    "4. Simple Moving average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3d9b110f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def moving_average_predict(X, window_size=3):\n",
    "#     # Calculate the moving average of the last 'window_size' time steps\n",
    "#     moving_avg = np.mean(X[:, -window_size:, :], axis=1)  # Shape: (10000, 128)\n",
    "    \n",
    "#     # Repeat the moving average for the next 5 time steps\n",
    "#     y_pred = np.tile(moving_avg[:, np.newaxis, :], (1, train_labels.shape[1], 1))  # Shape: (10000, 5, 128)\n",
    "    \n",
    "#     return y_pred\n",
    "\n",
    "# # Benchmark: Moving Average\n",
    "\n",
    "# for window_size in range(0,10):\n",
    "#     y_pred_moving_avg = moving_average_predict(train_data, window_size)\n",
    "#     mse = mean_squared_error(train_labels.reshape(-1, train_labels.shape[1] * 128), y_pred_moving_avg.reshape(-1, train_labels.shape[1] * 128))\n",
    "\n",
    "#     print(f'Mean Squared Error (MSE) of the benchmark model: {np.round(mse,3)} with window size: {window_size}')\n",
    "\n",
    "# plot_pred(y_pred_moving_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7e944500",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def moving_average_diff_predict(X, window_size=3, predict_steps=3):\n",
    "#     # Calculate the difference between consecutive time steps\n",
    "    \n",
    "#     # Calculate the mean of the differences over the window size\n",
    "#     moving_avg_diff = np.mean(X[:, -window_size:, :], axis=1)  # Shape: (10000, 128)\n",
    "    \n",
    "#     # Initialize predictions array\n",
    "#     y_pred = np.zeros((X.shape[0], predict_steps, X.shape[2]))  # Shape: (10000, 5, 128)\n",
    "    \n",
    "#     # Get the last observed time step\n",
    "#     last_time_step = X[:, -1, :]  # Shape: (10000, 128)\n",
    "    \n",
    "#     # Predict the future steps based on the mean difference\n",
    "#     for i in range(predict_steps):\n",
    "#         y_pred[:, i, :] = last_time_step + (i + 1) * moving_avg_diff\n",
    "    \n",
    "#     return y_pred\n",
    "\n",
    "\n",
    "# for window_size in range(0,10):\n",
    "#     y_pred_moving_avg = moving_average_diff_predict(train_data, window_size)\n",
    "#     mse = mean_squared_error(train_labels.reshape(-1, train_labels.shape[1] * 128), y_pred_moving_avg.reshape(-1, train_labels.shape[1] * 128))\n",
    "\n",
    "#     print(f'Mean Squared Error (MSE) of the benchmark model: {np.round(mse,4)} with window size: {window_size}')\n",
    "\n",
    "# plot_pred(y_pred_moving_avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fcd2ea1",
   "metadata": {},
   "source": [
    "5. Difference Informed Moving Average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0b67d0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def moving_average_diff_predict(X, window_size=3, predict_steps=3):\n",
    "#     # Calculate the difference between consecutive time steps\n",
    "#     grad = np.diff(X, axis=1)  # Shape: (10000, 25, 128)\n",
    "    \n",
    "#     # Calculate the mean of the differences over the window size\n",
    "#     moving_avg_diff = np.mean(grad[:, -window_size:, :], axis=1)  # Shape: (10000, 128)\n",
    "    \n",
    "#     # Initialize predictions array\n",
    "#     y_pred = np.zeros((X.shape[0], predict_steps, X.shape[2]))  # Shape: (10000, 5, 128)\n",
    "    \n",
    "#     # Get the last observed time step\n",
    "#     last_time_step = X[:, -1, :]  # Shape: (10000, 128)\n",
    "    \n",
    "#     # Predict the future steps based on the mean difference\n",
    "#     for i in range(predict_steps):\n",
    "#         y_pred[:, i, :] = last_time_step + (i + 1) * moving_avg_diff\n",
    "    \n",
    "#     return y_pred\n",
    "\n",
    "\n",
    "# for window_size in range(0,10):\n",
    "#     y_pred_moving_avg = moving_average_diff_predict(train_data, window_size)\n",
    "#     mse = mean_squared_error(train_labels.reshape(-1, train_labels.shape[1] * 128), y_pred_moving_avg.reshape(-1, train_labels.shape[1] * 128))\n",
    "\n",
    "#     print(f'Mean Squared Error (MSE) of the benchmark model: {np.round(mse,4)} with window size: {window_size}')\n",
    "# plot_pred(y_pred_moving_avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e014ed37",
   "metadata": {},
   "source": [
    "Statistical Models Univariate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "29f662aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from statsmodels.tsa.ar_model import AutoReg\n",
    "# from statsmodels.tsa.arima.model import ARIMA\n",
    "# from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "# from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "# from statsmodels.tsa.holtwinters import Holt\n",
    "# from statsmodels.tsa.holtwinters import SimpleExpSmoothing\n",
    "# from statsmodels.tsa.forecasting.theta import ThetaModel\n",
    "# from statsmodels.tsa.seasonal import STL\n",
    "# from prophet import Prophet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "709c8d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # AR Model\n",
    "# def ar_model(data, steps=3):\n",
    "#     model = AutoReg(data, lags=3).fit()\n",
    "#     forecast = model.predict(start=len(data), end=len(data) + steps - 1)\n",
    "#     # plot_forecast(data, forecast, 'AR Model')\n",
    "#     return forecast\n",
    "\n",
    "# forecast_arr = np.zeros_like(train_labels)\n",
    "# for c1, conc_enc in tqdm(enumerate(train_data)):\n",
    "#     for inx in range(128):\n",
    "#         fcast = ar_model(conc_enc[:,inx])\n",
    "#         forecast_arr[c1,:,inx] = fcast\n",
    "#     if c1==500:\n",
    "#         break\n",
    "\n",
    "# mse = mean_squared_error(train_labels[:c1].reshape(-1, train_labels.shape[1] * 128), forecast_arr[:c1].reshape(-1, train_labels.shape[1] * 128))\n",
    "\n",
    "# print(f'Mean Squared Error (MSE) of the AR model: {np.round(mse,4)}')\n",
    "\n",
    "# plot_pred(forecast_arr,max_inx=c1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831ea841",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from statsmodels.tsa.stattools import adfuller\n",
    "# import warnings \n",
    "# # Ignore warnings\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# # Function to perform grid search for ARIMA parameters\n",
    "# def grid_search_arima(data, p_values, d_values, q_values):\n",
    "#     best_score, best_cfg = float(\"inf\"), None\n",
    "#     for p in p_values:\n",
    "#         for d in d_values:\n",
    "#             for q in q_values:\n",
    "#                 order = (p, d, q)\n",
    "#                 try:\n",
    "#                     model = ARIMA(data, order=order)\n",
    "#                     model_fit = model.fit()\n",
    "#                     aic = model_fit.aic\n",
    "#                     if aic < best_score:\n",
    "#                         best_score, best_cfg = aic, order\n",
    "#                     # print(f'ARIMA{order} AIC={aic}')\n",
    "#                 except:\n",
    "#                     continue\n",
    "#     # print(f'Best ARIMA{best_cfg} AIC={best_score}')\n",
    "#     return best_cfg\n",
    "\n",
    "# # Define the p, d, q ranges to search\n",
    "# p_values = range(0, 4)\n",
    "# d_values = range(0, 3)\n",
    "# q_values = range(0, 4)\n",
    "\n",
    "# forecast_arr = np.zeros_like(train_labels)\n",
    "# for c1, conc_enc in tqdm(enumerate(train_data)):\n",
    "#     for inx in range(128):\n",
    "#         best_order = grid_search_arima(conc_enc[:,inx], p_values, d_values, q_values)\n",
    "#         print(best_order)\n",
    "#         fcast = arima_model(conc_enc[:,inx],best_order)\n",
    "#         forecast_arr[c1,:,inx] = fcast\n",
    "#         print(inx)\n",
    "#     if c1==1:\n",
    "#         break\n",
    "\n",
    "# mse = mean_squared_error(train_labels[:c1].reshape(-1, train_labels.shape[1] * 128), forecast_arr[:c1].reshape(-1, train_labels.shape[1] * 128))\n",
    "\n",
    "# print(f'Mean Squared Error (MSE) of the arima model: {np.round(mse,4)}')\n",
    "\n",
    "# plot_pred(forecast_arr,max_inx=c1)\n",
    "\n",
    "\n",
    "# best_score, best_cfg = float(\"inf\"), None\n",
    "# for p in tqdm(p_values):\n",
    "#     for d in d_values:\n",
    "#         for q in q_values:\n",
    "#             order = (p, d, q)\n",
    "#             forecast_arr = np.zeros_like(train_labels)\n",
    "#             for c1, conc_enc in enumerate(train_data):\n",
    "#                 for inx in range(128):\n",
    "#                     fcast = arima_model(conc_enc[:,inx],order)\n",
    "#                     forecast_arr[c1,:,inx] = fcast\n",
    "#                 if c1==5:\n",
    "#                     break\n",
    "\n",
    "#             mse = mean_squared_error(train_labels[:c1].reshape(-1, train_labels.shape[1] * 128), forecast_arr[:c1].reshape(-1, train_labels.shape[1] * 128))\n",
    "#             if mse < best_score:\n",
    "#                         best_score, best_cfg = mse, order\n",
    "# print(f'Best ARIMA{best_cfg} AIC={best_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5b9e2ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ARIMA Model\n",
    "# def arima_model(data, order, steps=3):\n",
    "#     model = ARIMA(data, order=order).fit()\n",
    "#     forecast = model.forecast(steps=steps)\n",
    "#     # plot_forecast(data, forecast, 'ARIMA Model')\n",
    "#     return forecast\n",
    "\n",
    "# forecast_arr = np.zeros_like(train_labels)\n",
    "# for c1, conc_enc in tqdm(enumerate(train_data)):\n",
    "#     for inx in range(128):\n",
    "#         fcast = arima_model(conc_enc[:,inx],(0,1,0))\n",
    "#         forecast_arr[c1,:,inx] = fcast\n",
    "#     if c1==500:\n",
    "#         break\n",
    "\n",
    "# mse = mean_squared_error(train_labels[:c1].reshape(-1, train_labels.shape[1] * 128), forecast_arr[:c1].reshape(-1, train_labels.shape[1] * 128))\n",
    "\n",
    "# print(f'Mean Squared Error (MSE) of the arima model: {np.round(mse,4)}')\n",
    "# plot_pred(forecast_arr,max_inx=c1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2cc5850d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Simple Exponential Smoothing\n",
    "# def ses_model(data, steps=3):\n",
    "#     model = SimpleExpSmoothing(data, initialization_method=\"heuristic\").fit(\n",
    "#             smoothing_level=0.75, optimized=False \n",
    "#         )\n",
    "#     forecast = model.forecast(steps=steps)\n",
    "#     # plot_forecast(data, forecast, 'Simple Exponential Smoothing')\n",
    "#     return forecast\n",
    "\n",
    "# forecast_arr = np.zeros_like(train_labels)\n",
    "# for c1, conc_enc in tqdm(enumerate(train_data)):\n",
    "#     for inx in range(128):\n",
    "#         fcast = ses_model(conc_enc[:,inx])\n",
    "#         forecast_arr[c1,:,inx] = fcast\n",
    "#     if c1==500:\n",
    "#         break\n",
    "\n",
    "# mse = mean_squared_error(train_labels[:c1].reshape(-1, train_labels.shape[1] * 128), forecast_arr[:c1].reshape(-1, train_labels.shape[1] * 128))\n",
    "\n",
    "# print(f'Mean Squared Error (MSE) of the benchmark model: {np.round(mse,4)}')\n",
    "\n",
    "# plot_pred(forecast_arr,max_inx=c1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3c3b2477",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Holt’s Linear Trend Model\n",
    "# def holt_model(data, steps=3):\n",
    "#     model = Holt(data,initialization_method=\"heuristic\").fit(\n",
    "#             smoothing_level=0.75, optimized=False, smoothing_trend=0.1\n",
    "#         )\n",
    "#     forecast = model.forecast(steps=steps)\n",
    "#     # plot_forecast(data, forecast, 'Holt’s Linear Trend Model')\n",
    "#     return forecast\n",
    "\n",
    "# forecast_arr = np.zeros_like(train_labels)\n",
    "# for c1, conc_enc in tqdm(enumerate(train_data)):\n",
    "#     for inx in range(128):\n",
    "#         fcast = holt_model(conc_enc[:,inx])\n",
    "#         forecast_arr[c1,:,inx] = fcast\n",
    "#     if c1==500:\n",
    "#         break\n",
    "\n",
    "# mse = mean_squared_error(train_labels[:c1].reshape(-1, train_labels.shape[1] * 128), forecast_arr[:c1].reshape(-1, train_labels.shape[1] * 128))\n",
    "\n",
    "# print(f'Mean Squared Error (MSE) of the benchmark model: {np.round(mse,4)}')\n",
    "\n",
    "# plot_pred(forecast_arr,max_inx=c1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c891cfe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Holt-Winters Seasonal Model\n",
    "# def holt_winters_model(data, steps=3):\n",
    "#     model = ExponentialSmoothing(data, trend='add', damped_trend=True).fit(\n",
    "#             smoothing_level=1, smoothing_trend=0.1,damping_trend=0.2,optimized=True\n",
    "#         )\n",
    "#     forecast = model.forecast(steps=steps)\n",
    "#     # plot_forecast(data, forecast, 'Holt-Winters Seasonal Model')\n",
    "#     return forecast\n",
    "\n",
    "# forecast_arr = np.zeros_like(train_labels)\n",
    "# for c1, conc_enc in tqdm(enumerate(train_data)):\n",
    "#     for inx in range(128):\n",
    "#         fcast = holt_winters_model(conc_enc[:,inx])\n",
    "#         forecast_arr[c1,:,inx] = fcast\n",
    "#     if c1==500:\n",
    "#         break\n",
    "\n",
    "# mse = mean_squared_error(train_labels[:c1].reshape(-1, train_labels.shape[1] * 128), forecast_arr[:c1].reshape(-1, train_labels.shape[1] * 128))\n",
    "\n",
    "# print(f'Mean Squared Error (MSE) of the benchmark model: {np.round(mse,4)}')\n",
    "\n",
    "# plot_pred(forecast_arr,max_inx=c1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "edcc5a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import logging\n",
    "# # Prophet Model\n",
    "# def prophet_model(data, steps=3):\n",
    "#     df = pd.DataFrame({'ds': pd.date_range(start='2023-01-01', periods=len(data), freq='D'), 'y': data})\n",
    "#     model = Prophet(daily_seasonality=False)\n",
    "#     model.fit(df)\n",
    "#     future = model.make_future_dataframe(periods=steps)\n",
    "#     forecast = model.predict(future)\n",
    "#     forecast_values = forecast['yhat'].iloc[-steps:].values\n",
    "#     # plot_forecast(data, forecast_values, 'Prophet Model')\n",
    "#     return forecast_values\n",
    "\n",
    "# logging.getLogger(\"cmdstanpy\").disabled = True #  turn 'cmdstanpy' logs off\n",
    "\n",
    "# forecast_arr = np.zeros_like(train_labels)\n",
    "# for c1, conc_enc in tqdm(enumerate(train_data)):\n",
    "#     for inx in range(128):\n",
    "#         fcast = prophet_model(conc_enc[:,inx])\n",
    "#         forecast_arr[c1,:,inx] = fcast\n",
    "#     if c1==10:\n",
    "#         break\n",
    "# logging.getLogger(\"cmdstanpy\").disabled = False #  revert original setting\n",
    "\n",
    "# mse = mean_squared_error(train_labels[:c1].reshape(-1, train_labels.shape[1] * 128), forecast_arr[:c1].reshape(-1, train_labels.shape[1] * 128))\n",
    "\n",
    "# print(f'Mean Squared Error (MSE) of the benchmark model: {np.round(mse,4)}')\n",
    "\n",
    "# plot_pred(forecast_arr,max_inx=c1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "17825487",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Theta Model\n",
    "# def theta_model(data, steps=3):\n",
    "#     model = ThetaModel(data,period=10).fit()\n",
    "#     forecast = model.forecast(steps=steps)\n",
    "#     # plot_forecast(data, forecast, 'Theta Model')\n",
    "#     return forecast\n",
    "\n",
    "# forecast_arr = np.zeros_like(train_labels)\n",
    "# for c1, conc_enc in tqdm(enumerate(train_data)):\n",
    "#     for inx in range(128):\n",
    "#         fcast = theta_model(conc_enc[:,inx])\n",
    "#         forecast_arr[c1,:,inx] = fcast\n",
    "#     if c1==50:\n",
    "#         break\n",
    "\n",
    "# mse = mean_squared_error(train_labels[:c1].reshape(-1, train_labels.shape[1] * 128), forecast_arr[:c1].reshape(-1, train_labels.shape[1] * 128))\n",
    "\n",
    "# print(f'Mean Squared Error (MSE) of the benchmark model: {np.round(mse,4)}')\n",
    "\n",
    "# plot_pred(forecast_arr,max_inx=c1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e15cf2a7",
   "metadata": {},
   "source": [
    "Statistical Models Multivariate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "10e2633e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def reject_outliers(data, m = 10.):\n",
    "#     d = np.abs(data - np.median(data))\n",
    "#     mdev = np.median(d)\n",
    "#     s = d/mdev if mdev else np.zeros(len(d))\n",
    "\n",
    "#     data[s>=m]=0\n",
    "#     return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ff70642a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from statsmodels.tsa.api import VAR\n",
    "\n",
    "# cnt = 0\n",
    "# forecast_arr = np.zeros_like(train_labels)\n",
    "# for c1, conc_enc in tqdm(enumerate(train_data)):\n",
    "#   try:\n",
    "#     model = VAR(conc_enc)\n",
    "#     model_fit = model.fit(trend=\"n\")\n",
    "#     forecast_arr[c1,:] = model_fit.forecast(y=conc_enc,steps=3)\n",
    "#   except:\n",
    "#     forecast_arr[c1,:] = np.zeros(128)\n",
    "#     cnt += 1 \n",
    "\n",
    "# mse = mean_squared_error(train_labels.reshape(-1, train_labels.shape[1] * 128).flatten(), reject_outliers(forecast_arr.reshape(-1, train_labels.shape[1] * 128).flatten()))\n",
    "\n",
    "# print(f'Mean Squared Error (MSE) of the benchmark model: {np.round(mse,4)}')\n",
    "# plot_pred(forecast_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "de558caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from statsmodels.tsa.statespace.dynamic_factor import DynamicFactor\n",
    "\n",
    "# def dfm_model(df, steps=3):\n",
    "#     model = DynamicFactor(df, k_factors=1, factor_order=2,enforce_stationarity=False)\n",
    "#     results = model.fit(disp=False)\n",
    "#     forecast = results.forecast(steps=steps)\n",
    "#     return forecast\n",
    "\n",
    "# forecast_arr = np.zeros_like(train_labels)\n",
    "# for c1, conc_enc in tqdm(enumerate(train_data)):\n",
    "# #   try:\n",
    "    \n",
    "#     forecast_arr[c1,:] = dfm_model(conc_enc)\n",
    "# #   except:\n",
    "# #     forecast_arr[c1,:] = np.zeros(128)\n",
    "# #     cnt += 1 \n",
    "#     if c1==5:\n",
    "#         break\n",
    "\n",
    "# mse = mean_squared_error(train_labels[:c1].reshape(-1, train_labels.shape[1] * 128), forecast_arr[:c1].reshape(-1, train_labels.shape[1] * 128))\n",
    "\n",
    "# print(f'Mean Squared Error (MSE) of the benchmark model: {np.round(mse,4)}')\n",
    "# plot_pred(forecast_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543707e3",
   "metadata": {},
   "source": [
    "ML Models Univariate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bbcd787",
   "metadata": {},
   "source": [
    " Linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f30eb343",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.linear_model import LinearRegression\n",
    "# from sklearn.model_selection import KFold\n",
    "# from sklearn.metrics import mean_squared_error\n",
    "# kf = KFold(n_splits=5, shuffle=True)\n",
    "\n",
    "# for fold, (train_idx, val_idx) in enumerate(kf.split(train_data)):\n",
    "#     print(f\"FOLD {fold+1}\")\n",
    "#     print(\"--------------------------------\")\n",
    "    \n",
    "#     # Split data\n",
    "#     train_fold_data = torch.Tensor(train_data[train_idx].reshape(-1,train_data.shape[1]*train_data.shape[2]))\n",
    "#     train_fold_labels = torch.Tensor(train_labels[train_idx].reshape(-1,train_labels.shape[1]*train_labels.shape[2]))\n",
    "#     val_fold_data = torch.Tensor(train_data[val_idx].reshape(-1,train_data.shape[1]*train_data.shape[2]))\n",
    "#     val_fold_labels = torch.Tensor(train_labels[val_idx].reshape(-1,train_labels.shape[1]*train_labels.shape[2]))\n",
    "\n",
    "#     # Model\n",
    "#     lr = LinearRegression()\n",
    "#     lr.fit(train_fold_data, train_fold_labels)\n",
    "#     predictions_train = lr.predict(train_fold_data)\n",
    "#     predictions_val = lr.predict(val_fold_data)\n",
    "#     print(mean_squared_error(train_fold_labels,predictions_train))\n",
    "#     print(mean_squared_error(val_fold_labels,predictions_val))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09f88b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import xgboost as xgb\n",
    "# from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# # Example data (replace with your actual data)\n",
    "# train_data = np.random.rand(10, 28, 128)  # Array with shape (10, 28, 128)\n",
    "# train_labels = np.random.rand(10, 3, 128)  # Array with shape (10, 3, 128)\n",
    "\n",
    "# for t_d, t_l in zip(train_data, train_labels):\n",
    "    \n",
    "#     # Prepare the data with lagged features\n",
    "#     window_size = 20\n",
    "#     future_step = 2  # Third step ahead, considering zero-based indexing\n",
    "\n",
    "#     X_lagged = []\n",
    "#     y_lagged = []\n",
    "#     for i in range(len(t_d) - window_size - future_step ):\n",
    "#         X_lagged.append(t_d[i:i+window_size].reshape(-1))  # Flatten to (20, 128)\n",
    "#         y_lagged.append(t_d[i+window_size+future_step].reshape(1, -1))  # Reshape to (1, 128)\n",
    "\n",
    "#     X_lagged = np.array(X_lagged)\n",
    "#     y_lagged = np.array(y_lagged)\n",
    "\n",
    "#     # Create DMatrix for XGBoost\n",
    "#     dtrain = xgb.DMatrix(X_lagged, label=y_lagged)\n",
    "\n",
    "#     # Define XGBoost parameters\n",
    "#     params = {\n",
    "#         'objective': 'reg:squarederror',\n",
    "#         'eval_metric': 'rmse',\n",
    "#     }\n",
    "\n",
    "#     # Train the model\n",
    "#     num_boost_round = 100\n",
    "#     bst = xgb.train(params, dtrain, num_boost_round)\n",
    "\n",
    "#     # Validation\n",
    "#     last_window = t_d[-window_size:].reshape(1, -1)  # Flatten to (20, 128)\n",
    "#     dfuture = xgb.DMatrix(last_window)\n",
    "#     future_pred = bst.predict(dfuture)\n",
    "\n",
    "#     val_mse = mean_squared_error(future_pred, t_l[future_step].reshape(1, -1))\n",
    "#     print(\"Validation MSE:\", val_mse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826a6d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import xgboost as xgb\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "# from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# kf = KFold(n_splits=5, shuffle=True)\n",
    "\n",
    "# for fold, (train_idx, val_idx) in enumerate(kf.split(train_data)):\n",
    "#     print(f\"FOLD {fold+1}\")\n",
    "#     print(\"--------------------------------\")\n",
    "    \n",
    "#     # Split data\n",
    "#     train_fold_data = train_data[train_idx].reshape(-1,train_data.shape[1]*train_data.shape[2])\n",
    "#     train_fold_labels = train_labels[train_idx].reshape(-1,train_labels.shape[1]*train_labels.shape[2])\n",
    "#     val_fold_data = train_data[val_idx].reshape(-1,train_data.shape[1]*train_data.shape[2])\n",
    "#     val_fold_labels = train_labels[val_idx].reshape(-1,train_labels.shape[1]*train_labels.shape[2])\n",
    "\n",
    "#     # Model\n",
    "\n",
    "#     for X, y_true in zip(train_fold_data,train_fold_labels):\n",
    "\n",
    "#         # X = train_data[0]  # Array with shape (28, 128)\n",
    "#         # y_true = train_labels[0]  # Vector of length 3\n",
    "\n",
    "#         # Prepare the data with lagged features\n",
    "#         window_size = 20\n",
    "#         future_steps = 3\n",
    "\n",
    "#         X_lagged = []\n",
    "#         y_lagged = []\n",
    "#         for i in range(len(X) - window_size - future_steps + 1):\n",
    "#             X_lagged.append(X[i:i+window_size])\n",
    "#             y_lagged.append(X[i+window_size:i+window_size+future_steps])\n",
    "\n",
    "#         X_lagged = np.array(X_lagged)\n",
    "#         y_lagged = np.array(y_lagged)\n",
    "\n",
    "\n",
    "#         # Create DMatrix for XGBoost\n",
    "#         dtrain = xgb.DMatrix(X_lagged, label=y_lagged)\n",
    "\n",
    "#         # Define XGBoost parameters\n",
    "#         params = {\n",
    "#             'objective': 'reg:squarederror',\n",
    "#             'eval_metric': 'rmse',\n",
    "#         }\n",
    "\n",
    "#         # Train the model\n",
    "#         num_boost_round = 100\n",
    "#         bst = xgb.train(params, dtrain, num_boost_round)\n",
    "\n",
    "#         last_window = X[-window_size:].flatten()\n",
    "#         dfuture = xgb.DMatrix(np.array([last_window]))\n",
    "#         future_pred = bst.predict(dfuture)\n",
    "\n",
    "\n",
    "#         print(mean_squared_error(future_pred.flatten(),y_true.flatten()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0188482b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# kf = KFold(n_splits=5, shuffle=True)\n",
    "\n",
    "# for fold, (train_idx, val_idx) in enumerate(kf.split(train_data)):\n",
    "#     print(f\"FOLD {fold+1}\")\n",
    "#     print(\"--------------------------------\")\n",
    "    \n",
    "#     # Split data\n",
    "#     train_fold_data = torch.Tensor(train_data[train_idx].reshape(-1,train_data.shape[1]*train_data.shape[2]))\n",
    "#     train_fold_labels = torch.Tensor(train_labels[train_idx].reshape(-1,train_labels.shape[1]*train_labels.shape[2]))\n",
    "#     val_fold_data = torch.Tensor(train_data[val_idx].reshape(-1,train_data.shape[1]*train_data.shape[2]))\n",
    "#     val_fold_labels = torch.Tensor(train_labels[val_idx].reshape(-1,train_labels.shape[1]*train_labels.shape[2]))\n",
    "\n",
    "#     # Model\n",
    "#     lr = GradientBoostingRegressor()\n",
    "#     lr.fit(train_fold_data, train_fold_labels)\n",
    "#     predictions_train = lr.predict(train_fold_data)\n",
    "#     predictions_val = lr.predict(val_fold_data)\n",
    "#     print(mean_squared_error(train_fold_labels,predictions_train))\n",
    "#     print(mean_squared_error(val_fold_labels,predictions_val))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6861754c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import xgboost as xgb\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# # Example data (replace with your actual data)\n",
    "# X = np.random.rand(28, 128)  # 28 time steps, 128 features\n",
    "# Y = np.random.rand(3, 128)   # 3 time steps to forecast, 128 features\n",
    "\n",
    "# # Flatten the data\n",
    "# X_flattened = X.reshape(X.shape[0] * X.shape[1], -1)\n",
    "# Y_flattened = Y.flatten()\n",
    "\n",
    "# # Split data into train and test sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X_flattened[:,0], Y_flattened, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Create DMatrix for XGBoost\n",
    "# dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "# dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "# # Define XGBoost parameters\n",
    "# params = {\n",
    "#     'objective': 'reg:squarederror',\n",
    "#     'eval_metric': 'rmse',\n",
    "# }\n",
    "\n",
    "# # Train the model\n",
    "# num_boost_round = 100\n",
    "# bst = xgb.train(params, dtrain, num_boost_round)\n",
    "\n",
    "# # Make predictions\n",
    "# y_pred = bst.predict(dtest)\n",
    "\n",
    "# # Calculate and print RMSE\n",
    "# rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "# print(f'RMSE: {rmse}')\n",
    "\n",
    "# # Predict the next 3 time steps\n",
    "# # Note: The following is a simplified example, adapt as needed for your use case\n",
    "# X_future = X[-3:]  # Use the last 3 observations as input for the forecast\n",
    "# X_future_flattened = X_future.flatten().reshape(1, -1)\n",
    "# dfuture = xgb.DMatrix(X_future_flattened)\n",
    "# future_pred = bst.predict(dfuture)\n",
    "# future_pred_reshaped = future_pred.reshape(3, 128)\n",
    "\n",
    "# print(\"Future predictions:\")\n",
    "# print(future_pred_reshaped)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f918f443",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Chomp1d(nn.Module):\n",
    "#     def __init__(self, chomp_size):\n",
    "#         super(Chomp1d, self).__init__()\n",
    "#         self.chomp_size = chomp_size\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return x[:, :, :-self.chomp_size].contiguous()\n",
    "\n",
    "# class TemporalBlock(nn.Module):\n",
    "#     def __init__(self, n_inputs, n_outputs, kernel_size, stride, dilation, padding, dropout=0.2):\n",
    "#         super(TemporalBlock, self).__init__()\n",
    "#         self.conv1 = nn.Conv1d(n_inputs, n_outputs, kernel_size, stride=stride,\n",
    "#                                padding=padding, dilation=dilation)\n",
    "#         self.chomp1 = Chomp1d(padding)\n",
    "#         self.relu1 = nn.ReLU()\n",
    "#         self.dropout1 = nn.Dropout(dropout)\n",
    "#         self.conv2 = nn.Conv1d(n_outputs, n_outputs, kernel_size, stride=stride,\n",
    "#                                padding=padding, dilation=dilation)\n",
    "#         self.chomp2 = Chomp1d(padding)\n",
    "#         self.relu2 = nn.ReLU()\n",
    "#         self.dropout2 = nn.Dropout(dropout)\n",
    "        \n",
    "#         self.net = nn.Sequential(self.conv1, self.chomp1, self.relu1, self.dropout1,\n",
    "#                                  self.conv2, self.chomp2, self.relu2, self.dropout2)\n",
    "#         self.downsample = nn.Conv1d(n_inputs, n_outputs, 1) if n_inputs != n_outputs else None\n",
    "#         self.relu = nn.ReLU()\n",
    "#         self.init_weights()\n",
    "\n",
    "#     def init_weights(self):\n",
    "#         self.conv1.weight.data.normal_(0, 0.01)\n",
    "#         self.conv2.weight.data.normal_(0, 0.01)\n",
    "#         if self.downsample is not None:\n",
    "#             self.downsample.weight.data.normal_(0, 0.01)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         out = self.net(x)\n",
    "#         res = x if self.downsample is None else self.downsample(x)\n",
    "#         return self.relu(out + res)\n",
    "\n",
    "# class TemporalConvNet(nn.Module):\n",
    "#     def __init__(self, num_inputs, num_channels, kernel_size=2, dropout=0.2):\n",
    "#         super(TemporalConvNet, self).__init__()\n",
    "#         layers = []\n",
    "#         num_levels = len(num_channels)\n",
    "#         for i in range(num_levels):\n",
    "#             dilation_size = 2 ** i\n",
    "#             in_channels = num_inputs if i == 0 else num_channels[i-1]\n",
    "#             out_channels = num_channels[i]\n",
    "#             layers += [TemporalBlock(in_channels, out_channels, kernel_size, stride=1, dilation=dilation_size,\n",
    "#                                      padding=(kernel_size-1) * dilation_size, dropout=dropout)]\n",
    "\n",
    "#         self.network = nn.Sequential(*layers)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return self.network(x)\n",
    "\n",
    "# class TCNModel(nn.Module):\n",
    "#     def __init__(self, input_size, output_size, num_channels, kernel_size, dropout, num_pred_steps):\n",
    "#         super(TCNModel, self).__init__()\n",
    "#         self.tcn = TemporalConvNet(input_size, num_channels, kernel_size=kernel_size, dropout=dropout)\n",
    "#         self.linear = nn.Linear(num_channels[-1], output_size * num_pred_steps)\n",
    "#         self.num_pred_steps = num_pred_steps\n",
    "#         self.output_size = output_size\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         y1 = self.tcn(x.transpose(1, 2))\n",
    "#         o = self.linear(y1[:, :, -1])\n",
    "#         return o.view(-1, self.num_pred_steps, self.output_size)\n",
    "\n",
    "# # Hyperparameters\n",
    "# input_size = 128\n",
    "# output_size = 128\n",
    "# num_channels = [128, 256, 128]\n",
    "# kernel_size = 3\n",
    "# dropout = 0.2\n",
    "# num_epochs = 50\n",
    "# learning_rate = 0.001\n",
    "# batch_size = 128\n",
    "# k_folds = 2\n",
    "# num_pred_steps = 3  # Predicting the next 6 time steps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "dee3a461",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def test_tcn_model():\n",
    "#     model = TCNModel(input_size, output_size, num_channels, kernel_size, dropout, num_pred_steps)\n",
    "#     dummy_input = torch.randn(10, 25, 128)  # (batch_size, timesteps, features)\n",
    "#     output = model(dummy_input)\n",
    "#     assert output.shape == (10, num_pred_steps, output_size), f\"Expected output shape (10, {num_pred_steps}, {output_size}), but got {output.shape}\"\n",
    "\n",
    "# def test_tcn_training_step():\n",
    "#     model = TCNModel(input_size, output_size, num_channels, kernel_size, dropout, num_pred_steps)\n",
    "#     criterion = nn.MSELoss()\n",
    "#     optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "#     dummy_input = torch.randn(10, 25, 128)  # (batch_size, timesteps, features)\n",
    "#     dummy_target = torch.randn(10, num_pred_steps, output_size)  # (batch_size, num_pred_steps, output_size)\n",
    "#     model.train()\n",
    "#     outputs = model(dummy_input)\n",
    "#     loss = criterion(outputs, dummy_target)\n",
    "#     optimizer.zero_grad()\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "#     assert loss.item() > 0, \"Loss should be greater than 0 after one training step\"\n",
    "\n",
    "# # Run unit tests\n",
    "# test_tcn_model()\n",
    "# test_tcn_training_step()\n",
    "# print(\"Unit tests passed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ddd060a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # K-Fold Cross Validation\n",
    "# kf = KFold(n_splits=k_folds, shuffle=True)\n",
    "\n",
    "# for fold, (train_idx, val_idx) in enumerate(kf.split(train_data)):\n",
    "#     print(f\"FOLD {fold+1}\")\n",
    "#     print(\"--------------------------------\")\n",
    "    \n",
    "#     # Split data\n",
    "#     train_fold_data = torch.Tensor(train_data[train_idx])\n",
    "#     train_fold_labels = torch.Tensor(train_labels[train_idx])\n",
    "#     val_fold_data = torch.Tensor(train_data[val_idx])\n",
    "#     val_fold_labels = torch.Tensor(train_labels[val_idx])\n",
    "\n",
    "#     # Create DataLoader\n",
    "#     train_dataset = TensorDataset(train_fold_data, train_fold_labels)\n",
    "#     val_dataset = TensorDataset(val_fold_data, val_fold_labels)\n",
    "#     train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "#     val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "#     # Model, loss, optimizer\n",
    "#     model = TCNModel(input_size, output_size, num_channels, kernel_size, dropout, num_pred_steps).to(train_fold_data.device)\n",
    "#     criterion = nn.MSELoss()\n",
    "#     optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "#     # Training loop\n",
    "#     for epoch in range(num_epochs):\n",
    "#         model.train()\n",
    "#         train_loss = 0.0\n",
    "#         for inputs, targets in train_loader:\n",
    "#             outputs = model(inputs)\n",
    "#             loss = criterion(outputs, targets)\n",
    "#             optimizer.zero_grad()\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "#             train_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "#         train_loss /= len(train_loader.dataset)\n",
    "\n",
    "#         # Validation step\n",
    "#         model.eval()\n",
    "#         val_loss = 0.0\n",
    "#         with torch.no_grad():\n",
    "#             for inputs, targets in val_loader:\n",
    "#                 outputs = model(inputs)\n",
    "#                 loss = criterion(outputs, targets)\n",
    "#                 val_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "#         val_loss /= len(val_loader.dataset)\n",
    "\n",
    "#         if (epoch+1) % 10 == 0:\n",
    "#             print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
    "\n",
    "#     print(\"--------------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f0e73d-8e47-420b-8987-d654445f49c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ngram_abstracts = np.load(\"files/ngram_abstracts.npy\", mmap_mode=\"r\")\n",
    "# concept_arr = np.unique(np.load(\"files/overlapping_concepts.npy\"))\n",
    "# year_arr = np.load(\"files/year_arr.npy\", mmap_mode=\"r\")\n",
    "# month_arr = np.load(\"files/month_arr.npy\", mmap_mode=\"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a05ded5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def compute_word_count_subset(corpus, subset_words):\n",
    "    \n",
    "#     for document in tqdm(corpus):\n",
    "#         for word in document:\n",
    "#             if word in subset_words:\n",
    "#                 subset_words[word] += 1\n",
    "#     return subset_words\n",
    "\n",
    "# # Compute word count for the subset of words \n",
    "# word_count_subset = compute_word_count_subset([row.split() for row in ngram_abstracts], {k:0 for k in np.unique(concept_arr)})\n",
    "\n",
    "# def filter_dict_by_occurrence(word_count_dict, n):\n",
    "#     return {word: count for word, count in word_count_dict.items() if count > n}\n",
    "\n",
    "# filtered_concept_dict = np.array(list(filter_dict_by_occurrence(word_count_subset, 10).keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba00ece1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_ab = pd.DataFrame(data=ngram_abstracts,    # values\n",
    "#                 columns=[\"ab\"])  # 1st row as the column names\n",
    "# df_ab[\"year\"] = year_arr\n",
    "# df_ab[\"month_arr\"] = month_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28d4103",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_ab.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb118cd",
   "metadata": {},
   "source": [
    "First pass: Find all occurances of concepts individually & save them in a sfdict format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3cd3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# c_sfdict = sfdict(filename='files/concept_year_dict_filled_1.db') \n",
    "# c_dict = c_sfdict.to_dict_nested()\n",
    "# c_sfdict.close()  # should always close a db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fdb9a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# c_dict[\"ab_initio\"][\"1994\"].shape\n",
    "# np.unique(year_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0cb46b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# c_encoding_arr = np.zeros((len(c_dict),31,128))\n",
    "# c_inx_arr = []\n",
    "# for cnt, (k,v) in enumerate(c_dict.items()):\n",
    "#     c_encoding_arr[cnt] = np.array([v[str(i)] for i in np.unique(year_arr)])\n",
    "#     c_inx_arr.append(k) \n",
    "# c_inx_arr = np.array(c_inx_arr)\n",
    "# # c_encoding_arr = np.reshape(c_encoding_arr,(31,len(c_dict),128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a8f496",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save(\"c_encoding_arr\",c_encoding_arr)\n",
    "# np.save(\"c_inx_arr\",c_inx_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "50e895ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from kerastuner import HyperModel, RandomSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "75f05b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import tensorflow as tf\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional, RepeatVector, TimeDistributed, Input, BatchNormalization, LayerNormalization\n",
    "# from tensorflow.keras.models import Model\n",
    "# from keras_tuner import HyperModel, RandomSearch\n",
    "# from tensorflow.keras.callbacks import ModelCheckpoint, CSVLogger, ReduceLROnPlateau, EarlyStopping\n",
    "# from tensorflow.keras.regularizers import l2\n",
    "\n",
    "# # Configuration for window sizes\n",
    "\n",
    "# bs = 128\n",
    "# # Convert the NumPy arrays to TensorFlow datasets\n",
    "# train_dataset = tf.data.Dataset.from_tensor_slices((train_data, train_labels)).shuffle(buffer_size=10000).batch(bs)\n",
    "# # val_dataset = tf.data.Dataset.from_tensor_slices((X_val, Y_val)).batch(bs)\n",
    "# # test_dataset = tf.data.Dataset.from_tensor_slices((X_test, Y_test)).batch(bs)\n",
    "\n",
    "# class FeedBack(tf.keras.Model):\n",
    "#     def __init__(self, units, out_steps):\n",
    "#         super().__init__()\n",
    "#         self.out_steps = out_steps\n",
    "#         self.units = units\n",
    "#         self.lstm_cell = tf.keras.layers.LSTMCell(units, dropout=0.2, recurrent_dropout=0.2, kernel_regularizer=l2(0.001))\n",
    "#         self.lstm_cell2 = tf.keras.layers.LSTMCell(units, dropout=0.2, recurrent_dropout=0.2, kernel_regularizer=l2(0.001))\n",
    "        \n",
    "#         self.lstm_rnn = tf.keras.layers.RNN(self.lstm_cell, return_state=True)\n",
    "#         self.dense = tf.keras.layers.Dense(128)\n",
    "#         self.layer_norm = LayerNormalization()\n",
    "#         self.dense_warmup = TimeDistributed(Dense(128))\n",
    "\n",
    "#     def warmup(self, inputs):\n",
    "#         inputs = self.dense_warmup(inputs)\n",
    "#         x, *state = self.lstm_rnn(inputs)\n",
    "#         x = self.layer_norm(x)\n",
    "#         prediction = self.dense(x)\n",
    "#         return prediction, state\n",
    "\n",
    "#     def call(self, inputs, training=None):\n",
    "#         predictions = []\n",
    "#         prediction, state = self.warmup(inputs)\n",
    "#         predictions.append(prediction)\n",
    "\n",
    "#         for n in range(1, self.out_steps):\n",
    "#             x = prediction\n",
    "#             x, state = self.lstm_cell(x, states=state, training=training)\n",
    "#             x = self.layer_norm(x)\n",
    "#             x, state = self.lstm_cell2(x, states=state, training=training)\n",
    "#             x = self.layer_norm(x)\n",
    "#             prediction = self.dense(x)\n",
    "#             predictions.append(prediction)\n",
    "\n",
    "#         predictions = tf.stack(predictions)\n",
    "#         predictions = tf.transpose(predictions, [1, 0, 2])\n",
    "#         return predictions\n",
    "\n",
    "# multi_dense_model = tf.keras.Sequential([\n",
    "#     # Take the last time step.\n",
    "#     # Shape [batch, time, features] => [batch, 1, features]\n",
    "#     tf.keras.layers.Lambda(lambda x: x[:, -1:, :]),\n",
    "#     # Shape => [batch, 1, dense_units]\n",
    "#     tf.keras.layers.Dense(512, activation='relu',kernel_regularizer=l2(0.0001)),\n",
    "#     # tf.keras.layers.Dropout(0.1),\n",
    "#     tf.keras.layers.Dense(512, activation='relu',kernel_regularizer=l2(0.0001)),\n",
    "#     tf.keras.layers.Dense(512, activation='relu',kernel_regularizer=l2(0.0001)),\n",
    "#     # tf.keras.layers.Dropout(0.1),\n",
    "#     tf.keras.layers.Dense(512, activation='relu',kernel_regularizer=l2(0.0001)),\n",
    "#     # tf.keras.layers.Dropout(0.25),\n",
    "#     # Shape => [batch, out_steps*features]\n",
    "#     tf.keras.layers.Dense(output_window_size*128,\n",
    "#                           kernel_initializer=tf.initializers.zeros()),\n",
    "#     # Shape => [batch, out_steps, features]\n",
    "#     tf.keras.layers.Reshape([output_window_size, 128])\n",
    "# ])\n",
    "\n",
    "# CONV_WIDTH = 12\n",
    "# multi_conv_model = tf.keras.Sequential([\n",
    "#     # Shape [batch, time, features] => [batch, CONV_WIDTH, features]\n",
    "#     tf.keras.layers.Lambda(lambda x: x[:, -CONV_WIDTH:, :]),\n",
    "#     # Shape => [batch, 1, conv_units]\n",
    "#     tf.keras.layers.Conv1D(384, activation='relu', kernel_size=(CONV_WIDTH)),\n",
    "#     # tf.keras.layers.Conv1D(1, activation='relu', kernel_size=(CONV_WIDTH//2)),\n",
    "#     # Shape => [batch, 1,  out_steps*features]\n",
    "#     tf.keras.layers.Dense(384, activation='relu'),\n",
    "#     tf.keras.layers.Dense(3*128,\n",
    "#                           kernel_initializer=tf.initializers.zeros()),\n",
    "#     # Shape => [batch, out_steps, features]\n",
    "#     tf.keras.layers.Reshape([3, 128])\n",
    "# ])\n",
    "\n",
    "# # Example usage:\n",
    "# input_window_size = 26\n",
    "# output_window_size = 3\n",
    "\n",
    "# model = multi_conv_model#FeedBack(units=128, out_steps=output_window_size)\n",
    "\n",
    "# model.compile(loss='mse', optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001))\n",
    "\n",
    "# reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, verbose=1, min_lr=0)\n",
    "# early_stop = EarlyStopping(monitor='val_loss', patience=20, verbose=1)\n",
    "\n",
    "# history = model.fit(train_dataset, epochs=100)\n",
    "\n",
    "# # Evaluate the model on the test data\n",
    "# test_loss = model.evaluate(train_dataset)\n",
    "# print(f\"Test Loss: {test_loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "56489fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_tf_dataset(\n",
    "#     data_array: np.ndarray,\n",
    "#     input_sequence_length: int,\n",
    "#     forecast_horizon: int,\n",
    "#     batch_size: int = 128,\n",
    "#     shuffle=False,\n",
    "#     multi_horizon=True,\n",
    "# ):\n",
    "#     \"\"\"Creates tensorflow dataset from numpy array.\n",
    "\n",
    "#     This function creates a dataset where each element is a tuple `(inputs, targets)`.\n",
    "#     `inputs` is a Tensor\n",
    "#     of shape `(batch_size, input_sequence_length, num_routes, 1)` containing\n",
    "#     the `input_sequence_length` past values of the timeseries for each node.\n",
    "#     `targets` is a Tensor of shape `(batch_size, forecast_horizon, num_routes)`\n",
    "#     containing the `forecast_horizon`\n",
    "#     future values of the timeseries for each node.\n",
    "\n",
    "#     Args:\n",
    "#         data_array: np.ndarray with shape `(num_time_steps, num_routes)`\n",
    "#         input_sequence_length: Length of the input sequence (in number of timesteps).\n",
    "#         forecast_horizon: If `multi_horizon=True`, the target will be the values of the timeseries for 1 to\n",
    "#             `forecast_horizon` timesteps ahead. If `multi_horizon=False`, the target will be the value of the\n",
    "#             timeseries `forecast_horizon` steps ahead (only one value).\n",
    "#         batch_size: Number of timeseries samples in each batch.\n",
    "#         shuffle: Whether to shuffle output samples, or instead draw them in chronological order.\n",
    "#         multi_horizon: See `forecast_horizon`.\n",
    "\n",
    "#     Returns:\n",
    "#         A tf.data.Dataset instance.\n",
    "#     \"\"\"\n",
    "\n",
    "#     inputs = tf.keras.utils.timeseries_dataset_from_array(\n",
    "#         data_array[:-forecast_horizon],\n",
    "#         None,\n",
    "#         sequence_length=input_sequence_length,\n",
    "#         shuffle=False,\n",
    "#         batch_size=batch_size,\n",
    "#     )\n",
    "\n",
    "#     target_offset = (\n",
    "#         input_sequence_length\n",
    "#         if multi_horizon\n",
    "#         else input_sequence_length + forecast_horizon - 1\n",
    "#     )\n",
    "#     target_seq_length = forecast_horizon if multi_horizon else 1\n",
    "#     targets = tf.keras.utils.timeseries_dataset_from_array(\n",
    "#         data_array[target_offset:],\n",
    "#         None,\n",
    "#         sequence_length=target_seq_length,\n",
    "#         shuffle=False,\n",
    "#         batch_size=batch_size,\n",
    "#     )\n",
    "\n",
    "#     dataset = tf.data.Dataset.zip((inputs, targets))\n",
    "#     if shuffle:\n",
    "#         dataset = dataset.shuffle(100)\n",
    "\n",
    "#     return dataset.prefetch(16).cache()\n",
    "\n",
    "# def get_encoder(net_config, activation_function, data_shape, latent_dim, batch_size):  \n",
    "#     encoder_inputs = tf.keras.Input(shape=(data_shape), batch_size=batch_size)\n",
    "#     x = encoder_inputs\n",
    "#     for inx in net_config:\n",
    "#         x = tf.keras.layers.Dense(inx, activation=activation_function)(x)\n",
    "#     encoder_outputs = tf.keras.layers.Dense(latent_dim)(x)\n",
    "    \n",
    "#     return tf.keras.Model(encoder_inputs, encoder_outputs, name = \"encoder\")\n",
    "\n",
    "# def get_decoder(net_config, activation_function, data_shape, latent_dim, batch_size):\n",
    "\n",
    "#     latent_inputs = tf.keras.Input(shape=(data_shape[0],latent_dim), batch_size=(batch_size))\n",
    "#     x = latent_inputs\n",
    "#     for inx in net_config[::-1]:\n",
    "#         x = tf.keras.layers.Dense(inx, activation=activation_function)(x)\n",
    "    \n",
    "#     decoder_outputs = tf.keras.layers.Dense(data_shape[1])(x)\n",
    "    \n",
    "#     return tf.keras.Model(latent_inputs, decoder_outputs, name=\"decoder\")\n",
    "\n",
    "\n",
    "\n",
    "# def get_predecoder(data_shape, latent_dim, batch_size):\n",
    "\n",
    "#     x_predecoder_inputs = tf.keras.Input((data_shape[0]//2,latent_dim),batch_size=(batch_size))\n",
    "\n",
    "#     # Shape [batch, time, features] => [batch, out_steps, features].\n",
    "#     # Adding more `lstm_units` just overfits more quickly.\n",
    "#     x = tf.keras.layers.GRU(80, return_sequences=True)(x_predecoder_inputs)\n",
    "#     # x = tf.keras.layers.GRU(32, return_sequences=True)(x)\n",
    "#     # x = tf.keras.layers.GRU(32, return_sequences=True)(x)\n",
    "#     x = tf.keras.layers.GRU(80, return_sequences=False)(x)\n",
    "#     # Shape => [batch, out_steps*features].\n",
    "#     x = tf.keras.layers.Dense(data_shape[0]//2*latent_dim,\n",
    "#                           kernel_initializer=tf.initializers.zeros())(x)\n",
    "    \n",
    "#     x = tf.keras.layers.Reshape([data_shape[0]//2, latent_dim])(x)\n",
    "#     # Shape => [batch, out_steps, features].\n",
    "#     return tf.keras.Model(x_predecoder_inputs, [x,x,x], name=\"xpredecoder\")\n",
    "\n",
    "# # Only does signal processing and no actual time evo in its latent space\n",
    "\n",
    "# class TAE(tf.keras.Model):\n",
    "#     def __init__(self, encoder, decoder, pre_decoder, data_shape, **kwargs):\n",
    "#         super(TAE, self).__init__(**kwargs)\n",
    "#         self.encoder = encoder\n",
    "#         self.decoder = decoder\n",
    "#         self.pre_decoder = pre_decoder\n",
    "#         self.gamma = 0 \n",
    "#         self.optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "#         self.data_shape = data_shape\n",
    "#         self.latent_dim = 2\n",
    "        \n",
    "\n",
    "# @tf.function\n",
    "# def train_step(model, data,gamma):\n",
    "#     with tf.GradientTape() as tape:\n",
    "\n",
    "#         y, _ = data \n",
    "#         x = model.encoder(y)\n",
    "\n",
    "#         x_1, x_2 = tf.split(x,num_or_size_splits=2,axis=1)\n",
    "\n",
    "#         _, _, x_hat = model.pre_decoder(x_1)\n",
    "\n",
    "#         x_conc = tf.concat([x_1, x_hat], axis=1)\n",
    "\n",
    "#         seq_loss = tf.reduce_mean(\n",
    "#             tf.reduce_sum(tf.keras.losses.mean_squared_error(x_2, x_hat), axis=-1))        \n",
    "#         seq_loss = seq_loss/(model.latent_dim*(model.data_shape[0]//2))\n",
    "        \n",
    "#         y_hat = model.decoder(x_conc) \n",
    "#         reconstruction_loss = tf.reduce_mean(\n",
    "#             tf.reduce_sum(tf.keras.losses.mean_absolute_error(y, y_hat), axis=-1))\n",
    "#         reconstruction_loss = reconstruction_loss/(model.data_shape[0]*model.data_shape[1])\n",
    "            \n",
    "#         total_loss = reconstruction_loss + gamma*seq_loss\n",
    "    \n",
    "#     grads = tape.gradient(total_loss, model.trainable_weights)\n",
    "#     model.optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "\n",
    "#     return total_loss, reconstruction_loss, seq_loss\n",
    "\n",
    "# def gamma_sched(n_iter, start=0.0, stop=0.25,  n_cycle=1, ratio=0.8):\n",
    "#     L = np.ones(n_iter) * stop\n",
    "#     period = n_iter/n_cycle\n",
    "#     step = (stop-start)/(period*ratio) # linear schedule\n",
    "\n",
    "#     for c in range(n_cycle):\n",
    "#         v, i = start, 0\n",
    "#         while v <= stop and (int(i+c*period) < n_iter):\n",
    "#             L[int(i+c*period)] = v#np.sin(3*i/period)       \n",
    "#             v += step\n",
    "#             i += 1\n",
    "#     return L \n",
    "\n",
    "# batch_size = 128\n",
    "# input_sequence_length = 30\n",
    "# multi_horizon = True\n",
    "# n_traj = 128\n",
    "\n",
    "# train_dataset = (\n",
    "#     create_tf_dataset(normalized_data, input_sequence_length, 1, batch_size)\n",
    "# )\n",
    "\n",
    "# # test_dataset = create_tf_dataset(\n",
    "# #     test_array,\n",
    "# #     input_sequence_length,\n",
    "# #     1,\n",
    "# #     batch_size=test_array.shape[0],\n",
    "# #     shuffle=False,\n",
    "# #     multi_horizon=multi_horizon,\n",
    "# # )\n",
    "\n",
    "# data_shape = (input_sequence_length,n_traj)\n",
    "\n",
    "# latent_dim = 3\n",
    "# net_config = np.array([300,100])\n",
    "# activation_function = \"leaky_relu\"\n",
    "\n",
    "\n",
    "\n",
    "# encoder = get_encoder(net_config, activation_function, data_shape, latent_dim, batch_size)\n",
    "# decoder = get_decoder(net_config, activation_function, data_shape, latent_dim, batch_size)\n",
    "# predecoder = get_predecoder(data_shape, latent_dim, batch_size)\n",
    "\n",
    "# epochs = 500\n",
    "# lr_sched = tf.keras.optimizers.schedules.CosineDecay(\n",
    "#     initial_learning_rate=0.0001,\n",
    "#     decay_steps=epochs*int(len(train_dataset)),\n",
    "#     alpha=0.00005/0.0001,\n",
    "#     )\n",
    "# loss_arr = np.zeros(epochs)\n",
    "# rc_loss_arr =  np.zeros(epochs)\n",
    "# seq_loss_arr = np.zeros(epochs)\n",
    "\n",
    "# ae_model = TAE(encoder, decoder, predecoder, data_shape)\n",
    "# # ae_model.gamma = 0.5\n",
    "# # ae_model.latent_dim = 2\n",
    "# gamma_arr = gamma_sched(epochs,stop=0.15,ratio=0.75)\n",
    "# # ae_model.optimizer = tf.keras.optimizers.Adam(learning_rate=lr_sched)\n",
    "# # beta_arr = beta_sched(epochs,stop=0.15,ratio=0.5)\n",
    "\n",
    "# for epoch in tqdm(range(epochs)):\n",
    "#     gamma = gamma_arr[epoch] \n",
    "    \n",
    "#     for step, batch_data in enumerate(train_dataset):\n",
    "#         loss, rc_loss, seq_loss = train_step(ae_model, batch_data,np.float32(gamma))\n",
    "\n",
    "#         loss_arr[epoch] = loss\n",
    "#         rc_loss_arr[epoch] = rc_loss\n",
    "#         seq_loss_arr[epoch] = seq_loss\n",
    "\n",
    "\n",
    "# loss_arr[-1], rc_loss_arr[-1], seq_loss_arr[-1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bce655e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import tensorflow as tf\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional, RepeatVector, TimeDistributed, Input, BatchNormalization, LayerNormalization\n",
    "# from tensorflow.keras.models import Model\n",
    "# from keras_tuner import HyperModel, RandomSearch\n",
    "# from tensorflow.keras.callbacks import ModelCheckpoint, CSVLogger, ReduceLROnPlateau, EarlyStopping\n",
    "# from tensorflow.keras.regularizers import l2\n",
    "\n",
    "# # Configuration for window sizes\n",
    "# input_window_size = 26  # Number of input time steps (e.g., 0-25)\n",
    "# output_window_size = 3  # Number of time steps to predict (e.g., 25-28)\n",
    "\n",
    "# # Assuming your data is in a NumPy array of shape (10000, 31, 128)\n",
    "# data = c_encoding_arr\n",
    "\n",
    "# # Ensure the total number of time steps is enough to create the windows\n",
    "# total_time_steps = data.shape[1]\n",
    "# assert total_time_steps >= input_window_size + output_window_size, \"Insufficient time steps in data.\"\n",
    "\n",
    "# # Normalize the data\n",
    "# scaler = StandardScaler()\n",
    "\n",
    "# # Reshape data to 2D for normalization\n",
    "# reshaped_data = data.reshape(-1, data.shape[-1])  # Shape: (10000*31, 128)\n",
    "# normalized_data = scaler.fit_transform(reshaped_data)\n",
    "\n",
    "# # Reshape back to original shape\n",
    "# normalized_data = normalized_data.reshape(data.shape)\n",
    "\n",
    "# # Prepare training inputs and outputs\n",
    "# X_train = normalized_data[:, :input_window_size, :]  # Shape: (10000, input_window_size, 128)\n",
    "# Y_train = normalized_data[:, input_window_size:input_window_size + output_window_size, :]  # Shape: (10000, output_window_size, 128)\n",
    "\n",
    "# # Prepare validation inputs and outputs\n",
    "# X_val = normalized_data[:, 1:1 + input_window_size, :]  # Shape: (10000, input_window_size, 128)\n",
    "# Y_val = normalized_data[:, 1 + input_window_size:1 + input_window_size + output_window_size, :]  # Shape: (10000, output_window_size, 128)\n",
    "\n",
    "# # Prepare test inputs and outputs\n",
    "# X_test = normalized_data[:, 2:2 + input_window_size, :]  # Shape: (10000, input_window_size, 128)\n",
    "# Y_test = normalized_data[:, 2 + input_window_size:2 + input_window_size + output_window_size, :]  # Shape: (10000, output_window_size, 128)\n",
    "\n",
    "# bs = 64\n",
    "# # Convert the NumPy arrays to TensorFlow datasets\n",
    "# train_dataset = tf.data.Dataset.from_tensor_slices((X_train, Y_train)).shuffle(buffer_size=10000).batch(bs)\n",
    "# val_dataset = tf.data.Dataset.from_tensor_slices((X_val, Y_val)).batch(bs)\n",
    "# test_dataset = tf.data.Dataset.from_tensor_slices((X_test, Y_test)).batch(bs)\n",
    "\n",
    "# class StackedLSTM(tf.keras.Model):\n",
    "#     def __init__(self, units, num_layers, out_steps):\n",
    "#         super().__init__()\n",
    "#         self.out_steps = out_steps\n",
    "#         self.units = units\n",
    "#         self.num_layers = num_layers\n",
    "\n",
    "#         # Create a list of LSTM cells\n",
    "#         self.lstm_cells = [tf.keras.layers.LSTMCell(units, dropout=0.2, recurrent_dropout=0.2, kernel_regularizer=l2(0.001)) for _ in range(num_layers)]\n",
    "#         self.lstm_rnn = tf.keras.layers.RNN(self.lstm_cells, return_state=True)\n",
    "        \n",
    "#         self.dense = tf.keras.layers.Dense(128)\n",
    "#         self.layer_norm = LayerNormalization()\n",
    "#         self.dense_warmup = TimeDistributed(Dense(128))\n",
    "\n",
    "#     def warmup(self, inputs):\n",
    "#         inputs = self.dense_warmup(inputs)\n",
    "#         x, *state = self.lstm_rnn(inputs)\n",
    "#         x = self.layer_norm(x)\n",
    "#         prediction = self.dense(x)\n",
    "#         return prediction, state\n",
    "\n",
    "#     def call(self, inputs, training=None):\n",
    "#         predictions = []\n",
    "#         prediction, state = self.warmup(inputs)\n",
    "#         predictions.append(prediction)\n",
    "\n",
    "#         for n in range(1, self.out_steps):\n",
    "#             x = prediction\n",
    "#             x, state = self.lstm_cells[-1](x, states=state, training=training)\n",
    "#             x = self.layer_norm(x)\n",
    "#             prediction = self.dense(x)\n",
    "#             predictions.append(prediction)\n",
    "\n",
    "#         predictions = tf.stack(predictions)\n",
    "#         predictions = tf.transpose(predictions, [1, 0, 2])\n",
    "#         return predictions\n",
    "\n",
    "# # Example usage:\n",
    "# input_window_size = 26\n",
    "# output_window_size = 3\n",
    "# num_layers = 3  # Number of stacked LSTM layers\n",
    "\n",
    "# model = StackedLSTM(units=128, num_layers=num_layers, out_steps=output_window_size)\n",
    "\n",
    "# model.compile(loss='mse', optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001))\n",
    "\n",
    "# reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, verbose=1, min_lr=0)\n",
    "# early_stop = EarlyStopping(monitor='val_loss', patience=20, verbose=1)\n",
    "\n",
    "# history = model.fit(train_dataset, epochs=300, validation_data=val_dataset, callbacks=[reduce_lr, early_stop])\n",
    "\n",
    "# # Evaluate the model on the test data\n",
    "# test_loss = model.evaluate(test_dataset)\n",
    "# print(f\"Test Loss: {test_loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7eb15b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import tensorflow as tf\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional, RepeatVector, TimeDistributed, Input, BatchNormalization\n",
    "# from tensorflow.keras.models import Model\n",
    "# from keras_tuner import HyperModel, RandomSearch\n",
    "# from tensorflow.keras.callbacks import ModelCheckpoint,CSVLogger,ReduceLROnPlateau,EarlyStopping\n",
    "# from tensorflow.keras.regularizers import l2\n",
    "# from tensorflow.keras.layers import GRUCell\n",
    "\n",
    "# # Configuration for window sizes\n",
    "# input_window_size = 26  # Number of input time steps (e.g., 0-25)\n",
    "# output_window_size = 3  # Number of time steps to predict (e.g., 25-28)\n",
    "\n",
    "# # Assuming your data is in a NumPy array of shape (10000, 31, 128)\n",
    "# data = c_encoding_arr\n",
    "\n",
    "# # Ensure the total number of time steps is enough to create the windows\n",
    "# total_time_steps = data.shape[1]\n",
    "# assert total_time_steps >= input_window_size + output_window_size, \"Insufficient time steps in data.\"\n",
    "\n",
    "# # Normalize the data\n",
    "# scaler = StandardScaler()\n",
    "\n",
    "# # Reshape data to 2D for normalization\n",
    "# reshaped_data = data.reshape(-1, data.shape[-1])  # Shape: (10000*31, 128)\n",
    "# normalized_data = scaler.fit_transform(reshaped_data)\n",
    "\n",
    "# # Reshape back to original shape\n",
    "# normalized_data = normalized_data.reshape(data.shape)\n",
    "\n",
    "# # Prepare training inputs and outputs\n",
    "# X_train = normalized_data[:, :input_window_size, :]  # Shape: (10000, input_window_size, 128)\n",
    "# Y_train = normalized_data[:, input_window_size:input_window_size + output_window_size, :]  # Shape: (10000, output_window_size, 128)\n",
    "\n",
    "# # Prepare validation inputs and outputs\n",
    "# X_val = normalized_data[:, 1:1 + input_window_size, :]  # Shape: (10000, input_window_size, 128)\n",
    "# Y_val = normalized_data[:, 1 + input_window_size:1 + input_window_size + output_window_size, :]  # Shape: (10000, output_window_size, 128)\n",
    "\n",
    "# # Prepare test inputs and outputs\n",
    "# X_test = normalized_data[:, 2:2 + input_window_size, :]  # Shape: (10000, input_window_size, 128)\n",
    "# Y_test = normalized_data[:, 2 + input_window_size:2 + input_window_size + output_window_size, :]  # Shape: (10000, output_window_size, 128)\n",
    "\n",
    "\n",
    "# bs = 64\n",
    "# # Convert the NumPy arrays to TensorFlow datasets\n",
    "# train_dataset = tf.data.Dataset.from_tensor_slices((X_train, Y_train)).shuffle(buffer_size=10000).batch(bs)\n",
    "# val_dataset = tf.data.Dataset.from_tensor_slices((X_val, Y_val)).batch(bs)\n",
    "# test_dataset = tf.data.Dataset.from_tensor_slices((X_test, Y_test)).batch(bs)\n",
    "\n",
    "\n",
    "\n",
    "# class FeedBack(tf.keras.Model):\n",
    "#     def __init__(self, units, out_steps):\n",
    "#         super().__init__()\n",
    "#         self.out_steps = out_steps\n",
    "#         self.units = units\n",
    "#         self.lstm_cell = tf.keras.layers.LSTMCell(units,dropout=0.2,recurrent_dropout=0.2, kernel_regularizer=l2(0.001))\n",
    "#         # self.lstm_cell = tf.keras.layers.GRUCell(units,dropout=0.2,recurrent_dropout=0.2, kernel_regularizer=l2(0.001))\n",
    "\n",
    "#         # Also wrap the LSTMCell in an RNN to simplify the `warmup` method.\n",
    "#         self.lstm_rnn = tf.keras.layers.RNN(self.lstm_cell, return_state=True)\n",
    "#         self.dense = tf.keras.layers.Dense(128)\n",
    "#         self.batch_norm = BatchNormalization()\n",
    "#         self.dense_warmup = TimeDistributed(Dense(128))\n",
    "\n",
    "\n",
    "#     def warmup(self, inputs):\n",
    "#         # inputs.shape => (batch, time, features)\n",
    "#         # x.shape => (batch, lstm_units)\n",
    "#         inputs = self.dense_warmup(inputs)\n",
    "#         x, *state = self.lstm_rnn(inputs)\n",
    "#         x = self.batch_norm(x)\n",
    "#         # predictions.shape => (batch, features)\n",
    "#         prediction = self.dense(x)\n",
    "#         return prediction, state\n",
    "\n",
    "#     def call(self, inputs, training=None):\n",
    "#         # Use a TensorArray to capture dynamically unrolled outputs.\n",
    "#         predictions = []\n",
    "#         # Initialize the LSTM state.\n",
    "#         prediction, state = self.warmup(inputs)\n",
    "\n",
    "#         # Insert the first prediction.\n",
    "#         predictions.append(prediction)\n",
    "\n",
    "#         # Run the rest of the prediction steps.\n",
    "#         for n in range(1, self.out_steps):\n",
    "#             # Use the last prediction as input.\n",
    "#             x = prediction\n",
    "#             # Execute one lstm step.\n",
    "#             x, state = self.lstm_cell(x, states=state,\n",
    "#                                     training=training)\n",
    "#             x = self.batch_norm(x)\n",
    "#             # Convert the lstm output to a prediction.\n",
    "#             prediction = self.dense(x)\n",
    "#             # Add the prediction to the output.\n",
    "#             predictions.append(prediction)\n",
    "\n",
    "#         # predictions.shape => (time, batch, features)\n",
    "#         predictions = tf.stack(predictions)\n",
    "#         # predictions.shape => (batch, time, features)\n",
    "#         predictions = tf.transpose(predictions, [1, 0, 2])\n",
    "#         return predictions\n",
    "\n",
    "\n",
    "\n",
    "# # Example usage:\n",
    "# # Assuming input_window_size is defined as 26 and input features as 128.\n",
    "# input_window_size = 26\n",
    "# output_window_size = 3\n",
    "\n",
    "# model = FeedBack(units=128, out_steps=output_window_size)\n",
    "\n",
    "\n",
    "# model.compile(loss='mse', optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001))\n",
    "\n",
    "# reduce_lr = ReduceLROnPlateau(monitor='loss', factor=0.5,\n",
    "#                               patience=10, verbose=1,min_lr=0)\n",
    "\n",
    "\n",
    "# early_stop= EarlyStopping(monitor='loss', patience=20, verbose=1)\n",
    "\n",
    "# history = model.fit(train_dataset,\n",
    "#           epochs=300,\n",
    "#           validation_data=val_dataset,\n",
    "#           callbacks=[reduce_lr,early_stop])\n",
    "\n",
    "\n",
    "# # Evaluate the model on the test data\n",
    "# test_loss = model.evaluate(test_dataset)\n",
    "# print(f\"Test Loss: {test_loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e54bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Sequential([\n",
    "#     LSTM(128, activation='relu', input_shape=(input_window_size, 128),return_sequences=True,dropout=0.3),\n",
    "#     LSTM(128, activation='relu',dropout=0.3),\n",
    "#     RepeatVector(output_window_size),\n",
    "#     TimeDistributed(Dense(3))\n",
    "# ])\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "da29fddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# i = 4\n",
    "# predictions = model.predict(X_test)\n",
    "# inx = c_inx_arr\n",
    "\n",
    "# # Inverse transform the scaled predictions\n",
    "# original_predictions = scaler.inverse_transform(predictions.reshape(-1, 128)).reshape(predictions.shape)\n",
    "# goal_predictions = scaler.inverse_transform(Y_test.reshape(-1, 128)).reshape(Y_test.shape)\n",
    "\n",
    "# def similarity_cosine(vec1, vec2):\n",
    "#     cosine_similarity = np.dot(vec1, vec2)/(np.linalg.norm(vec1)* np.linalg.norm(vec2))\n",
    "#     return cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1d23738a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# p_arr = []\n",
    "# g_arr = []\n",
    "\n",
    "# i1 = np.random.randint(len(inx))\n",
    "# i2 = np.random.randint(len(inx))\n",
    "\n",
    "# for i in [0,1,2]:\n",
    "#     p_arr.append(similarity_cosine(original_predictions[i1,i],original_predictions[i2,i]))\n",
    "#     g_arr.append(similarity_cosine(goal_predictions[i1,i],goal_predictions[i2,i]))\n",
    "\n",
    "# plt.plot([1,2,3],p_arr,label=\"Prediction\")\n",
    "# plt.plot([1,2,3],g_arr,label=\"True\")\n",
    "# plt.gca().xaxis.set_major_locator(plt.MaxNLocator(integer=True))\n",
    "# plt.xlabel(\"Future Time Steps\")\n",
    "# plt.ylabel(\"Cosine Similarity\")\n",
    "# plt.legend()\n",
    "# plt.title(str(inx[i1])+\" \"+str(inx[i2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c69690c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# class TimeSeriesHyperModel(HyperModel):\n",
    "#     def build(self, hp):\n",
    "#         model = Sequential()\n",
    "#         model.add(Bidirectional(LSTM(units=hp.Int('units_1', min_value=32, max_value=128, step=32), \n",
    "#                                     activation='relu', return_sequences=True), \n",
    "#                                 input_shape=(input_window_size, 128)))\n",
    "#         model.add(Dropout(rate=hp.Float('dropout_1', min_value=0.1, max_value=0.5, step=0.1)))\n",
    "#         model.add(Bidirectional(LSTM(units=hp.Int('units_2', min_value=32, max_value=128, step=32), \n",
    "#                                     activation='relu', return_sequences=False)))\n",
    "#         model.add(RepeatVector(output_window_size))\n",
    "#         model.add(Bidirectional(LSTM(units=hp.Int('units_3', min_value=32, max_value=128, step=32), \n",
    "#                                     activation='relu', return_sequences=True)))\n",
    "#         model.add(Dropout(rate=hp.Float('dropout_2', min_value=0.1, max_value=0.5, step=0.1)))\n",
    "#         model.add(TimeDistributed(Dense(128)))\n",
    "        \n",
    "#         model.compile(optimizer='adam', loss='mse')\n",
    "#         return model\n",
    "\n",
    "# class TimeSeriesHyperModel(HyperModel):\n",
    "#     def build(self, hp):\n",
    "#         model = Sequential()\n",
    "#         model.add(LSTM(units=hp.Int('units_1', min_value=32, max_value=128, step=32), \n",
    "#                                     activation='relu',input_shape=(input_window_size, 128)))\n",
    "#         model.add(RepeatVector(output_window_size))\n",
    "#         model.add(Dropout(rate=hp.Float('dropout_1', min_value=0.1, max_value=0.5, step=0.1)))\n",
    "#         model.add(LSTM(units=hp.Int('units_3', min_value=32, max_value=128, step=32), \n",
    "#                                     activation='relu', return_sequences=True))\n",
    "#         model.add(Dropout(rate=hp.Float('dropout_2', min_value=0.1, max_value=0.5, step=0.1)))\n",
    "#         model.add(TimeDistributed(Dense(128)))\n",
    "        \n",
    "#         model.compile(optimizer='adam', loss='mse')\n",
    "#         return model\n",
    "\n",
    "\n",
    "# # Initialize the tuner\n",
    "# tuner = RandomSearch(\n",
    "#     TimeSeriesHyperModel(),\n",
    "#     objective='val_loss',\n",
    "#     max_trials=20,\n",
    "#     executions_per_trial=2,\n",
    "#     directory='hyperparameter_tuning',\n",
    "#     project_name='time_series_prediction'\n",
    "# )\n",
    "\n",
    "# Perform the search\n",
    "# tuner.search(train_dataset, epochs=20, validation_data=val_dataset)\n",
    "\n",
    "# Retrieve the best model\n",
    "# best_model = tuner.get_best_models(num_models=1)[0]\n",
    "\n",
    "# Train the best model on the full dataset\n",
    "# test_dataset = tf.data.Dataset.from_tensor_slices((X_test, Y_test)).batch(32)\n",
    "# best_model.fit(train_dataset, epochs=20, validation_data=val_dataset)\n",
    "\n",
    "# Evaluate the best model on the test data\n",
    "# test_loss = best_model.evaluate(test_dataset)\n",
    "# print(f\"Test Loss: {test_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "7ad3d424",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_encoder(net_config, activation_function, data_shape, latent_dim, batch_size):  \n",
    "#     encoder_inputs = tf.keras.Input(shape=(data_shape), batch_size=batch_size)\n",
    "#     x = encoder_inputs\n",
    "#     for inx in net_config:\n",
    "#         x = tf.keras.layers.Dense(inx, activation=activation_function)(x)\n",
    "#     encoder_outputs = tf.keras.layers.Dense(latent_dim)(x)\n",
    "#     return tf.keras.Model(encoder_inputs, encoder_outputs, name = \"encoder\")\n",
    "\n",
    "# def get_decoder(net_config, activation_function, data_shape, latent_dim, batch_size):\n",
    "#     latent_inputs = tf.keras.Input(shape=(data_shape[0],latent_dim), batch_size=(batch_size))\n",
    "#     x = latent_inputs\n",
    "#     for inx in net_config[::-1]:\n",
    "#         x = tf.keras.layers.Dense(inx, activation=activation_function)(x)\n",
    "#     decoder_outputs = tf.keras.layers.Dense(data_shape[1])(x)\n",
    "#     return tf.keras.Model(latent_inputs, decoder_outputs, name=\"decoder\")\n",
    "\n",
    "\n",
    "\n",
    "# def get_predecoder(data_shape, latent_dim, batch_size):\n",
    "\n",
    "#     x_predecoder_inputs = tf.keras.Input((data_shape[0]//2,latent_dim),batch_size=(batch_size))\n",
    "\n",
    "#     # Shape [batch, time, features] => [batch, out_steps, features].\n",
    "#     # Adding more `lstm_units` just overfits more quickly.\n",
    "#     x = tf.keras.layers.GRU(80, return_sequences=True)(x_predecoder_inputs)\n",
    "#     # x = tf.keras.layers.GRU(32, return_sequences=True)(x)\n",
    "#     # x = tf.keras.layers.GRU(32, return_sequences=True)(x)\n",
    "#     x = tf.keras.layers.GRU(80, return_sequences=False)(x)\n",
    "#     # Shape => [batch, out_steps*features].\n",
    "#     x = tf.keras.layers.Dense(data_shape[0]//2*latent_dim,\n",
    "#                           kernel_initializer=tf.initializers.zeros())(x)\n",
    "    \n",
    "#     x = tf.keras.layers.Reshape([data_shape[0]//2, latent_dim])(x)\n",
    "#     # Shape => [batch, out_steps, features].\n",
    "#     return tf.keras.Model(x_predecoder_inputs, [x,x,x], name=\"xpredecoder\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
